{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "final_results_jaccard_coefficient.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "luxERQDdueKw",
        "outputId": "0c32c19a-2f5e-4fc3-c85f-adeb7a4da25f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Error loading corpus: Package 'corpus' not found in index\n",
            "Collecting git+https://github.com/LIAAD/yake\n",
            "  Cloning https://github.com/LIAAD/yake to /tmp/pip-req-build-i0en_fj7\n",
            "  Running command git clone -q https://github.com/LIAAD/yake /tmp/pip-req-build-i0en_fj7\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.7/dist-packages (from yake==0.4.8) (0.8.9)\n",
            "Requirement already satisfied: click>=6.0 in /usr/local/lib/python3.7/dist-packages (from yake==0.4.8) (7.1.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from yake==0.4.8) (1.21.6)\n",
            "Requirement already satisfied: segtok in /usr/local/lib/python3.7/dist-packages (from yake==0.4.8) (1.5.11)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.7/dist-packages (from yake==0.4.8) (2.6.3)\n",
            "Requirement already satisfied: jellyfish in /usr/local/lib/python3.7/dist-packages (from yake==0.4.8) (0.9.0)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from segtok->yake==0.4.8) (2019.12.20)\n",
            "Requirement already satisfied: multi_rake in /usr/local/lib/python3.7/dist-packages (0.0.2)\n",
            "Requirement already satisfied: regex>=2018.6.6 in /usr/local/lib/python3.7/dist-packages (from multi_rake) (2019.12.20)\n",
            "Requirement already satisfied: numpy>=1.14.4 in /usr/local/lib/python3.7/dist-packages (from multi_rake) (1.21.6)\n",
            "Requirement already satisfied: pyrsistent>=0.14.2 in /usr/local/lib/python3.7/dist-packages (from multi_rake) (0.18.1)\n",
            "Requirement already satisfied: pycld2>=0.41 in /usr/local/lib/python3.7/dist-packages (from multi_rake) (0.41)\n",
            "Requirement already satisfied: summa in /usr/local/lib/python3.7/dist-packages (1.2.0)\n",
            "Requirement already satisfied: scipy>=0.19 in /usr/local/lib/python3.7/dist-packages (from summa) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from scipy>=0.19->summa) (1.21.6)\n",
            "Collecting keybert\n",
            "  Downloading keybert-0.5.1.tar.gz (19 kB)\n",
            "Collecting sentence-transformers>=0.3.8\n",
            "  Downloading sentence-transformers-2.2.0.tar.gz (79 kB)\n",
            "\u001b[K     |████████████████████████████████| 79 kB 6.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scikit-learn>=0.22.2 in /usr/local/lib/python3.7/dist-packages (from keybert) (1.0.2)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.7/dist-packages (from keybert) (1.21.6)\n",
            "Collecting rich>=10.4.0\n",
            "  Downloading rich-12.2.0-py3-none-any.whl (229 kB)\n",
            "\u001b[K     |████████████████████████████████| 229 kB 49.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions<5.0,>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from rich>=10.4.0->keybert) (4.1.1)\n",
            "Collecting commonmark<0.10.0,>=0.9.0\n",
            "  Downloading commonmark-0.9.1-py2.py3-none-any.whl (51 kB)\n",
            "\u001b[K     |████████████████████████████████| 51 kB 6.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pygments<3.0.0,>=2.6.0 in /usr/local/lib/python3.7/dist-packages (from rich>=10.4.0->keybert) (2.6.1)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.22.2->keybert) (1.4.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.22.2->keybert) (3.1.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.22.2->keybert) (1.1.0)\n",
            "Collecting transformers<5.0.0,>=4.6.0\n",
            "  Downloading transformers-4.18.0-py3-none-any.whl (4.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.0 MB 46.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from sentence-transformers>=0.3.8->keybert) (4.64.0)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from sentence-transformers>=0.3.8->keybert) (1.10.0+cu111)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from sentence-transformers>=0.3.8->keybert) (0.11.1+cu111)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from sentence-transformers>=0.3.8->keybert) (3.2.5)\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 45.6 MB/s \n",
            "\u001b[?25hCollecting huggingface-hub\n",
            "  Downloading huggingface_hub-0.5.1-py3-none-any.whl (77 kB)\n",
            "\u001b[K     |████████████████████████████████| 77 kB 6.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers>=0.3.8->keybert) (3.6.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers>=0.3.8->keybert) (4.11.3)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.49-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 44.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers>=0.3.8->keybert) (21.3)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 40.5 MB/s \n",
            "\u001b[?25hCollecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
            "  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.6 MB 47.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers>=0.3.8->keybert) (2019.12.20)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers>=0.3.8->keybert) (2.23.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers<5.0.0,>=4.6.0->sentence-transformers>=0.3.8->keybert) (3.0.8)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers<5.0.0,>=4.6.0->sentence-transformers>=0.3.8->keybert) (3.8.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk->sentence-transformers>=0.3.8->keybert) (1.15.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers>=0.3.8->keybert) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers>=0.3.8->keybert) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers>=0.3.8->keybert) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers>=0.3.8->keybert) (3.0.4)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers<5.0.0,>=4.6.0->sentence-transformers>=0.3.8->keybert) (7.1.2)\n",
            "Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->sentence-transformers>=0.3.8->keybert) (7.1.2)\n",
            "Building wheels for collected packages: keybert, sentence-transformers\n",
            "  Building wheel for keybert (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keybert: filename=keybert-0.5.1-py3-none-any.whl size=21332 sha256=cb60e78a2a0b61f1b99ca6225bdd1b52f86d0a86b536733e884776ce0d2f8526\n",
            "  Stored in directory: /root/.cache/pip/wheels/8e/95/c5/f5ceed2a9f9e80bc1a706a10a6fb03d726df7a3dd11800a58b\n",
            "  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sentence-transformers: filename=sentence_transformers-2.2.0-py3-none-any.whl size=120747 sha256=3757b20d8fbe259148d792d11328c0dd4b9cc6837201116df53897ecdf8a6374\n",
            "  Stored in directory: /root/.cache/pip/wheels/83/c0/df/b6873ab7aac3f2465aa9144b6b4c41c4391cfecc027c8b07e7\n",
            "Successfully built keybert sentence-transformers\n",
            "Installing collected packages: pyyaml, tokenizers, sacremoses, huggingface-hub, transformers, sentencepiece, commonmark, sentence-transformers, rich, keybert\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed commonmark-0.9.1 huggingface-hub-0.5.1 keybert-0.5.1 pyyaml-6.0 rich-12.2.0 sacremoses-0.0.49 sentence-transformers-2.2.0 sentencepiece-0.1.96 tokenizers-0.12.1 transformers-4.18.0\n"
          ]
        }
      ],
      "source": [
        "#Importing essential libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import csv\n",
        "import json\n",
        "from itertools import islice\n",
        "from collections import OrderedDict\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import os\n",
        "import nltk\n",
        "from glob import glob\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "from tqdm import tqdm\n",
        "import pickle\n",
        "import math\n",
        "import re\n",
        "from sklearn.model_selection import train_test_split\n",
        "import operator\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('corpus')\n",
        "! pip install git+https://github.com/LIAAD/yake\n",
        "import yake\n",
        "! pip install multi_rake\n",
        "from multi_rake import Rake\n",
        "! pip install summa\n",
        "from summa import keywords as summa_keywords\n",
        "! pip install keybert\n",
        "from keybert import KeyBERT"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LMXxc7Iyuzo-",
        "outputId": "4edf929f-feaa-4d65-8b40-e8819553b36d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kpEZE5B5u3e1",
        "outputId": "193608a7-d65c-40dd-918b-75ab2925b722"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "file1 = open(\"/content/drive/MyDrive/Tweelink_Dataset/twitter_base_preprocessed.pkl\", \"rb\")\n",
        "df = pickle.load(file1)\n",
        "file1.close()"
      ],
      "metadata": {
        "id": "N5elrkQT9Kd7"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 473
        },
        "id": "De2urWFZ9bzf",
        "outputId": "c77489a8-e11f-41e2-a2d5-b0e8832ea73e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "               author id                 created_at geo country country_code  \\\n",
              "0           1952510090.0  2022-02-14 00:11:44+00:00                            \n",
              "1             20453105.0  2022-02-14 00:04:56+00:00                            \n",
              "2  1492660754998247424.0  2022-02-14 00:39:55+00:00                            \n",
              "3            117812637.0  2022-02-14 00:20:05+00:00                            \n",
              "4  1490044524604928000.0  2022-02-14 00:10:26+00:00                            \n",
              "\n",
              "  place_full_name place_name place_type                     id lang  ...  \\\n",
              "0                                        1493015060943454208.0   en  ...   \n",
              "1                                        1493013352938934272.0   en  ...   \n",
              "2                                        1493022155239534336.0   en  ...   \n",
              "3                                        1493017163485044736.0   en  ...   \n",
              "4                                        1493014734903382016.0   en  ...   \n",
              "\n",
              "                source                                              tweet  \\\n",
              "0  Twitter for Android  ð¥ð¥ð¥ð¥ð¥ð¥ð¥â¤µï¸â¤µï¸â¤µï¸...   \n",
              "1  Twitter for Android  It's too bad these guys are afraid of needles,...   \n",
              "2  Twitter for Android  Cowboy dressed as #Furries now available at th...   \n",
              "3   Twitter for iPhone  We blocked these trucks from entering the dntn...   \n",
              "4   Twitter for iPhone  Krista is very pleased with how the @RCMPONT r...   \n",
              "\n",
              "                                            hashtags sensitive  \\\n",
              "0  CommonSenseGunLaws,GunControlNow,GunSafes,GunS...     False   \n",
              "1          ClownConvoy,FreeDumbConvoy,OttawaOccupied     False   \n",
              "2                    Furries,RamRanch,OttawaOccupied     False   \n",
              "3                           Riverside,OttawaOccupied     False   \n",
              "4  FluTruxKlanGoHome,OttawaOccupied,kkkonvoy,Otta...     False   \n",
              "\n",
              "                      urls context_text context_probability context_type  \\\n",
              "0  https://t.co/LjkEup24Dk                              0.0                \n",
              "1  https://t.co/MmFzuFjIDR                              0.0                \n",
              "2  https://t.co/GBuBCUtpXe                              0.0                \n",
              "3  https://t.co/KzLkBZvjgD                              0.0                \n",
              "4  https://t.co/kAm5KNugdA                              0.0                \n",
              "\n",
              "                                   Preprocessed_Data   Date_Only  \n",
              "0  [ð¥ð¥ð¥ð¥ð¥ð¥ð¥â¤µï¸â¤µï¸â¤µï¸...  2022-02-14  \n",
              "1  [bad, guy, afraid, needle, twinrix, would, pre...  2022-02-14  \n",
              "2  [cowboy, dressed, furries, available, ramranch...  2022-02-14  \n",
              "3  [blocked, truck, entering, dntn, core, riversi...  2022-02-14  \n",
              "4  [krista, pleased, rcmpont, responded, maskless...  2022-02-14  \n",
              "\n",
              "[5 rows x 24 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-c88e5e7c-83f9-499b-8cf6-3cb1a682695b\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>author id</th>\n",
              "      <th>created_at</th>\n",
              "      <th>geo</th>\n",
              "      <th>country</th>\n",
              "      <th>country_code</th>\n",
              "      <th>place_full_name</th>\n",
              "      <th>place_name</th>\n",
              "      <th>place_type</th>\n",
              "      <th>id</th>\n",
              "      <th>lang</th>\n",
              "      <th>...</th>\n",
              "      <th>source</th>\n",
              "      <th>tweet</th>\n",
              "      <th>hashtags</th>\n",
              "      <th>sensitive</th>\n",
              "      <th>urls</th>\n",
              "      <th>context_text</th>\n",
              "      <th>context_probability</th>\n",
              "      <th>context_type</th>\n",
              "      <th>Preprocessed_Data</th>\n",
              "      <th>Date_Only</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1952510090.0</td>\n",
              "      <td>2022-02-14 00:11:44+00:00</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>1493015060943454208.0</td>\n",
              "      <td>en</td>\n",
              "      <td>...</td>\n",
              "      <td>Twitter for Android</td>\n",
              "      <td>ð¥ð¥ð¥ð¥ð¥ð¥ð¥â¤µï¸â¤µï¸â¤µï¸...</td>\n",
              "      <td>CommonSenseGunLaws,GunControlNow,GunSafes,GunS...</td>\n",
              "      <td>False</td>\n",
              "      <td>https://t.co/LjkEup24Dk</td>\n",
              "      <td></td>\n",
              "      <td>0.0</td>\n",
              "      <td></td>\n",
              "      <td>[ð¥ð¥ð¥ð¥ð¥ð¥ð¥â¤µï¸â¤µï¸â¤µï¸...</td>\n",
              "      <td>2022-02-14</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>20453105.0</td>\n",
              "      <td>2022-02-14 00:04:56+00:00</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>1493013352938934272.0</td>\n",
              "      <td>en</td>\n",
              "      <td>...</td>\n",
              "      <td>Twitter for Android</td>\n",
              "      <td>It's too bad these guys are afraid of needles,...</td>\n",
              "      <td>ClownConvoy,FreeDumbConvoy,OttawaOccupied</td>\n",
              "      <td>False</td>\n",
              "      <td>https://t.co/MmFzuFjIDR</td>\n",
              "      <td></td>\n",
              "      <td>0.0</td>\n",
              "      <td></td>\n",
              "      <td>[bad, guy, afraid, needle, twinrix, would, pre...</td>\n",
              "      <td>2022-02-14</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1492660754998247424.0</td>\n",
              "      <td>2022-02-14 00:39:55+00:00</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>1493022155239534336.0</td>\n",
              "      <td>en</td>\n",
              "      <td>...</td>\n",
              "      <td>Twitter for Android</td>\n",
              "      <td>Cowboy dressed as #Furries now available at th...</td>\n",
              "      <td>Furries,RamRanch,OttawaOccupied</td>\n",
              "      <td>False</td>\n",
              "      <td>https://t.co/GBuBCUtpXe</td>\n",
              "      <td></td>\n",
              "      <td>0.0</td>\n",
              "      <td></td>\n",
              "      <td>[cowboy, dressed, furries, available, ramranch...</td>\n",
              "      <td>2022-02-14</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>117812637.0</td>\n",
              "      <td>2022-02-14 00:20:05+00:00</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>1493017163485044736.0</td>\n",
              "      <td>en</td>\n",
              "      <td>...</td>\n",
              "      <td>Twitter for iPhone</td>\n",
              "      <td>We blocked these trucks from entering the dntn...</td>\n",
              "      <td>Riverside,OttawaOccupied</td>\n",
              "      <td>False</td>\n",
              "      <td>https://t.co/KzLkBZvjgD</td>\n",
              "      <td></td>\n",
              "      <td>0.0</td>\n",
              "      <td></td>\n",
              "      <td>[blocked, truck, entering, dntn, core, riversi...</td>\n",
              "      <td>2022-02-14</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1490044524604928000.0</td>\n",
              "      <td>2022-02-14 00:10:26+00:00</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>1493014734903382016.0</td>\n",
              "      <td>en</td>\n",
              "      <td>...</td>\n",
              "      <td>Twitter for iPhone</td>\n",
              "      <td>Krista is very pleased with how the @RCMPONT r...</td>\n",
              "      <td>FluTruxKlanGoHome,OttawaOccupied,kkkonvoy,Otta...</td>\n",
              "      <td>False</td>\n",
              "      <td>https://t.co/kAm5KNugdA</td>\n",
              "      <td></td>\n",
              "      <td>0.0</td>\n",
              "      <td></td>\n",
              "      <td>[krista, pleased, rcmpont, responded, maskless...</td>\n",
              "      <td>2022-02-14</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 24 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c88e5e7c-83f9-499b-8cf6-3cb1a682695b')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-c88e5e7c-83f9-499b-8cf6-3cb1a682695b button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-c88e5e7c-83f9-499b-8cf6-3cb1a682695b');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "u_base_hashtag = input(\"Enter base hashtag: \")\n",
        "u_time = input(\"Enter time: \")\n",
        "u_location = input(\"Enter Location: \")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OiYHMMEyx1_z",
        "outputId": "8e6f107a-510b-4b80-b617-2169e477560c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter base hashtag: hijab\n",
            "Enter time: 2022-02-19\n",
            "Enter Location: india\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import datetime\n",
        "tweet_query = []\n",
        "format = '%Y-%m-%d'\n",
        "u_present_date = datetime.datetime.strptime(u_time, format)\n",
        "u_prev_date = u_present_date - datetime.timedelta(days=1)\n",
        "u_next_date = u_present_date + datetime.timedelta(days=1)\n",
        "df_query = df.loc[df['hashtags'].str.contains(u_base_hashtag) & df['Date_Only'].isin([str(u_present_date.date()), str(u_prev_date.date()), str(u_next_date.date())])]"
      ],
      "metadata": {
        "id": "Fnm3hyL-yBf7"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_query.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 577
        },
        "id": "juQWfYz8H0I3",
        "outputId": "2ba58ce2-534c-4343-9a37-c47eba2cb256"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                 author id                 created_at               geo  \\\n",
              "12546          123851279.0  2022-02-19 12:59:42+00:00                     \n",
              "8192            3187630801  2022-02-20 00:55:03+00:00                     \n",
              "8198            2443976946  2022-02-20 00:14:25+00:00                     \n",
              "8278   1483569403425959939  2022-02-20 01:06:11+00:00  18810aa5b43e76c7   \n",
              "8280              18931596  2022-02-20 01:55:42+00:00                     \n",
              "\n",
              "                country country_code place_full_name place_name place_type  \\\n",
              "12546                                                                        \n",
              "8192                                                                         \n",
              "8198                                                                         \n",
              "8278   Verenigde Staten           US      Dallas, TX     Dallas       city   \n",
              "8280                                                                         \n",
              "\n",
              "                          id lang  ...               source  \\\n",
              "12546  1495020266862174208.0   en  ...   Twitter for iPhone   \n",
              "8192     1495200290210910209   en  ...  Twitter for Android   \n",
              "8198     1495190066112651265   en  ...      Twitter Web App   \n",
              "8278     1495203094224904192   en  ...   Twitter for iPhone   \n",
              "8280     1495215554508075011   en  ...      Twitter Web App   \n",
              "\n",
              "                                                   tweet  \\\n",
              "12546  They are so desperate they have let loose #Yat...   \n",
              "8192   No Woman is Forced to wear and Not to wear Som...   \n",
              "8198   Protests in US cities against #Karnataka #hija...   \n",
              "8278   Protest by local Dallas Muslims at JFK square ...   \n",
              "8280   Feel sorry for #zahirawasim.  Religion must ma...   \n",
              "\n",
              "                                                hashtags sensitive  \\\n",
              "12546             YatiNarsinghanand,PlotToKillModi,hijab     False   \n",
              "8192   hijab,HijabIsOurPride,HijabIsFundamentalRight,...     False   \n",
              "8198                    Karnataka,hijab,Islamophobic,BJP     False   \n",
              "8278                                               hijab     False   \n",
              "8280                                   zahirawasim,hijab     False   \n",
              "\n",
              "                                                    urls context_text  \\\n",
              "12546                                                                   \n",
              "8192                                                                    \n",
              "8198   https://t.co/957kBM8AJT,https://t.co/5gY741DZP...                \n",
              "8278                             https://t.co/KCm413Yb8N                \n",
              "8280                                                                    \n",
              "\n",
              "      context_probability context_type  \\\n",
              "12546                 0.0                \n",
              "8192                  0.0                \n",
              "8198                  0.0                \n",
              "8278                  0.0                \n",
              "8280                  0.0                \n",
              "\n",
              "                                       Preprocessed_Data   Date_Only  \n",
              "12546  [desperate, let, loose, yatinarsinghanand, vio...  2022-02-19  \n",
              "8192   [woman, forced, wear, wear, something, hijab, ...  2022-02-20  \n",
              "8198   [protest, u, city, karnataka, hijab, ban, indi...  2022-02-20  \n",
              "8278   [protest, local, dallas, muslim, jfk, square, ...  2022-02-20  \n",
              "8280   [feel, sorry, zahirawasim, religion, must, mak...  2022-02-20  \n",
              "\n",
              "[5 rows x 24 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-2e2db11b-bd73-4211-bb0e-aa460ad8729a\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>author id</th>\n",
              "      <th>created_at</th>\n",
              "      <th>geo</th>\n",
              "      <th>country</th>\n",
              "      <th>country_code</th>\n",
              "      <th>place_full_name</th>\n",
              "      <th>place_name</th>\n",
              "      <th>place_type</th>\n",
              "      <th>id</th>\n",
              "      <th>lang</th>\n",
              "      <th>...</th>\n",
              "      <th>source</th>\n",
              "      <th>tweet</th>\n",
              "      <th>hashtags</th>\n",
              "      <th>sensitive</th>\n",
              "      <th>urls</th>\n",
              "      <th>context_text</th>\n",
              "      <th>context_probability</th>\n",
              "      <th>context_type</th>\n",
              "      <th>Preprocessed_Data</th>\n",
              "      <th>Date_Only</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>12546</th>\n",
              "      <td>123851279.0</td>\n",
              "      <td>2022-02-19 12:59:42+00:00</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>1495020266862174208.0</td>\n",
              "      <td>en</td>\n",
              "      <td>...</td>\n",
              "      <td>Twitter for iPhone</td>\n",
              "      <td>They are so desperate they have let loose #Yat...</td>\n",
              "      <td>YatiNarsinghanand,PlotToKillModi,hijab</td>\n",
              "      <td>False</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>0.0</td>\n",
              "      <td></td>\n",
              "      <td>[desperate, let, loose, yatinarsinghanand, vio...</td>\n",
              "      <td>2022-02-19</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8192</th>\n",
              "      <td>3187630801</td>\n",
              "      <td>2022-02-20 00:55:03+00:00</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>1495200290210910209</td>\n",
              "      <td>en</td>\n",
              "      <td>...</td>\n",
              "      <td>Twitter for Android</td>\n",
              "      <td>No Woman is Forced to wear and Not to wear Som...</td>\n",
              "      <td>hijab,HijabIsOurPride,HijabIsFundamentalRight,...</td>\n",
              "      <td>False</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>0.0</td>\n",
              "      <td></td>\n",
              "      <td>[woman, forced, wear, wear, something, hijab, ...</td>\n",
              "      <td>2022-02-20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8198</th>\n",
              "      <td>2443976946</td>\n",
              "      <td>2022-02-20 00:14:25+00:00</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>1495190066112651265</td>\n",
              "      <td>en</td>\n",
              "      <td>...</td>\n",
              "      <td>Twitter Web App</td>\n",
              "      <td>Protests in US cities against #Karnataka #hija...</td>\n",
              "      <td>Karnataka,hijab,Islamophobic,BJP</td>\n",
              "      <td>False</td>\n",
              "      <td>https://t.co/957kBM8AJT,https://t.co/5gY741DZP...</td>\n",
              "      <td></td>\n",
              "      <td>0.0</td>\n",
              "      <td></td>\n",
              "      <td>[protest, u, city, karnataka, hijab, ban, indi...</td>\n",
              "      <td>2022-02-20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8278</th>\n",
              "      <td>1483569403425959939</td>\n",
              "      <td>2022-02-20 01:06:11+00:00</td>\n",
              "      <td>18810aa5b43e76c7</td>\n",
              "      <td>Verenigde Staten</td>\n",
              "      <td>US</td>\n",
              "      <td>Dallas, TX</td>\n",
              "      <td>Dallas</td>\n",
              "      <td>city</td>\n",
              "      <td>1495203094224904192</td>\n",
              "      <td>en</td>\n",
              "      <td>...</td>\n",
              "      <td>Twitter for iPhone</td>\n",
              "      <td>Protest by local Dallas Muslims at JFK square ...</td>\n",
              "      <td>hijab</td>\n",
              "      <td>False</td>\n",
              "      <td>https://t.co/KCm413Yb8N</td>\n",
              "      <td></td>\n",
              "      <td>0.0</td>\n",
              "      <td></td>\n",
              "      <td>[protest, local, dallas, muslim, jfk, square, ...</td>\n",
              "      <td>2022-02-20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8280</th>\n",
              "      <td>18931596</td>\n",
              "      <td>2022-02-20 01:55:42+00:00</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>1495215554508075011</td>\n",
              "      <td>en</td>\n",
              "      <td>...</td>\n",
              "      <td>Twitter Web App</td>\n",
              "      <td>Feel sorry for #zahirawasim.  Religion must ma...</td>\n",
              "      <td>zahirawasim,hijab</td>\n",
              "      <td>False</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>0.0</td>\n",
              "      <td></td>\n",
              "      <td>[feel, sorry, zahirawasim, religion, must, mak...</td>\n",
              "      <td>2022-02-20</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 24 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2e2db11b-bd73-4211-bb0e-aa460ad8729a')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-2e2db11b-bd73-4211-bb0e-aa460ad8729a button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-2e2db11b-bd73-4211-bb0e-aa460ad8729a');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def keyword_extractor(dataset):\n",
        "  preprocessed_vocabulary = dict()\n",
        "\n",
        "  #Converting to lowercase\n",
        "  def to_lower_case(text):\n",
        "    text = text.lower()\n",
        "    return text\n",
        "\n",
        "  def remove_at_word(text):\n",
        "    data = text.split()\n",
        "    data = [d for d in data if d[0]!='@']\n",
        "    text = ' '.join(data)\n",
        "    return text\n",
        "\n",
        "  def remove_hashtag(text):\n",
        "    data = text.split()\n",
        "    data = [d if (d[0]!='#' or len(d) == 1) else d[1:] for d in data]\n",
        "    data = [d for d in data if d[0]!='#']\n",
        "    text = ' '.join(data)\n",
        "    return text\n",
        "\n",
        "  def remove_URL(text):\n",
        "    text = re.sub(r\"http\\S+\", \"\", text)\n",
        "    text = re.sub(r'bit.ly\\S+', '', text, flags=re.MULTILINE)\n",
        "    return text\n",
        "\n",
        "  #Removing stopwords\n",
        "  def remove_stopwords(text):\n",
        "    stopword = stopwords.words('english')\n",
        "    new_list = [x for x in text.split() if x not in stopword]\n",
        "    return ' '.join(new_list)\n",
        "\n",
        "  #Removing punctuations\n",
        "  def remove_punctuations(text):\n",
        "    punctuations = '''!()-[|]`{};:'\"\\,<>./?@#$=+%^&*_~'''\n",
        "    new_list = ['' if x in punctuations else x for x in text.split()]\n",
        "    new_list_final = []\n",
        "    for token in new_list:\n",
        "      new_token=\"\"\n",
        "      for char in token:\n",
        "        if(char not in punctuations):\n",
        "          new_token+=char\n",
        "      if(len(new_token)!=0):\n",
        "        new_list_final.append(new_token)\n",
        "    return ' '.join(new_list_final)\n",
        "\n",
        "  #Tokenization\n",
        "  def tokenization(text):\n",
        "    return word_tokenize(text)\n",
        "\n",
        "  def pre_process(text):\n",
        "    text = to_lower_case(text)\n",
        "    text = remove_at_word(text)\n",
        "    text = remove_hashtag(text)\n",
        "    text = remove_URL(text)\n",
        "    text = remove_stopwords(text)\n",
        "    text = remove_punctuations(text)\n",
        "    text = tokenization(text)\n",
        "    for token in text:\n",
        "      if token in preprocessed_vocabulary.keys():\n",
        "        preprocessed_vocabulary[token] += 1\n",
        "      else:\n",
        "        preprocessed_vocabulary[token] = 1\n",
        "    return text\n",
        "  \n",
        "  preprocessed_data = [pre_process(text) for text in dataset]\n",
        "\n",
        "  #print(preprocessed_vocabulary)\n",
        "\n",
        "  AOF_coefficient = sum(preprocessed_vocabulary.values())/len(preprocessed_vocabulary)\n",
        "  vocabulary = {token.strip():preprocessed_vocabulary[token] for token in preprocessed_vocabulary.keys() if preprocessed_vocabulary[token] > AOF_coefficient and len(token.strip())}\n",
        "\n",
        "  #print(vocabulary)\n",
        "\n",
        "  final_tokens_per_tweet = []\n",
        "  for data in preprocessed_data:\n",
        "    final_tokens_per_tweet.append([token for token in data if token in vocabulary.keys()])\n",
        "\n",
        "  #print(preprocessed_data)\n",
        "  #print(final_tokens_per_tweet)\n",
        "\n",
        "  word2id = dict()\n",
        "  id2word = dict()\n",
        "  vocabulary_size = len(vocabulary)\n",
        "  count = 0\n",
        "  for token in vocabulary.keys():\n",
        "    word2id[token] = count\n",
        "    id2word[count] = token\n",
        "    count += 1\n",
        "\n",
        "  #print(word2id)\n",
        "  #print(id2word)\n",
        "\n",
        "  directed_graph_adjacency_matrix = np.zeros((vocabulary_size, vocabulary_size))\n",
        "  edge_weight_matrix = np.zeros((vocabulary_size, vocabulary_size))\n",
        "  first_frequency = dict()\n",
        "  last_frequency = dict()\n",
        "  term_frequency = vocabulary\n",
        "  strength = dict()\n",
        "  degree = dict()\n",
        "  selective_centraility = dict()\n",
        "\n",
        "\n",
        "  for tweet in final_tokens_per_tweet:\n",
        "\n",
        "    if tweet[0] in first_frequency.keys():\n",
        "      first_frequency[tweet[0]] += 1\n",
        "    else:\n",
        "      first_frequency[tweet[0]] = 1\n",
        "\n",
        "    if tweet[-1] in last_frequency.keys():\n",
        "      last_frequency[tweet[-1]] += 1\n",
        "    else:\n",
        "      last_frequency[tweet[-1]] = 1\n",
        "    \n",
        "\n",
        "\n",
        "    for i in range(len(tweet)-1):\n",
        "      if tweet[i] == tweet[i+1]:\n",
        "        continue\n",
        "      x = word2id[tweet[i]]\n",
        "      y = word2id[tweet[i+1]]\n",
        "      directed_graph_adjacency_matrix[x][y] += 1\n",
        "\n",
        "  for tweet in final_tokens_per_tweet:\n",
        "    for i in range(len(tweet)-1):\n",
        "\n",
        "\n",
        "      if tweet[i] == tweet[i+1]:\n",
        "        continue\n",
        "      x = word2id[tweet[i]]\n",
        "      y = word2id[tweet[i+1]]\n",
        "\n",
        "    # Updating degree..\n",
        "      if tweet[i] in degree.keys():\n",
        "        degree[tweet[i]] += 1\n",
        "      else:\n",
        "        degree[tweet[i]] = 1\n",
        "        \n",
        "      if tweet[i+1] in degree.keys():\n",
        "        degree[tweet[i+1]] += 1\n",
        "      else:\n",
        "        degree[tweet[i+1]] = 1\n",
        "\n",
        "      edge_weight_matrix[x][y] = directed_graph_adjacency_matrix[x][y]/(vocabulary[tweet[i]] + vocabulary[tweet[i+1]] - directed_graph_adjacency_matrix[x][y])\n",
        "\n",
        "      if tweet[i] in strength.keys():\n",
        "        strength[tweet[i]] += edge_weight_matrix[x][y]\n",
        "      else:\n",
        "        strength[tweet[i]] = edge_weight_matrix[x][y]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  first_frequency = {token:(first_frequency[token]/vocabulary[token] if token in first_frequency else 0) for token in vocabulary.keys()}\n",
        "  last_frequency = {token:(last_frequency[token]/vocabulary[token] if token in last_frequency else 0) for token in vocabulary.keys()}\n",
        "  degree = {token:(degree[token] if token in degree else 0) for token in vocabulary.keys()}\n",
        "  strength = {token:(strength[token] if token in strength else 0) for token in vocabulary.keys()}\n",
        "  selective_centraility = {token:(strength[token]/degree[token] if degree[token]!=0 else 0) for token in vocabulary.keys()}\n",
        "\n",
        "  #print(degree)\n",
        "  #print(vocabulary)\n",
        "\n",
        "  maxdegree = max(degree.items(), key=lambda x: x[1])[1]\n",
        "  max_degree_nodes_with_freq = {key:term_frequency[key] for key in degree.keys() if degree[key] == maxdegree}\n",
        "  maxfreq = max(max_degree_nodes_with_freq.items(), key=lambda x: x[1])[1]\n",
        "  central_node_name = [key for key in max_degree_nodes_with_freq.keys() if max_degree_nodes_with_freq[key] == maxfreq][0]\n",
        "  #print(\"central node: \", central_node_name)\n",
        "\n",
        "  # bfs\n",
        "  distance_from_central_node = dict()\n",
        "  central_node_id = word2id[central_node_name]\n",
        "  q = [(central_node_id, 0)]\n",
        "\n",
        "  # Set source as visited\n",
        "  distance_from_central_node[central_node_name] = 0\n",
        "\n",
        "  while q:\n",
        "      vis = q[0]\n",
        "      # Print current node\n",
        "      #print(id2word[vis[0]], vis[1])\n",
        "      q.pop(0)\n",
        "        \n",
        "      # For every adjacent vertex to\n",
        "      # the current vertex\n",
        "      for i in range(len(directed_graph_adjacency_matrix[vis[0]])):\n",
        "          if (directed_graph_adjacency_matrix[vis[0]][i] == 1 and (id2word[i] not in distance_from_central_node.keys())):\n",
        "              # Push the adjacent node\n",
        "              # in the queue\n",
        "              q.append((i, vis[1]+1))\n",
        "              distance_from_central_node[id2word[i]] = vis[1]+1\n",
        "\n",
        "  #print(distance_from_central_node)\n",
        "  inverse_distance_from_central_node = {token:(1/distance_from_central_node[token] if token in distance_from_central_node and token != central_node_name else 0) for token in vocabulary.keys()}\n",
        "  inverse_distance_from_central_node[central_node_name] = 1.0\n",
        "  #print(inverse_distance_from_central_node)\n",
        "\n",
        "  neighbour_importance = dict()\n",
        "\n",
        "  for i in range(len(directed_graph_adjacency_matrix)):\n",
        "    neighbours = set()\n",
        "\n",
        "    # traversing outgoing edges\n",
        "    for j in range(len(directed_graph_adjacency_matrix)):\n",
        "      if i == j:\n",
        "        continue\n",
        "      if directed_graph_adjacency_matrix[i][j] > 0:\n",
        "        neighbours.add(j)\n",
        "    for j in range(len(directed_graph_adjacency_matrix)):\n",
        "      if i == j:\n",
        "        continue\n",
        "      if directed_graph_adjacency_matrix[j][i] > 0:\n",
        "          neighbours.add(j)\n",
        "    if len(neighbours) != 0:\n",
        "      neighbour_importance[id2word[i]] = sum([strength[id2word[j]] for j in neighbours])/len(neighbours)\n",
        "    else:\n",
        "      neighbour_importance[id2word[i]] = 0\n",
        "      \n",
        "  #print(neighbour_importance)\n",
        "\n",
        "  unnormalized_node_weight = {node: (first_frequency[node] + last_frequency[node] + term_frequency[node] + selective_centraility[node] + inverse_distance_from_central_node[node] + neighbour_importance[node]) for node in vocabulary.keys()}\n",
        "  max_node_weight = max(unnormalized_node_weight.items(), key=lambda x: x[1])[1]\n",
        "  min_node_weight = min(unnormalized_node_weight.items(), key=lambda x: x[1])[1]\n",
        "  #print(\"max node weight: \", max_node_weight, \"min node weight: \", min_node_weight)\n",
        "  normalized_node_weight = {node: ((unnormalized_node_weight[node] - min_node_weight)/(max_node_weight - min_node_weight) if max_node_weight != min_node_weight else unnormalized_node_weight[node]) for node in unnormalized_node_weight.keys()}\n",
        "  #print(\"Unnormalized score: \", unnormalized_node_weight)\n",
        "  #print(\"Normalized score: \", normalized_node_weight)\n",
        "\n",
        "  damping_factor = 0.85\n",
        "  relevance_of_node = {node: np.random.uniform(0,1,1)[0] for node in vocabulary.keys()}\n",
        "  threshold = 0.000000001\n",
        "\n",
        "\n",
        "  #print(relevance_of_node)\n",
        "\n",
        "  count = 0\n",
        "  while True:\n",
        "    count += 1\n",
        "    current_relevance_of_node = dict()\n",
        "    for node in vocabulary.keys():\n",
        "      outer_sum = 0\n",
        "      node_idx = word2id[node]\n",
        "      for j in range(len(directed_graph_adjacency_matrix)):\n",
        "        if j == node_idx:\n",
        "          continue\n",
        "        if directed_graph_adjacency_matrix[j][node_idx] > 0:\n",
        "          den_sum = 0\n",
        "          for k in range(len(directed_graph_adjacency_matrix)):\n",
        "            if k == j:\n",
        "              continue\n",
        "            den_sum += directed_graph_adjacency_matrix[j][k]\n",
        "          outer_sum += ((directed_graph_adjacency_matrix[j][node_idx]/den_sum) * relevance_of_node[id2word[j]])\n",
        "      current_relevance_of_node[node] = (1-damping_factor)*normalized_node_weight[node] + damping_factor*normalized_node_weight[node]*outer_sum\n",
        "    \n",
        "\n",
        "    # checking convergence..\n",
        "    sq_error = sum([(current_relevance_of_node[node] - relevance_of_node[node])**2 for node in vocabulary.keys()])\n",
        "    relevance_of_node = current_relevance_of_node\n",
        "    if sq_error < threshold:\n",
        "      break\n",
        "\n",
        "  #print(relevance_of_node)\n",
        "  #print(count)\n",
        "\n",
        "  degree_centrality  = {node: 0 for node in vocabulary.keys()}\n",
        "\n",
        "  if len(directed_graph_adjacency_matrix) > 1:\n",
        "    for i in range(len(directed_graph_adjacency_matrix)):\n",
        "      count = 0\n",
        "      for j in range(len(directed_graph_adjacency_matrix)):\n",
        "        if i == j:\n",
        "          continue\n",
        "        if directed_graph_adjacency_matrix[j][i] > 0:\n",
        "          count += 1\n",
        "      degree_centrality[id2word[i]] = count / (len(directed_graph_adjacency_matrix)-1)\n",
        "\n",
        "  #print(degree_centrality)\n",
        "\n",
        "  final_keyword_rank = [{'node': node, 'NE_rank': relevance_of_node[node], 'Degree': degree_centrality[node]} for node in vocabulary.keys()]\n",
        "\n",
        "  #print(\"-----------\")\n",
        "  final_keyword_rank = sorted(final_keyword_rank, key = lambda i: (i['NE_rank'], i['Degree']), reverse = True)\n",
        "\n",
        "  final_keywords = [keyword['node'] for keyword in final_keyword_rank]\n",
        "\n",
        "  return final_keywords"
      ],
      "metadata": {
        "id": "mhKWoMzBadTG"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for tweet in df_query['Preprocessed_Data']:\n",
        "  tweet_query.extend(tweet)"
      ],
      "metadata": {
        "id": "0GzM1RBI_dNr"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tweet_query"
      ],
      "metadata": {
        "id": "mivIpXBG_6c_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d3cc002-fb27-4a6c-86aa-7cd83ba47515"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['desperate',\n",
              " 'let',\n",
              " 'loose',\n",
              " 'yatinarsinghanand',\n",
              " 'violent',\n",
              " 'rhetoric',\n",
              " 'plottokillmodi',\n",
              " 'hijab',\n",
              " 'hate',\n",
              " 'hateâ\\x80¦',\n",
              " 'yatinarsinghanand',\n",
              " 'plottokillmodi',\n",
              " 'hijab',\n",
              " 'woman',\n",
              " 'forced',\n",
              " 'wear',\n",
              " 'wear',\n",
              " 'something',\n",
              " 'hijab',\n",
              " 'hijabisourpride',\n",
              " 'hijabisfundamentalright',\n",
              " 'hijaboruniform',\n",
              " 'hijabcontroversy',\n",
              " 'hijabcircular',\n",
              " 'hijabisindividualright',\n",
              " 'ð\\x9f§\\x95',\n",
              " 'hijab',\n",
              " 'hijabisourpride',\n",
              " 'hijabisfundamentalright',\n",
              " 'hijaboruniform',\n",
              " 'hijabcontroversy',\n",
              " 'hijabcircular',\n",
              " 'hijabisindividualright',\n",
              " 'protest',\n",
              " 'u',\n",
              " 'city',\n",
              " 'karnataka',\n",
              " 'hijab',\n",
              " 'ban',\n",
              " 'indian',\n",
              " 'american',\n",
              " 'ally',\n",
              " 'demonstrated',\n",
              " 'islamophobic',\n",
              " 'unconstitutional',\n",
              " 'ban',\n",
              " 'school',\n",
              " 'india',\n",
              " 's',\n",
              " 'nationalist',\n",
              " 'bjp',\n",
              " 'government',\n",
              " 'take',\n",
              " 'action',\n",
              " 'canadaâ\\x9e¡ï¸\\x8f',\n",
              " 'http',\n",
              " 'tco957kbm8ajt',\n",
              " 'take',\n",
              " 'action',\n",
              " 'globallyâ\\x9e¡ï¸\\x8f',\n",
              " 'http',\n",
              " 'tco5gy741dzp1',\n",
              " 'http',\n",
              " 'tcobvsjhjohkc',\n",
              " 'karnataka',\n",
              " 'hijab',\n",
              " 'islamophobic',\n",
              " 'bjp',\n",
              " 'protest',\n",
              " 'local',\n",
              " 'dallas',\n",
              " 'muslim',\n",
              " 'jfk',\n",
              " 'square',\n",
              " 'downtown',\n",
              " 'draconian',\n",
              " 'hijab',\n",
              " 'law',\n",
              " 'today',\n",
              " 'entire',\n",
              " 'u',\n",
              " 'listening',\n",
              " 'voice',\n",
              " 'muslim',\n",
              " 'woman',\n",
              " 'thelivetvnews',\n",
              " 'http',\n",
              " 'tcokcm413yb8n',\n",
              " 'hijab',\n",
              " 'dallas',\n",
              " 'tx',\n",
              " 'feel',\n",
              " 'sorry',\n",
              " 'zahirawasim',\n",
              " 'religion',\n",
              " 'must',\n",
              " 'make',\n",
              " 'ppl',\n",
              " 'grow',\n",
              " 'make',\n",
              " 'ppl',\n",
              " 'regressive',\n",
              " 'time',\n",
              " 'reform',\n",
              " 'dear',\n",
              " 'zahirawasim',\n",
              " 'aisha',\n",
              " 'fought',\n",
              " 'ali',\n",
              " 'war',\n",
              " 'wearing',\n",
              " 'hijab',\n",
              " 'u',\n",
              " 'think',\n",
              " 'u',\n",
              " 'know',\n",
              " 'religion',\n",
              " 'aisha',\n",
              " 'stop',\n",
              " 'spreading',\n",
              " 'myth',\n",
              " 'ur',\n",
              " 'faith',\n",
              " 'hijab',\n",
              " 'obligation',\n",
              " 'zahirawasim',\n",
              " 'hijab',\n",
              " 'good',\n",
              " 'sunday',\n",
              " 'ð\\x9f\\x98\\x8e',\n",
              " 'tour',\n",
              " 'castle',\n",
              " 'click',\n",
              " 'link',\n",
              " 'bio',\n",
              " 'humairasquare',\n",
              " 'dropshipdiperlukan',\n",
              " 'dropshipmalaysia',\n",
              " 'hijab',\n",
              " 'love',\n",
              " 'bawalcottonvoile',\n",
              " 'tudungbawal',\n",
              " 'tudungbawalkayangan',\n",
              " 'ootd',\n",
              " 'http',\n",
              " 'tconjhzlywwvg',\n",
              " 'humairasquare',\n",
              " 'dropshipdiperlukan',\n",
              " 'dropshipmalaysia',\n",
              " 'hijab',\n",
              " 'love',\n",
              " 'bawalcottonvoile',\n",
              " 'tudungbawal',\n",
              " 'tudungbawalkayangan',\n",
              " 'ootd',\n",
              " 'illeberasim',\n",
              " 'school',\n",
              " 'uniform',\n",
              " 'fr',\n",
              " 'everyone',\n",
              " 'ban',\n",
              " 'anythingonly',\n",
              " '5',\n",
              " 'girl',\n",
              " 'suddenly',\n",
              " 'wanted',\n",
              " 'hijabcanada',\n",
              " 'using',\n",
              " 'emergency',\n",
              " '2',\n",
              " 'counter',\n",
              " 'canadafreedomconvoycan',\n",
              " 'anyone',\n",
              " 'dare',\n",
              " 'criticize',\n",
              " 'blm',\n",
              " 'amp',\n",
              " 'risk',\n",
              " 'loosing',\n",
              " 'job',\n",
              " 'kind',\n",
              " 'free',\n",
              " 'speech',\n",
              " 'liberalism',\n",
              " 'avatans',\n",
              " 'http',\n",
              " 'tcozrcptpnvip',\n",
              " 'hijab',\n",
              " 'canadafreedomconvoy',\n",
              " 'blm',\n",
              " 'hijab',\n",
              " 'woman',\n",
              " 'lead',\n",
              " 'way',\n",
              " 'raffia',\n",
              " 'arshad',\n",
              " 'appointment',\n",
              " 'judge',\n",
              " 'midland',\n",
              " 'circuit',\n",
              " 'see',\n",
              " 'personal',\n",
              " 'achievement',\n",
              " 'bigger',\n",
              " 'joint',\n",
              " 'head',\n",
              " 'st',\n",
              " 'mary',\n",
              " 's',\n",
              " 'chamber',\n",
              " 'said',\n",
              " 'led',\n",
              " 'way',\n",
              " 'muslim',\n",
              " 'woman',\n",
              " 'succeed',\n",
              " 'http',\n",
              " 'tcowgezqfoojz',\n",
              " 'hijab',\n",
              " 'ambreenzaidi',\n",
              " 'woman',\n",
              " 'hijab',\n",
              " 'lead',\n",
              " 'way',\n",
              " 'hijab',\n",
              " 'never',\n",
              " 'hijab',\n",
              " 'hijabisourpride',\n",
              " 'brahmin',\n",
              " 'dalitlivesmatter',\n",
              " 'http',\n",
              " 'tconszf9vzmjy',\n",
              " 'hijab',\n",
              " 'hijabisourpride',\n",
              " 'brahmin',\n",
              " 'dalitlivesmatter',\n",
              " 'kick',\n",
              " 'footballhelps',\n",
              " 'hijabclad',\n",
              " 'girl',\n",
              " 'mumbai',\n",
              " 'breaknewground',\n",
              " 'http',\n",
              " 'tcojzz8pg0wbq',\n",
              " 'via',\n",
              " 'timesofindia',\n",
              " 'coachzakirhussainansari',\n",
              " 'started',\n",
              " 'girl',\n",
              " 'team',\n",
              " 'group',\n",
              " '20',\n",
              " '2018',\n",
              " 'footballhelps',\n",
              " 'hijab',\n",
              " 'girl',\n",
              " 'breaknewground',\n",
              " 'coachzakirhussainansari',\n",
              " 'wish',\n",
              " 'great',\n",
              " 'shahid',\n",
              " 'palni',\n",
              " 'baba',\n",
              " 'alive',\n",
              " 'hijab',\n",
              " 'issue',\n",
              " 'ongoing',\n",
              " 'karnataka',\n",
              " 'ð\\x9f\\x98\\x93ð\\x9f\\x98\\x93',\n",
              " 'may',\n",
              " 'allah',\n",
              " 'pleased',\n",
              " 'give',\n",
              " 'higher',\n",
              " 'rank',\n",
              " 'jannah',\n",
              " 'hijab',\n",
              " 'zafarsareshwala',\n",
              " 'unfortunate',\n",
              " 'set',\n",
              " 'unpleasant',\n",
              " 'precedence',\n",
              " 'next',\n",
              " 'demand',\n",
              " 'hijab',\n",
              " 'police',\n",
              " 'military',\n",
              " 'airline',\n",
              " 'show',\n",
              " 'bunch',\n",
              " 'rogue',\n",
              " 'element',\n",
              " 'arm',\n",
              " 'twist',\n",
              " 'get',\n",
              " 'unreasonable',\n",
              " 'demand',\n",
              " 'hijab',\n",
              " 'â\\x80\\x9cfatâ\\x80\\x9d',\n",
              " 'girl',\n",
              " 'post',\n",
              " 'photo',\n",
              " 'wearing',\n",
              " 'booty',\n",
              " 'short',\n",
              " 'freefromhijab',\n",
              " 'â\\x80\\x9cskinnyâ\\x80\\x9d',\n",
              " 'girl',\n",
              " 'go',\n",
              " 'school',\n",
              " 'wearing',\n",
              " 'fitted',\n",
              " 'jean',\n",
              " 'â\\x80\\x9cpromoting',\n",
              " 'anorexiaâ\\x80\\x9d',\n",
              " 'start',\n",
              " 'wearing',\n",
              " 'loose',\n",
              " 'clothes',\n",
              " 'â\\x80\\x9cpromoting',\n",
              " 'hijabâ\\x80\\x9d',\n",
              " 'skinnyshaming',\n",
              " 'freefromhijab',\n",
              " 'hijab',\n",
              " 'skinnyshaming',\n",
              " 'mindset',\n",
              " 'congressif',\n",
              " 'muslim',\n",
              " 'allowed',\n",
              " 'wear',\n",
              " 'hijab',\n",
              " 'congress',\n",
              " 'sp',\n",
              " 'said',\n",
              " 'punjabies',\n",
              " 'allowed',\n",
              " 'wear',\n",
              " 'traditional',\n",
              " 'dress',\n",
              " 'incindia',\n",
              " 'bjp4india',\n",
              " 'http',\n",
              " 'tcoxwvwwaeqtl',\n",
              " 'hijab',\n",
              " 'à¤\\x95à¥\\x87à¤°à¤²',\n",
              " 'à¤\\xadà¤¾à¤°à¤¤',\n",
              " 'educated',\n",
              " 'woman',\n",
              " 'progressive',\n",
              " 'family',\n",
              " 'freedom',\n",
              " 'education',\n",
              " 'dressing',\n",
              " 'lifestyle',\n",
              " 'want',\n",
              " 'young',\n",
              " 'girl',\n",
              " 'wear',\n",
              " 'hijab',\n",
              " 'burka',\n",
              " 'hijab',\n",
              " 'hijab',\n",
              " 'iamcouncil',\n",
              " 'khanabadosh0',\n",
              " 'huge',\n",
              " 'rally',\n",
              " 'dallas',\n",
              " 'protesting',\n",
              " 'nazi',\n",
              " 'inspired',\n",
              " 'hijab',\n",
              " 'ban',\n",
              " 'sister',\n",
              " 'clearly',\n",
              " 'made',\n",
              " 'clear',\n",
              " 'hijabisourright',\n",
              " 'http',\n",
              " 'tcobmyhvqsep4',\n",
              " 'hijab',\n",
              " 'hijabisourright',\n",
              " 'karnataka',\n",
              " 'critical',\n",
              " 'moment',\n",
              " 'wear',\n",
              " 'hijab',\n",
              " 'asked',\n",
              " 'remove',\n",
              " 'hijabisfundamentalright',\n",
              " 'hijab',\n",
              " 'hijabisfundamentalright',\n",
              " 'reposted',\n",
              " 'isabellapodesta',\n",
              " 'ð\\x9f\\x93¸',\n",
              " 'â\\x80¢',\n",
              " 'â\\x80¢',\n",
              " 'â\\x80¢',\n",
              " 'â\\x80¢',\n",
              " 'â\\x80¢',\n",
              " 'â\\x80¢',\n",
              " 'modestfashion',\n",
              " 'hijab',\n",
              " 'modestclothing',\n",
              " 'modest',\n",
              " 'revert',\n",
              " 'reverttings',\n",
              " 'fashion',\n",
              " 'hijabifashion',\n",
              " 'abaya',\n",
              " 'muslim',\n",
              " 'muslimah',\n",
              " 'http',\n",
              " 'tcocqcnehlaty',\n",
              " 'modestfashion',\n",
              " 'hijab',\n",
              " 'modestclothing',\n",
              " 'modest',\n",
              " 'revert',\n",
              " 'reverttings',\n",
              " 'fashion',\n",
              " 'hijabifashion',\n",
              " 'abaya',\n",
              " 'muslim',\n",
              " 'muslimah',\n",
              " 'woman',\n",
              " 'iran',\n",
              " 'deal',\n",
              " 'daily',\n",
              " 'basis',\n",
              " 'iran',\n",
              " 'womensrights',\n",
              " 'hijab',\n",
              " 'veil',\n",
              " 'islam',\n",
              " 'atheist',\n",
              " 'agnostic',\n",
              " 'discrimination',\n",
              " 'http',\n",
              " 'tcoksquzftfmp',\n",
              " 'iran',\n",
              " 'womensrights',\n",
              " 'hijab',\n",
              " 'veil',\n",
              " 'islam',\n",
              " 'atheist',\n",
              " 'agnostic',\n",
              " 'discrimination',\n",
              " 'yesterday',\n",
              " 'noticed',\n",
              " 'one',\n",
              " 'mobile',\n",
              " 'store',\n",
              " 'owned',\n",
              " 'pissfuls',\n",
              " 'mu',\n",
              " 'l',\n",
              " 'men',\n",
              " 'sale',\n",
              " 'executive',\n",
              " 'mu',\n",
              " 'l',\n",
              " 'woman',\n",
              " 'r',\n",
              " 'employed',\n",
              " 'pissfuls',\n",
              " 'anywhere',\n",
              " 'known',\n",
              " 'fact',\n",
              " 'reminder',\n",
              " 'want',\n",
              " 'hijab',\n",
              " 'education',\n",
              " 'bsbommai',\n",
              " 'bcnageshbjp',\n",
              " 'hijab',\n",
              " 'zairawasim',\n",
              " 'say',\n",
              " 'absolute',\n",
              " 'justice',\n",
              " 'woman',\n",
              " 'ask',\n",
              " 'give',\n",
              " 'either',\n",
              " 'education',\n",
              " 'hijab',\n",
              " 'citing',\n",
              " 'woman',\n",
              " 'empowerment',\n",
              " 'even',\n",
              " 'worst',\n",
              " 'read',\n",
              " 'full',\n",
              " 'post',\n",
              " 'hijab',\n",
              " 'row',\n",
              " 'hijab',\n",
              " 'hijabrow',\n",
              " 'http',\n",
              " 'tcomgovcwpjbr',\n",
              " 'zairawasim',\n",
              " 'hijab',\n",
              " 'hijabrow',\n",
              " 'cleaver',\n",
              " 'strategy',\n",
              " 'whole',\n",
              " 'hijab',\n",
              " 'issue',\n",
              " 'claim',\n",
              " 'want',\n",
              " 'hijab',\n",
              " 'actually',\n",
              " 'coming',\n",
              " 'burka',\n",
              " 'afghanistan',\n",
              " 'style',\n",
              " 'shah',\n",
              " 'bano',\n",
              " '20',\n",
              " 'talibanisation',\n",
              " 'http',\n",
              " 'tcoysvcewmuig',\n",
              " 'hijab',\n",
              " 'hijabcontroversy',\n",
              " 'unveils',\n",
              " 'religious',\n",
              " 'political',\n",
              " 'faultlines',\n",
              " 'india',\n",
              " 'wearing',\n",
              " 'hijab',\n",
              " 'expression',\n",
              " 'protected',\n",
              " 'article19',\n",
              " '1',\n",
              " 'constitution',\n",
              " 'guarantee',\n",
              " 'right',\n",
              " 'freedom',\n",
              " 'speech',\n",
              " 'expression',\n",
              " 'writes',\n",
              " 'ashokbhan2',\n",
              " 'http',\n",
              " 'tconrx75zpzt2',\n",
              " 'hijabcontroversy',\n",
              " 'faultlines',\n",
              " 'india',\n",
              " 'hijab',\n",
              " 'article19',\n",
              " 'constitution',\n",
              " 'guarantee',\n",
              " 'freedom',\n",
              " 'speech',\n",
              " 'expression',\n",
              " 'slogan',\n",
              " 'bharatmatakijai',\n",
              " 'vandemataram',\n",
              " 'raised',\n",
              " 'saffronclad',\n",
              " 'hijab',\n",
              " 'set',\n",
              " 'fire',\n",
              " 'aligarh',\n",
              " 'http',\n",
              " 'tcoynrfyh4n5j',\n",
              " 'bharatmatakijai',\n",
              " 'vandemataram',\n",
              " 'saffronclad',\n",
              " 'hijab',\n",
              " 'aligarh',\n",
              " 'hijab',\n",
              " 'protest',\n",
              " 'downtown',\n",
              " 'dallasmy',\n",
              " 'sister',\n",
              " 'also',\n",
              " 'thereð\\x9f\\x99\\x82',\n",
              " 'thank',\n",
              " 'much',\n",
              " 'muslim',\n",
              " 'brother',\n",
              " 'sister',\n",
              " 'support',\n",
              " 'hijabisourright',\n",
              " 'hijabisourpride',\n",
              " 'hijabisfundamentalright',\n",
              " 'hijab',\n",
              " 'karnatakahijabrow',\n",
              " 'islamophobiainindia',\n",
              " 'http',\n",
              " 'tcoudanmcicea',\n",
              " 'hijabisourright',\n",
              " 'hijabisourpride',\n",
              " 'hijabisfundamentalright',\n",
              " 'hijab',\n",
              " 'karnatakahijabrow',\n",
              " 'islamophobiainindia',\n",
              " 'dmk',\n",
              " 'à®¤à®¿à®°à¯\\x81à®\\x9fà¯\\x8dà®\\x9fà¯\\x81à®¤à®¿à®°à®¾à®µà®¿à®\\x9fà®®à¯\\x8d',\n",
              " 'abuse',\n",
              " 'hindu',\n",
              " 'hindu',\n",
              " 'god',\n",
              " 'heritage',\n",
              " 'tradition',\n",
              " 'normal',\n",
              " 'hindu',\n",
              " 'human',\n",
              " 'amp',\n",
              " 'stone',\n",
              " 'block',\n",
              " 'harmless',\n",
              " 'factual',\n",
              " 'opinion',\n",
              " 'hijab',\n",
              " 'wearing',\n",
              " 'voter',\n",
              " 'localbodyelection',\n",
              " 'hurt',\n",
              " 'muslim',\n",
              " 'ever',\n",
              " 'cm',\n",
              " 'biased',\n",
              " 'mkstalin',\n",
              " 'http',\n",
              " 'tcozdlhlodur4',\n",
              " 'dmk',\n",
              " 'à®¤à®¿à®°à¯\\x81à®\\x9fà¯\\x8dà®\\x9fà¯\\x81à®¤à®¿à®°à®¾à®µà®¿à®\\x9fà®®à¯\\x8d',\n",
              " 'hindu',\n",
              " 'hijab',\n",
              " 'localbodyelection',\n",
              " 'mkstalin',\n",
              " 'karnatakahijabcontroversy',\n",
              " '58',\n",
              " 'student',\n",
              " 'allegedly',\n",
              " 'suspended',\n",
              " 'college',\n",
              " 'removing',\n",
              " 'hijab',\n",
              " 'however',\n",
              " 'shivamogga',\n",
              " 'deputy',\n",
              " 'commissioner',\n",
              " 'claimed',\n",
              " 'principal',\n",
              " 'institution',\n",
              " 'threatened',\n",
              " 'suspension',\n",
              " 'order',\n",
              " 'issued',\n",
              " 'http',\n",
              " 'tcoll4n3rhgne',\n",
              " 'karnatakahijabcontroversy',\n",
              " 'hijab',\n",
              " 'radrama',\n",
              " 'rohinisgh',\n",
              " 'never',\n",
              " 'hijab',\n",
              " 'wanted',\n",
              " 'create',\n",
              " 'optic',\n",
              " 'uttarakhandelections2022',\n",
              " 'hijab',\n",
              " 'uttarakhandelections2022',\n",
              " 'bommai',\n",
              " 'priyaakulkarni2',\n",
              " 'currently',\n",
              " 'hijab',\n",
              " 'valentineday',\n",
              " 'point',\n",
              " 'achieve',\n",
              " 'attack',\n",
              " 'hindu',\n",
              " 'religious',\n",
              " 'festival',\n",
              " 'like',\n",
              " 'diwali',\n",
              " 'holi',\n",
              " 'bhai',\n",
              " 'duaj',\n",
              " 'durga',\n",
              " 'pooja',\n",
              " 'ganesh',\n",
              " 'chaturthi',\n",
              " 'many',\n",
              " 'banhijab',\n",
              " 'muslim',\n",
              " 'liveorleave',\n",
              " 'hijab',\n",
              " 'valentineday',\n",
              " 'banhijab',\n",
              " 'liveorleave',\n",
              " 'india',\n",
              " 'would',\n",
              " 'hijab',\n",
              " 'perfect',\n",
              " 'tool',\n",
              " 'transport',\n",
              " 'someone',\n",
              " 'hidden',\n",
              " 'plain',\n",
              " 'sight',\n",
              " 'hijab',\n",
              " '3',\n",
              " 'phase',\n",
              " 'voting',\n",
              " 'suddenly',\n",
              " 'hijabrow',\n",
              " 'news',\n",
              " 'medium',\n",
              " 'reporting',\n",
              " 'drama',\n",
              " 'bring',\n",
              " 'bjp',\n",
              " 'election',\n",
              " 'amp',\n",
              " 'saturated',\n",
              " 'election',\n",
              " 'student',\n",
              " 'amp',\n",
              " 'relegion',\n",
              " 'used',\n",
              " 'puppet',\n",
              " 'political',\n",
              " 'people',\n",
              " 'yet',\n",
              " 'hijabcontroversy',\n",
              " 'hijab',\n",
              " 'hijabrow',\n",
              " 'hijabcontroversy',\n",
              " 'hijab',\n",
              " 'ktaka',\n",
              " 'minister',\n",
              " 'k',\n",
              " 'ishwarappa',\n",
              " 'said',\n",
              " 'saffron',\n",
              " 'flag',\n",
              " 'replace',\n",
              " 'tricolour',\n",
              " 'red',\n",
              " 'fort',\n",
              " 'day',\n",
              " 'bjp',\n",
              " 'never',\n",
              " 'fails',\n",
              " 'exhibit',\n",
              " 'disrespect',\n",
              " 'nation',\n",
              " 'pride',\n",
              " 'seditious',\n",
              " 'crime',\n",
              " 'tricolor',\n",
              " 'ishwarappa',\n",
              " 'redfort',\n",
              " 'saffronflag',\n",
              " 'saffron',\n",
              " 'hijab',\n",
              " 'tricolor',\n",
              " 'ishwarappa',\n",
              " 'redfort',\n",
              " 'saffronflag',\n",
              " 'saffron',\n",
              " 'hijab',\n",
              " 'frontlineindia',\n",
              " 'bharat4justice',\n",
              " 'yes',\n",
              " 'itâ\\x80\\x99s',\n",
              " 'hindutva',\n",
              " 'agenda',\n",
              " 'hater',\n",
              " 'hijab',\n",
              " 'excuse',\n",
              " 'hindutva',\n",
              " 'hater',\n",
              " 'hijab',\n",
              " 'shakibhaq',\n",
              " 'ð\\x9f¤£ð\\x9f¤£ð\\x9f¤£',\n",
              " 'converted',\n",
              " 'duplicate',\n",
              " 'demand',\n",
              " 'anything',\n",
              " 'original',\n",
              " 'say',\n",
              " 'hijab',\n",
              " 'http',\n",
              " 'tcoqzxnsh8mkq',\n",
              " 'hijab',\n",
              " 'dhaka',\n",
              " 'ð\\x9f\\x87§ð\\x9f\\x87©',\n",
              " 'choose',\n",
              " 'wear',\n",
              " 'hijab',\n",
              " 'protect',\n",
              " 'choice',\n",
              " 'making',\n",
              " 'equally',\n",
              " 'sure',\n",
              " 'stand',\n",
              " 'choice',\n",
              " 'someone',\n",
              " 'turn',\n",
              " 'http',\n",
              " 'tcozmafaxxboe',\n",
              " 'hijab',\n",
              " 'bommai',\n",
              " 'busy',\n",
              " 'spreading',\n",
              " 'hatred',\n",
              " 'schoolcollege',\n",
              " 'going',\n",
              " 'hijab',\n",
              " 'wearing',\n",
              " 'muslim',\n",
              " 'girl',\n",
              " 'http',\n",
              " 'tcou4plmdbvqz',\n",
              " 'bommai',\n",
              " 'hijab',\n",
              " 'advaidism',\n",
              " '1',\n",
              " 'support',\n",
              " 'hijab',\n",
              " 'burka',\n",
              " 'muslim',\n",
              " 'fellow',\n",
              " 'citizen',\n",
              " 'hijabburka',\n",
              " 'existence',\n",
              " 'prof',\n",
              " 'ultimate',\n",
              " 'expression',\n",
              " 'freedom',\n",
              " 'doesnâ\\x80\\x99t',\n",
              " 'need',\n",
              " 'logic',\n",
              " 'logic',\n",
              " 'patriarchal',\n",
              " 'tool',\n",
              " 'fool',\n",
              " 'rationally',\n",
              " 'marginalized',\n",
              " 'people',\n",
              " 'hijab',\n",
              " 'beyond',\n",
              " 'logic',\n",
              " 'hijab',\n",
              " 'burka',\n",
              " 'mysuru',\n",
              " 'anniezaidi',\n",
              " 'people',\n",
              " 'like',\n",
              " 'keep',\n",
              " 'muslim',\n",
              " 'backward',\n",
              " 'propoganda',\n",
              " 'changed',\n",
              " '2022',\n",
              " 'suddenly',\n",
              " 'muslim',\n",
              " 'girl',\n",
              " 'want',\n",
              " 'hijab',\n",
              " 'hijab',\n",
              " 'false',\n",
              " 'propegenda',\n",
              " 'hijab',\n",
              " 'burqa',\n",
              " 'spreading',\n",
              " 'called',\n",
              " 'indian',\n",
              " 'muslim',\n",
              " 'woman',\n",
              " 's',\n",
              " 'actually',\n",
              " 'r',\n",
              " 'propengendist',\n",
              " 'practice',\n",
              " 'since',\n",
              " '2030',\n",
              " 'year',\n",
              " 'india',\n",
              " 'true',\n",
              " 'hijab',\n",
              " 'practicing',\n",
              " '1400',\n",
              " 'year',\n",
              " 'among',\n",
              " 'muslim',\n",
              " 'woman',\n",
              " 'either',\n",
              " 'form',\n",
              " 'burqa',\n",
              " 'hijab',\n",
              " 'burqa',\n",
              " 'karnataka',\n",
              " 'ð\\x9f\\x91\\x89',\n",
              " 'http',\n",
              " 'tcoqnb5661ycy',\n",
              " 'à¤®à¤°à¤¾à¤',\n",
              " 'à¥\\x80',\n",
              " 'à¤®à¤°à¤¾à¤',\n",
              " 'à¥\\x80à¤¬à¤¾à¤¤à¤®à¥\\x8dà¤¯à¤¾',\n",
              " 'marathi',\n",
              " 'marathinews',\n",
              " 'navarashtra',\n",
              " 'hijab',\n",
              " 'controversy',\n",
              " 'karnataka',\n",
              " 'à¤®à¤°à¤¾à¤',\n",
              " 'à¥\\x80',\n",
              " 'à¤®à¤°à¤¾à¤',\n",
              " 'à¥\\x80à¤¬à¤¾à¤¤à¤®à¥\\x8dà¤¯à¤¾',\n",
              " 'marathi',\n",
              " 'marathinews',\n",
              " 'navarashtra',\n",
              " 'hijab',\n",
              " 'donâ\\x80\\x99t',\n",
              " 'know',\n",
              " 'headscarf',\n",
              " 'girl',\n",
              " 'think',\n",
              " 'men',\n",
              " 'taken',\n",
              " 'copyright',\n",
              " 'thought',\n",
              " 'thatâ\\x80\\x99s',\n",
              " 'religionâ\\x80\\x99s',\n",
              " 'worst',\n",
              " 'crime',\n",
              " 'writes',\n",
              " 'prasannara',\n",
              " 'hijab',\n",
              " 'orhanpamuk',\n",
              " 'snow',\n",
              " 'http',\n",
              " 'tcogojmyre0ba',\n",
              " 'http',\n",
              " 'tcoihxp4yn4jq',\n",
              " 'hijab',\n",
              " 'orhanpamuk',\n",
              " 'snow',\n",
              " 'tamilnadu',\n",
              " 'row',\n",
              " 'bjp',\n",
              " 'booth',\n",
              " 'agent',\n",
              " 'asks',\n",
              " 'woman',\n",
              " 'remove',\n",
              " 'hijab',\n",
              " 'http',\n",
              " 'tcosa6jmnmtzz',\n",
              " 'booth',\n",
              " 'agent',\n",
              " 'girirajan',\n",
              " 'created',\n",
              " 'ruckus',\n",
              " 'alameen',\n",
              " 'school',\n",
              " 'polling',\n",
              " 'booth',\n",
              " '8th',\n",
              " 'ward',\n",
              " 'melur',\n",
              " 'municipality',\n",
              " 'handsoffmyhijab',\n",
              " 'tamilnadu',\n",
              " 'hijab',\n",
              " 'girirajan',\n",
              " 'handsoffmyhijab',\n",
              " 'wearing',\n",
              " 'hijab',\n",
              " 'personal',\n",
              " 'choice',\n",
              " 'governmentâ\\x80\\x99s',\n",
              " 'hijab',\n",
              " 'restriction',\n",
              " 'school',\n",
              " 'college',\n",
              " 'violate',\n",
              " 'indiaâ\\x80\\x99s',\n",
              " 'obligation',\n",
              " 'international',\n",
              " 'human',\n",
              " 'right',\n",
              " 'law',\n",
              " 'human',\n",
              " 'right',\n",
              " 'watch',\n",
              " 'hijabrow',\n",
              " 'hijab',\n",
              " 'hijabisindividualright',\n",
              " 'hijabrow',\n",
              " 'hijab',\n",
              " 'hijabisindividualright',\n",
              " 'view',\n",
              " 'follow',\n",
              " 'upscinsta',\n",
              " ...]"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "keyword_dataset = df_query['tweet'].tolist()\n",
        "tweet_query_keyword_extractor = keyword_extractor(keyword_dataset)"
      ],
      "metadata": {
        "id": "eZn2SW4QdE4t"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tweet_query_keyword_extractor"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bMBdj6F4gQ0H",
        "outputId": "7fb9d377-a629-4985-85f4-95b417215a41"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['hijab',\n",
              " 'muslim',\n",
              " 'women',\n",
              " 'india',\n",
              " 'wear',\n",
              " 'muslims',\n",
              " 'hijabrow',\n",
              " 'islam',\n",
              " 'allah',\n",
              " 'like',\n",
              " 'girls',\n",
              " 'hijabisourright',\n",
              " 'karnataka',\n",
              " 'wearing',\n",
              " 'amp',\n",
              " 'school',\n",
              " 'hijabisourpride',\n",
              " 'ban',\n",
              " 'you',\n",
              " 'hijabisfundamentalright',\n",
              " 'girl',\n",
              " 'hijabcontroversy',\n",
              " 'one',\n",
              " 'education',\n",
              " 'choice',\n",
              " 'need',\n",
              " 'world',\n",
              " 'schools',\n",
              " 'people',\n",
              " 'islamic',\n",
              " 'woman',\n",
              " 'also',\n",
              " 'want',\n",
              " 'religion',\n",
              " 'first',\n",
              " 'bjp',\n",
              " 'right',\n",
              " 'even',\n",
              " 'sitting',\n",
              " 'hijabisindividualright',\n",
              " 'leaders',\n",
              " 'judge',\n",
              " 'hijabban',\n",
              " 'protest',\n",
              " 'quietly',\n",
              " 'students',\n",
              " 'uniform',\n",
              " 'tell',\n",
              " 'allowed',\n",
              " 'fighting',\n",
              " 'colleges',\n",
              " 'burqa',\n",
              " 'issue',\n",
              " 'started',\n",
              " 'surely',\n",
              " 'public',\n",
              " 'decision',\n",
              " 'religious',\n",
              " 'said',\n",
              " 'support',\n",
              " 'indian',\n",
              " 'rely',\n",
              " 'alive',\n",
              " 'abaya',\n",
              " 'islamophobia',\n",
              " 'freedom',\n",
              " 'muslimah',\n",
              " 'persecution',\n",
              " 'allahuakbar',\n",
              " 'sunnah',\n",
              " 'see',\n",
              " 'ramadan',\n",
              " 'makkah',\n",
              " 'alhamdulillah',\n",
              " 'get',\n",
              " 'islamicquotes',\n",
              " 'dua',\n",
              " 'quran',\n",
              " 'rights',\n",
              " 'hijabplot',\n",
              " 'country',\n",
              " 'hindu',\n",
              " 'practice',\n",
              " 'remove',\n",
              " 'men',\n",
              " 'make',\n",
              " 'karnatakahijabcontroversy',\n",
              " 'cocktribute',\n",
              " 'u',\n",
              " 'jannah',\n",
              " 'saudi',\n",
              " 'prophetmuhammad',\n",
              " 'modest',\n",
              " 'asked',\n",
              " 'dress',\n",
              " 'give',\n",
              " 'college',\n",
              " 'us',\n",
              " 'whole',\n",
              " 'khan',\n",
              " 'subuhi',\n",
              " 'court',\n",
              " 'hindus',\n",
              " 'state',\n",
              " 'saffron',\n",
              " 'many',\n",
              " 'hijaboruniform',\n",
              " 'face',\n",
              " 'stateâ\\x80\\x99',\n",
              " 'hijabaurkitab',\n",
              " 'modestfashion',\n",
              " 'hijabi',\n",
              " 'lashes',\n",
              " 'muhammad',\n",
              " 'way',\n",
              " 'deen',\n",
              " 'stand',\n",
              " 'islamists',\n",
              " 'must',\n",
              " 'important',\n",
              " 'lawyer',\n",
              " 'itâ\\x80\\x99s',\n",
              " 'latest',\n",
              " 'educated',\n",
              " 'â\\x80\\x93',\n",
              " 'much',\n",
              " 'made',\n",
              " 'it',\n",
              " 'not',\n",
              " 'please',\n",
              " 'anything',\n",
              " 'supreme',\n",
              " 'ongoing',\n",
              " 'never',\n",
              " 'class',\n",
              " 'hindutva',\n",
              " 'existence',\n",
              " 'money',\n",
              " 'towards',\n",
              " 'thanks',\n",
              " 'demanding',\n",
              " 'say',\n",
              " 'teen',\n",
              " 'karnatakahijabrow',\n",
              " 'today',\n",
              " 'day',\n",
              " 'home',\n",
              " 'choose',\n",
              " 'understand',\n",
              " 'â\\x80\\x98hijab',\n",
              " 'good',\n",
              " 'entire',\n",
              " 'bank',\n",
              " 'suddenly',\n",
              " 'called',\n",
              " 'order',\n",
              " 'protesting',\n",
              " 'time',\n",
              " 'police',\n",
              " 'muslimgirls',\n",
              " 'go',\n",
              " 'hate',\n",
              " 'cover',\n",
              " 'faith',\n",
              " 'nsfw',\n",
              " 'respect',\n",
              " 'around',\n",
              " 'sometimes',\n",
              " 'across',\n",
              " 'oribelle',\n",
              " 'name',\n",
              " 'islamophobiainindia',\n",
              " 'logic',\n",
              " 'this',\n",
              " 'movie',\n",
              " 'still',\n",
              " 'think',\n",
              " 'since',\n",
              " '»',\n",
              " 'ask',\n",
              " 'attack',\n",
              " 'donâ\\x80\\x99t',\n",
              " 'read',\n",
              " 'hai',\n",
              " 'love',\n",
              " 'upcoming',\n",
              " 'cant',\n",
              " 'permission',\n",
              " 'years',\n",
              " 'protests',\n",
              " 'modesty',\n",
              " 'ð\\x9f\\x91\\x89ð\\x9f\\x91\\x89',\n",
              " 'â\\x80¢',\n",
              " 'stop',\n",
              " 'fact',\n",
              " 'asking',\n",
              " 'writes',\n",
              " 'agenda',\n",
              " 'shop',\n",
              " 'â\\x81',\n",
              " 'come',\n",
              " 'know',\n",
              " 'actor',\n",
              " 'im',\n",
              " 'follow',\n",
              " 'controversy',\n",
              " 'can',\n",
              " '2022',\n",
              " 'demand',\n",
              " 'her',\n",
              " 'wears',\n",
              " 'fight',\n",
              " 'daily',\n",
              " 'put',\n",
              " 'secular',\n",
              " 'veil',\n",
              " 'old',\n",
              " 'step',\n",
              " 'plain',\n",
              " 'case',\n",
              " 'looking',\n",
              " 'congress',\n",
              " 'femboy',\n",
              " 'wrong',\n",
              " 'milf',\n",
              " 'propaganda',\n",
              " 'square',\n",
              " 'obligation',\n",
              " 'without',\n",
              " 'male',\n",
              " 'might',\n",
              " 'supporting',\n",
              " 'thats',\n",
              " 'booth',\n",
              " 'came',\n",
              " 'ð\\x9f\\x91\\x89',\n",
              " 'post',\n",
              " 'speech',\n",
              " 'paid',\n",
              " 'wish',\n",
              " 'males',\n",
              " 'hijabisouridentity',\n",
              " 'row',\n",
              " 'concept',\n",
              " 'elections',\n",
              " 'banned',\n",
              " 'media',\n",
              " 'place',\n",
              " 'issues',\n",
              " 'pakistan',\n",
              " 'full',\n",
              " 'niqab',\n",
              " 'election',\n",
              " 'yasser',\n",
              " 'biased',\n",
              " 'let',\n",
              " 'create',\n",
              " 'purchased',\n",
              " 'skull',\n",
              " 'rule',\n",
              " 'per',\n",
              " 'used',\n",
              " 'too',\n",
              " 'law',\n",
              " 'issued',\n",
              " 'islamicpost',\n",
              " 'well',\n",
              " 'video',\n",
              " 'political',\n",
              " 'look',\n",
              " 'pk',\n",
              " 'phone',\n",
              " 'hijabcircular',\n",
              " 'dmme',\n",
              " 'says',\n",
              " 'places',\n",
              " 'life',\n",
              " 'beautiful',\n",
              " 'beard',\n",
              " 'zairawasim',\n",
              " 'spread',\n",
              " 'part',\n",
              " 'crown',\n",
              " 'banning',\n",
              " 'via',\n",
              " 'nations',\n",
              " 'karnatakahijab',\n",
              " 'clearly',\n",
              " 'arafath',\n",
              " 'every',\n",
              " 'job',\n",
              " 'bwc',\n",
              " 'hijabisourfaith',\n",
              " 'property',\n",
              " 'soon',\n",
              " 'shivamogga',\n",
              " 'france',\n",
              " 'view',\n",
              " 'true',\n",
              " 'really',\n",
              " 'roza',\n",
              " 'caste',\n",
              " 'wife',\n",
              " 'namaz',\n",
              " 'expression',\n",
              " 'community',\n",
              " 'i',\n",
              " 'burkah',\n",
              " 'friend',\n",
              " 'already',\n",
              " 'international',\n",
              " 'hypocrisy',\n",
              " 'clothes',\n",
              " 'prince',\n",
              " 'everyone',\n",
              " 'footfetish',\n",
              " 'keep',\n",
              " 'sisters',\n",
              " '«',\n",
              " 'solidarity',\n",
              " 'spreading',\n",
              " 'news',\n",
              " 'r',\n",
              " 'meher',\n",
              " 'father',\n",
              " 'justice',\n",
              " 'month',\n",
              " 'someone',\n",
              " 'rss',\n",
              " 'things',\n",
              " 'loose',\n",
              " 'talak',\n",
              " '1',\n",
              " 'history',\n",
              " 'front',\n",
              " '2',\n",
              " 'real',\n",
              " 'else',\n",
              " 'minorities',\n",
              " 'muslimmen',\n",
              " 'using',\n",
              " 'link',\n",
              " 'taken',\n",
              " 'watch',\n",
              " 'govt',\n",
              " 'city',\n",
              " 'reason',\n",
              " 'either',\n",
              " 'shirt',\n",
              " 'to',\n",
              " 'eventð\\x9f\\x98\\x8d',\n",
              " 'bella',\n",
              " 'regressive',\n",
              " 'â\\x9d¤ï¸\\x8f',\n",
              " 'ago',\n",
              " 'that',\n",
              " 'worship',\n",
              " 'fear',\n",
              " 'black',\n",
              " 'back',\n",
              " 'empowerment',\n",
              " 'kuwait',\n",
              " 'long',\n",
              " 'wasim',\n",
              " 'boy']"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tweet_keywords_yake = []\n",
        "kw_extractor = yake.KeywordExtractor(top=20, stopwords=None)\n",
        "keywords = kw_extractor.extract_keywords(' '.join(tweet_query))\n",
        "#keywords = kw_extractor.extract_keywords(' '.join(df_query['tweet'].tolist()))\n",
        "for kw, v in keywords:\n",
        "  #print(\"Keyphrase: \",kw, \": score\", v)\n",
        "  for key in kw.split():\n",
        "    if(key.lower() not in tweet_keywords_yake):\n",
        "      tweet_keywords_yake.append(key.lower())\n",
        "\n",
        "print(tweet_keywords_yake)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VUljLxKDOAwW",
        "outputId": "b291e968-e7d2-4367-8f28-cb38b96ebf0e"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keyphrase:  hijab hijab hijab : score 3.538515818965514e-07\n",
            "Keyphrase:  hijab karnataka india : score 4.4295050053984284e-07\n",
            "Keyphrase:  muslim islam muslim : score 7.831784725969966e-07\n",
            "Keyphrase:  hijab hijabisfundamentalright india : score 8.733034882458195e-07\n",
            "Keyphrase:  hijabisourright hijabcontroversy hijab : score 1.2084471666090418e-06\n",
            "Keyphrase:  karnataka india hijab : score 1.2402614015115601e-06\n",
            "Keyphrase:  india hijab karnataka : score 1.2402614015115601e-06\n",
            "Keyphrase:  muslim hijab started : score 1.3280528877404291e-06\n",
            "Keyphrase:  hijab abaya muslim : score 1.3444833831980227e-06\n",
            "Keyphrase:  hijab hijabrow http : score 1.425915298203952e-06\n",
            "Keyphrase:  persecution muslim hijab : score 1.4602327688888964e-06\n",
            "Keyphrase:  hijabcontroversy hijab hijabisfundamentalright : score 1.5153685192729542e-06\n",
            "Keyphrase:  hijabisourright hijab http : score 1.6067046974268703e-06\n",
            "Keyphrase:  wear hijab hijab : score 1.6311894877235963e-06\n",
            "Keyphrase:  hijab muslim india : score 1.6435816858664886e-06\n",
            "Keyphrase:  islam muslim allah : score 1.6736686738241312e-06\n",
            "Keyphrase:  hijab started india : score 1.6956730006765869e-06\n",
            "Keyphrase:  hijab muslim woman : score 1.7331287757545118e-06\n",
            "Keyphrase:  hijab karnatakahijabcontroversy hijabrow : score 2.070500854170206e-06\n",
            "Keyphrase:  wearing hijab hijab : score 2.179807258036074e-06\n",
            "['hijab', 'karnataka', 'india', 'muslim', 'islam', 'hijabisfundamentalright', 'hijabisourright', 'hijabcontroversy', 'started', 'abaya', 'hijabrow', 'http', 'persecution', 'wear', 'allah', 'woman', 'karnatakahijabcontroversy', 'wearing']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tweet_keywords_rake = []\n",
        "rake = Rake()\n",
        "rake_keywords = rake.apply(' '.join(tweet_query).encode('ascii', 'ignore').decode())\n",
        "for kw,score in rake_keywords[:20]:\n",
        "  for key in kw.split():\n",
        "    if(key.lower() not in tweet_keywords_yake):\n",
        "      tweet_keywords_rake.append(key.lower())\n",
        "print(tweet_keywords_rake)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9NSIkve2QACN",
        "outputId": "b7a3a7ff-022c-4397-c0ad-8c973251e131"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['nationalist', 'bjp', 'government', 'puppet', 'political', 'people', 'remove', '1st', 'sanghis', 'carrying', 'religious', 'commitment', 'uk', 'usa', 'dude', 'destruction', 'implosion', 'fear', 'side', 'coin', 'matter', 'member', 'faith', 'bradford', 'bid', 'scaleup', 'tourism', 'shawl', 'order', 'visit', 'understand', 'prince', 'respect', 'part', 'dress', 'code', 'sexually', 'abusing', '9nyear', 'upper', 'caste', 'idiot', 'allowed', 'wait', 'god', 'vote', 'bjp', 'understand', 'kasuwa24', 'youre', 'click', 'subahanahuwataala', 'bless', 'issue', 'claim', 'pardah', 'varies']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tweet_keywords_text_rank = []\n",
        "TR_keywords = summa_keywords.keywords(' '.join(tweet_query), scores=True)\n",
        "for kw,score in TR_keywords[:20]:\n",
        "  for key in kw.split():\n",
        "    if(key.lower() not in tweet_keywords_text_rank):\n",
        "      tweet_keywords_text_rank.append(key.lower())\n",
        "print(tweet_keywords_text_rank)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QsHvmAgMWOoQ",
        "outputId": "74c0ba6c-5d03-4a43-849e-749200e6941a"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['hijabers', 'http', 'hijab', 'hate', 'ð', 'â', 'school', 'india', 'wearing', 'girls', 'woman', 'forced', 'wear', 'hijabrow', 'amp', 'islamic', 'local', 'dallas', 'muslim', 'tco', 'educated', 'like', 'banning', 'banned', 'girl', 'suddenly', 'wanted', 'bjp']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "keybert_model = KeyBERT(model='all-mpnet-base-v2')\n",
        "keybert_keywords = keybert_model.extract_keywords(' '.join(tweet_query), keyphrase_ngram_range=(1,1), stop_words='english', highlight=False, top_n=20)\n",
        "tweet_keywords_keybert = list(dict(keybert_keywords).keys())\n",
        "print(tweet_keywords_keybert)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qlj2TnBifv7c",
        "outputId": "e1b78863-7cb5-4e5c-9a97-ed618d86aa34"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['hijabisfundamentalright', 'hijabers', 'hijabkarnataka', 'hijab', 'hijabi', 'hijabifashion', 'hijabday', 'hijabisourright', 'hijabporn', 'hijabisindividualright', 'hijabfashion', 'hijaband', 'hijabban', 'hijabislut', 'hijabisouridentity', 'hijabismyright', 'hijabð', 'hijabis', 'hijaab', 'burkah']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "docs_preprocessed = []"
      ],
      "metadata": {
        "id": "wSuHVdgnMhMz"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Storing file name and data\n",
        "total_documents = 0\n",
        "path = '/content/drive/MyDrive/Tweelink_Dataset/Tweelink_Articles_Processed'\n",
        "for filename in glob(os.path.join(path, '*')):\n",
        "   with open(os.path.join(os.getcwd(), filename), 'r', encoding = 'utf-8',errors = 'ignore') as f:\n",
        "     filename = os.path.basename(f.name)\n",
        "     data = json.load(f)\n",
        "     d_date = data[\"Date\"]\n",
        "     if(d_date==\"\" or d_date==\"Date\"):\n",
        "       continue\n",
        "     format = '%Y-%m-%d'\n",
        " \n",
        "     d_present_date = datetime.datetime.strptime(d_date, format)\n",
        " \n",
        "     if(str(d_present_date.date()) not in [str(u_present_date.date()), str(u_prev_date.date()), str(u_next_date.date())]):\n",
        "       continue\n",
        "   \n",
        "     docs_preprocessed.append({'Name':filename, 'Data':data})\n",
        "     total_documents+=1\n",
        "print(total_documents)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UcXI0WGeEjPG",
        "outputId": "8e366086-c376-4892-e67b-93e1271961db"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "295\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_relevant_docs_list_for_base_hashtag(base_hashtag, base_date, docs_preprocessed):\n",
        "  relevant_docs_list = []\n",
        "  for doc in docs_preprocessed:\n",
        "    if doc['Data']['Base Hashtag']==base_hashtag:\n",
        "      current_date = datetime.datetime.strptime(base_date, format)\n",
        "      prev_date = current_date - datetime.timedelta(days=1)\n",
        "      next_date = current_date + datetime.timedelta(days=1)\n",
        "      if(doc['Data']['Date'] in [str(prev_date.date()), str(current_date.date()), str(next_date.date())]):\n",
        "        relevant_docs_list.append(doc['Name'])\n",
        "  return relevant_docs_list"
      ],
      "metadata": {
        "id": "kUxQkyxyjOJp"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def precision_at_k(k, base_hashtag, base_date, prediction_list, docs_preprocessed):\n",
        "  relevant_docs_list = get_relevant_docs_list_for_base_hashtag(base_hashtag, base_date, docs_preprocessed)\n",
        "  num_of_relevant_results=0\n",
        "  for itr in range(k):\n",
        "    if (prediction_list[itr][0] in relevant_docs_list):\n",
        "      num_of_relevant_results+=1\n",
        "  return num_of_relevant_results/k"
      ],
      "metadata": {
        "id": "9w3YOj_PcnaK"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def mean_average_precision(max_k, base_hashtag, base_date, relevant_docs, docs_preprocessed):\n",
        "  average_precision=0\n",
        "  ctr=0\n",
        "  for k_val in range(1,max_k+1):\n",
        "    ctr+=1\n",
        "    precision_at_k_val = precision_at_k(k_val, base_hashtag, base_date, relevant_docs, docs_preprocessed)\n",
        "    #print('Hashtag: {}   Precision@{}: {}'.format(base_hashtag, k_val, precision_at_k_val))\n",
        "    average_precision += precision_at_k_val\n",
        "  return average_precision/ctr"
      ],
      "metadata": {
        "id": "ZxkPnyRes6GW"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def recall_at_k(k, base_hashtag, base_date, prediction_list, docs_preprocessed):\n",
        "  relevant_docs_list = get_relevant_docs_list_for_base_hashtag(base_hashtag, base_date, docs_preprocessed)\n",
        "  current_num_of_relevant_results=0\n",
        "  for itr in range(k):\n",
        "    if (prediction_list[itr][0] in relevant_docs_list):\n",
        "      current_num_of_relevant_results+=1\n",
        "  if(len(relevant_docs_list)==0):\n",
        "    return 0\n",
        "  return current_num_of_relevant_results/len(relevant_docs_list)"
      ],
      "metadata": {
        "id": "uxEEPQTYu8G7"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def mean_average_recall(max_k, base_hashtag, base_date, relevant_docs, docs_preprocessed):\n",
        "  average_recall=0\n",
        "  ctr=0\n",
        "  for k_val in range(1,max_k+1):\n",
        "    ctr+=1\n",
        "    recall_at_k_val = recall_at_k(k_val, base_hashtag, base_date, relevant_docs, docs_preprocessed)\n",
        "    #print('Hashtag: {}   Recall@{}: {}'.format(base_hashtag, k_val, recall_at_k_val))\n",
        "    average_recall += recall_at_k_val\n",
        "  return average_recall/ctr"
      ],
      "metadata": {
        "id": "_OLiT_Q6vsDO"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import operator\n",
        "\n",
        "def find_relevant_documents(docs_preprocessed, processed_query):\n",
        "  jaccard_coefficients = {}\n",
        "  for document in docs_preprocessed:\n",
        "    doc_text = set(document['Data']['Body_processed'])\n",
        "    query = set(processed_query)\n",
        "    jaccard_coefficients[document['Name']] = len(doc_text.intersection(query))/len(doc_text.union(query))\n",
        "  relevant_docs = list( sorted(jaccard_coefficients.items(), key=operator.itemgetter(1),reverse=True))[:20]\n",
        "  for i in range(len(relevant_docs)):\n",
        "    for j in range(len(docs_preprocessed)):\n",
        "      if(relevant_docs[i][0] == docs_preprocessed[j]['Name']):\n",
        "        relevant_docs[i] = (relevant_docs[i][0], relevant_docs[i][1], docs_preprocessed[j]['Data']['Date'] )\n",
        "\n",
        "  return relevant_docs\n",
        "\n"
      ],
      "metadata": {
        "id": "scWzkhA_vKmd"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plain Model"
      ],
      "metadata": {
        "id": "P1V-ZYVeh7M3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plain Model without YAKE / Keyword Extraction\n",
        "relevant_docs_plain = find_relevant_documents(docs_preprocessed, tweet_query)\n",
        "\n",
        "for rank, doc in enumerate(relevant_docs_plain):\n",
        "  print('Rank: {} Relevant Document: {}'.format(rank+1,doc))\n",
        "\n",
        "print()\n",
        "\n",
        "mean_average_precision_hashtag_plain = mean_average_precision(20, u_base_hashtag, u_time, relevant_docs_plain, docs_preprocessed)\n",
        "print('Mean Average Precision Plain Model: {}'.format(mean_average_precision_hashtag_plain))\n",
        "\n",
        "mean_average_recall_hashtag_plain = mean_average_recall(20, u_base_hashtag, u_time, relevant_docs_plain, docs_preprocessed)\n",
        "print('Mean Average Recall Plain Model: {}'.format(mean_average_recall_hashtag_plain))"
      ],
      "metadata": {
        "id": "ym0bjkhDAeQz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "29d5b180-bb0c-4f6b-c50a-df8292610261"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rank: 1 Relevant Document: ('WriddhimanSaha_133.json', 0.08778625954198473, '2022-02-20')\n",
            "Rank: 2 Relevant Document: ('PunjabElections2022_127.json', 0.08217317487266554, '2022-02-20')\n",
            "Rank: 3 Relevant Document: ('PunjabElections2022_124.json', 0.06238587948874011, '2022-02-20')\n",
            "Rank: 4 Relevant Document: ('UkraineRussiaCrisis_220.json', 0.059113300492610835, '2022-02-18')\n",
            "Rank: 5 Relevant Document: ('vaccine_299.json', 0.058758314855875834, '2022-02-19')\n",
            "Rank: 6 Relevant Document: ('UkraineRussiaCrisis_212.json', 0.05858511422254974, '2022-02-18')\n",
            "Rank: 7 Relevant Document: ('MultiverseOfMadness_16.json', 0.058354476259514315, '2022-02-18')\n",
            "Rank: 8 Relevant Document: ('BJPwinningUP_63.json', 0.057449253159708925, '2022-02-18')\n",
            "Rank: 9 Relevant Document: ('OperationDudula_261.json', 0.05601503759398496, '2022-02-20')\n",
            "Rank: 10 Relevant Document: ('ScottyFromWelding_170.json', 0.054324517512508934, '2022-02-20')\n",
            "Rank: 11 Relevant Document: ('UkraineRussiaCrisis_261.json', 0.05400572246065808, '2022-02-19')\n",
            "Rank: 12 Relevant Document: ('Cyberpunk2077_58.json', 0.053597650513950074, '2022-02-18')\n",
            "Rank: 13 Relevant Document: ('hijab_284.json', 0.053578262533486416, '2022-02-19')\n",
            "Rank: 14 Relevant Document: ('CallTheMidwife_251.json', 0.053103964098728494, '2022-02-20')\n",
            "Rank: 15 Relevant Document: ('UkraineRussiaCrisis_264.json', 0.05204043429427181, '2022-02-19')\n",
            "Rank: 16 Relevant Document: ('hijab_286.json', 0.050972762645914396, '2022-02-19')\n",
            "Rank: 17 Relevant Document: ('OperationDudula_269.json', 0.0508160237388724, '2022-02-20')\n",
            "Rank: 18 Relevant Document: ('UkraineRussiaCrisis_213.json', 0.05065123010130246, '2022-02-18')\n",
            "Rank: 19 Relevant Document: ('narendramodi_305.json', 0.05056840454723638, '2022-02-20')\n",
            "Rank: 20 Relevant Document: ('WriddhimanSaha_134.json', 0.050287907869481764, '2022-02-20')\n",
            "\n",
            "Mean Average Precision Plain Model: 0.038701982142384614\n",
            "Mean Average Recall Plain Model: 0.032499999999999994\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model with Keyword Extractor"
      ],
      "metadata": {
        "id": "j0cPN8WyiY-m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Model with Keyword Extractor\n",
        "relevant_docs_keyword_extractor = find_relevant_documents(docs_preprocessed, tweet_query_keyword_extractor[:20])\n",
        "\n",
        "for rank, doc in enumerate(relevant_docs_keyword_extractor):\n",
        "  print('Rank: {} Relevant Document: {}'.format(rank+1,doc))\n",
        "\n",
        "print()\n",
        "\n",
        "mean_average_precision_hashtag_keyword_extractor = mean_average_precision(20, u_base_hashtag, u_time, relevant_docs_keyword_extractor, docs_preprocessed)\n",
        "print('Mean Average Precision Keyword Extractor Model: {}'.format(mean_average_precision_hashtag_keyword_extractor))\n",
        "\n",
        "mean_average_recall_hashtag_keyword_extractor = mean_average_recall(20, u_base_hashtag, u_time, relevant_docs_keyword_extractor, docs_preprocessed)\n",
        "print('Mean Average Recall Keyword Extractor Model: {}'.format(mean_average_recall_hashtag_keyword_extractor))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "feH1-DpbhLm2",
        "outputId": "acf72ac4-72ec-4ccd-a85c-c6125788dbac"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rank: 1 Relevant Document: ('hijab_288.json', 0.0898876404494382, '2022-02-19')\n",
            "Rank: 2 Relevant Document: ('hijab_282.json', 0.04046242774566474, '2022-02-19')\n",
            "Rank: 3 Relevant Document: ('hijab_281.json', 0.03864734299516908, '2022-02-19')\n",
            "Rank: 4 Relevant Document: ('hijab_290.json', 0.03864734299516908, '2022-02-19')\n",
            "Rank: 5 Relevant Document: ('hijab_285.json', 0.03501945525291829, '2022-02-19')\n",
            "Rank: 6 Relevant Document: ('hijab_287.json', 0.03389830508474576, '2022-02-19')\n",
            "Rank: 7 Relevant Document: ('hijab_283.json', 0.026737967914438502, '2022-02-19')\n",
            "Rank: 8 Relevant Document: ('hijab_286.json', 0.02622950819672131, '2022-02-19')\n",
            "Rank: 9 Relevant Document: ('BJPwinningUP_66.json', 0.024844720496894408, '2022-02-18')\n",
            "Rank: 10 Relevant Document: ('hijab_289.json', 0.023952095808383235, '2022-02-19')\n",
            "Rank: 11 Relevant Document: ('hijab_284.json', 0.019553072625698324, '2022-02-19')\n",
            "Rank: 12 Relevant Document: ('WriddhimanSaha_132.json', 0.015625, '2022-02-20')\n",
            "Rank: 13 Relevant Document: ('QueenElizabeth_160.json', 0.015209125475285171, '2022-02-20')\n",
            "Rank: 14 Relevant Document: ('IPL_226.json', 0.01485148514851485, '2022-02-18')\n",
            "Rank: 15 Relevant Document: ('IPL_227.json', 0.013157894736842105, '2022-02-18')\n",
            "Rank: 16 Relevant Document: ('IPL_224.json', 0.012987012987012988, '2022-02-18')\n",
            "Rank: 17 Relevant Document: ('PlotToKillModi_216.json', 0.012903225806451613, '2022-02-18')\n",
            "Rank: 18 Relevant Document: ('IndiaFightsCorona_246.json', 0.012048192771084338, '2022-02-18')\n",
            "Rank: 19 Relevant Document: ('TaylorSwift_238.json', 0.011904761904761904, '2022-02-18')\n",
            "Rank: 20 Relevant Document: ('ShivajiJayanti_208.json', 0.011363636363636364, '2022-02-18')\n",
            "\n",
            "Mean Average Precision Keyword Extractor Model: 0.8238301460321583\n",
            "Mean Average Recall Keyword Extractor Model: 0.3825\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model with YAKE"
      ],
      "metadata": {
        "id": "_YWxGEn7ihIX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Model with YAKE\n",
        "relevant_docs_yake = find_relevant_documents(docs_preprocessed, tweet_keywords_yake)\n",
        "\n",
        "for rank, doc in enumerate(relevant_docs_yake):\n",
        "  print('Rank: {} Relevant Document: {}'.format(rank+1,doc))\n",
        "\n",
        "print()\n",
        "\n",
        "mean_average_precision_hashtag_yake = mean_average_precision(20, u_base_hashtag, u_time, relevant_docs_yake, docs_preprocessed)\n",
        "print('Mean Average Precision YAKE Model: {}'.format(mean_average_precision_hashtag_yake))\n",
        "\n",
        "mean_average_recall_hashtag_yake = mean_average_recall(20, u_base_hashtag, u_time, relevant_docs_yake, docs_preprocessed)\n",
        "print('Mean Average Recall YAKE Model: {}'.format(mean_average_recall_hashtag_yake))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oMRwRc6Oijga",
        "outputId": "dd9262f5-85d0-476d-8136-e00ea494fc8c"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rank: 1 Relevant Document: ('hijab_288.json', 0.06741573033707865, '2022-02-19')\n",
            "Rank: 2 Relevant Document: ('hijab_282.json', 0.04093567251461988, '2022-02-19')\n",
            "Rank: 3 Relevant Document: ('hijab_285.json', 0.03529411764705882, '2022-02-19')\n",
            "Rank: 4 Relevant Document: ('hijab_290.json', 0.03398058252427184, '2022-02-19')\n",
            "Rank: 5 Relevant Document: ('hijab_281.json', 0.028985507246376812, '2022-02-19')\n",
            "Rank: 6 Relevant Document: ('hijab_287.json', 0.028409090909090908, '2022-02-19')\n",
            "Rank: 7 Relevant Document: ('hijab_283.json', 0.02702702702702703, '2022-02-19')\n",
            "Rank: 8 Relevant Document: ('narendramodi_255.json', 0.022727272727272728, '2022-02-19')\n",
            "Rank: 9 Relevant Document: ('IndiaFightsCorona_241.json', 0.020202020202020204, '2022-02-18')\n",
            "Rank: 10 Relevant Document: ('hijab_286.json', 0.019672131147540985, '2022-02-19')\n",
            "Rank: 11 Relevant Document: ('BJPwinningUP_66.json', 0.01875, '2022-02-18')\n",
            "Rank: 12 Relevant Document: ('hijab_289.json', 0.018072289156626505, '2022-02-19')\n",
            "Rank: 13 Relevant Document: ('vaccine_295.json', 0.017391304347826087, '2022-02-19')\n",
            "Rank: 14 Relevant Document: ('vaccine_292.json', 0.014925373134328358, '2022-02-19')\n",
            "Rank: 15 Relevant Document: ('hijab_284.json', 0.013966480446927373, '2022-02-19')\n",
            "Rank: 16 Relevant Document: ('IPL_227.json', 0.013333333333333334, '2022-02-18')\n",
            "Rank: 17 Relevant Document: ('PlotToKillModi_216.json', 0.013071895424836602, '2022-02-18')\n",
            "Rank: 18 Relevant Document: ('IndiaFightsCorona_246.json', 0.012195121951219513, '2022-02-18')\n",
            "Rank: 19 Relevant Document: ('PlotToKillModi_214.json', 0.011299435028248588, '2022-02-18')\n",
            "Rank: 20 Relevant Document: ('narendramodi_253.json', 0.011235955056179775, '2022-02-19')\n",
            "\n",
            "Mean Average Precision YAKE Model: 0.7863494323014446\n",
            "Mean Average Recall YAKE Model: 0.3625\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model with RAKE"
      ],
      "metadata": {
        "id": "FHQJaUz0Vmdu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Model with RAKE\n",
        "relevant_docs_rake = find_relevant_documents(docs_preprocessed, tweet_keywords_rake)\n",
        "\n",
        "for rank, doc in enumerate(relevant_docs_rake):\n",
        "  print('Rank: {} Relevant Document: {}'.format(rank+1,doc))\n",
        "\n",
        "print()\n",
        "\n",
        "mean_average_precision_hashtag_rake = mean_average_precision(20, u_base_hashtag, u_time, relevant_docs_rake, docs_preprocessed)\n",
        "print('Mean Average Precision RAKE Model: {}'.format(mean_average_precision_hashtag_rake))\n",
        "\n",
        "mean_average_recall_hashtag_rake = mean_average_recall(20, u_base_hashtag, u_time, relevant_docs_rake, docs_preprocessed)\n",
        "print('Mean Average Recall RAKE Model: {}'.format(mean_average_recall_hashtag_rake))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OKEB6pUgVkpE",
        "outputId": "f6707835-dc27-40d6-b64c-2dcfcb52b367"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rank: 1 Relevant Document: ('hijab_282.json', 0.043689320388349516, '2022-02-19')\n",
            "Rank: 2 Relevant Document: ('narendramodi_209.json', 0.041379310344827586, '2022-02-18')\n",
            "Rank: 3 Relevant Document: ('narendramodi_255.json', 0.040983606557377046, '2022-02-19')\n",
            "Rank: 4 Relevant Document: ('hijab_289.json', 0.035175879396984924, '2022-02-19')\n",
            "Rank: 5 Relevant Document: ('hijab_284.json', 0.03359173126614987, '2022-02-19')\n",
            "Rank: 6 Relevant Document: ('narendramodi_253.json', 0.03333333333333333, '2022-02-19')\n",
            "Rank: 7 Relevant Document: ('ShivajiJayanti_205.json', 0.03225806451612903, '2022-02-18')\n",
            "Rank: 8 Relevant Document: ('hijab_283.json', 0.031818181818181815, '2022-02-19')\n",
            "Rank: 9 Relevant Document: ('hijab_288.json', 0.03125, '2022-02-19')\n",
            "Rank: 10 Relevant Document: ('ScottyFromWelding_164.json', 0.030864197530864196, '2022-02-20')\n",
            "Rank: 11 Relevant Document: ('hijab_285.json', 0.030821917808219176, '2022-02-19')\n",
            "Rank: 12 Relevant Document: ('JohnsonOut21_45.json', 0.03048780487804878, '2022-02-20')\n",
            "Rank: 13 Relevant Document: ('QueenElizabeth_155.json', 0.030434782608695653, '2022-02-20')\n",
            "Rank: 14 Relevant Document: ('hijab_281.json', 0.02880658436213992, '2022-02-19')\n",
            "Rank: 15 Relevant Document: ('ShivajiJayanti_202.json', 0.02857142857142857, '2022-02-18')\n",
            "Rank: 16 Relevant Document: ('PlotToKillModi_215.json', 0.02843601895734597, '2022-02-18')\n",
            "Rank: 17 Relevant Document: ('hijab_287.json', 0.02830188679245283, '2022-02-19')\n",
            "Rank: 18 Relevant Document: ('PlotToKillModi_218.json', 0.028037383177570093, '2022-02-18')\n",
            "Rank: 19 Relevant Document: ('BJPwinningUP_63.json', 0.027707808564231738, '2022-02-18')\n",
            "Rank: 20 Relevant Document: ('PunjabElections2022_125.json', 0.026923076923076925, '2022-02-20')\n",
            "\n",
            "Mean Average Precision RAKE Model: 0.503235265121875\n",
            "Mean Average Recall RAKE Model: 0.24750000000000005\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model with TextRank"
      ],
      "metadata": {
        "id": "0rR31TWWmtot"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Model with TextRank\n",
        "relevant_docs_text_rank = find_relevant_documents(docs_preprocessed, tweet_keywords_text_rank)\n",
        "\n",
        "for rank, doc in enumerate(relevant_docs_text_rank):\n",
        "  print('Rank: {} Relevant Document: {}'.format(rank+1,doc))\n",
        "\n",
        "print()\n",
        "\n",
        "mean_average_precision_hashtag_text_rank = mean_average_precision(20, u_base_hashtag, u_time, relevant_docs_text_rank, docs_preprocessed)\n",
        "print('Mean Average Precision TextRank Model: {}'.format(mean_average_precision_hashtag_text_rank))\n",
        "\n",
        "mean_average_recall_hashtag_text_rank = mean_average_recall(20, u_base_hashtag, u_time, relevant_docs_text_rank, docs_preprocessed)\n",
        "print('Mean Average Recall TextRank Model: {}'.format(mean_average_recall_hashtag_text_rank))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JcxYxOdZe4Rt",
        "outputId": "a2ffae4a-8409-4e94-edf2-f7bcf992fa2c"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rank: 1 Relevant Document: ('hijab_288.json', 0.07142857142857142, '2022-02-19')\n",
            "Rank: 2 Relevant Document: ('BJPwinningUP_66.json', 0.048484848484848485, '2022-02-18')\n",
            "Rank: 3 Relevant Document: ('hijab_282.json', 0.03867403314917127, '2022-02-19')\n",
            "Rank: 4 Relevant Document: ('hijab_285.json', 0.03787878787878788, '2022-02-19')\n",
            "Rank: 5 Relevant Document: ('hijab_281.json', 0.037209302325581395, '2022-02-19')\n",
            "Rank: 6 Relevant Document: ('hijab_290.json', 0.037209302325581395, '2022-02-19')\n",
            "Rank: 7 Relevant Document: ('hijab_283.json', 0.03626943005181347, '2022-02-19')\n",
            "Rank: 8 Relevant Document: ('hijab_286.json', 0.035483870967741936, '2022-02-19')\n",
            "Rank: 9 Relevant Document: ('hijab_287.json', 0.032432432432432434, '2022-02-19')\n",
            "Rank: 10 Relevant Document: ('PlotToKillModi_220.json', 0.023121387283236993, '2022-02-18')\n",
            "Rank: 11 Relevant Document: ('hijab_289.json', 0.022857142857142857, '2022-02-19')\n",
            "Rank: 12 Relevant Document: ('BoycottWalgreens_249.json', 0.020833333333333332, '2022-02-18')\n",
            "Rank: 13 Relevant Document: ('BoycottWalgreens_119.json', 0.020833333333333332, '2022-02-18')\n",
            "Rank: 14 Relevant Document: ('ShivajiJayanti_210.json', 0.02040816326530612, '2022-02-18')\n",
            "Rank: 15 Relevant Document: ('hijab_284.json', 0.01912568306010929, '2022-02-19')\n",
            "Rank: 16 Relevant Document: ('IPL_227.json', 0.018867924528301886, '2022-02-18')\n",
            "Rank: 17 Relevant Document: ('IPL_224.json', 0.018633540372670808, '2022-02-18')\n",
            "Rank: 18 Relevant Document: ('PlotToKillModi_213.json', 0.018518518518518517, '2022-02-18')\n",
            "Rank: 19 Relevant Document: ('PlotToKillModi_216.json', 0.018518518518518517, '2022-02-18')\n",
            "Rank: 20 Relevant Document: ('PlotToKillModi_211.json', 0.017482517482517484, '2022-02-18')\n",
            "\n",
            "Mean Average Precision TextRank Model: 0.7168075852595975\n",
            "Mean Average Recall TextRank Model: 0.35250000000000004\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model with KeyBERT"
      ],
      "metadata": {
        "id": "b6qJAGaMmwes"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Model with KeyBERT\n",
        "relevant_docs_keybert = find_relevant_documents(docs_preprocessed, tweet_keywords_keybert)\n",
        "\n",
        "for rank, doc in enumerate(relevant_docs_keybert):\n",
        "  print('Rank: {} Relevant Document: {}'.format(rank+1,doc))\n",
        "\n",
        "print()\n",
        "\n",
        "mean_average_precision_hashtag_keybert = mean_average_precision(20, u_base_hashtag, u_time, relevant_docs_keybert, docs_preprocessed)\n",
        "print('Mean Average Precision KeyBERT Model: {}'.format(mean_average_precision_hashtag_keybert))\n",
        "\n",
        "mean_average_recall_hashtag_keybert = mean_average_recall(20, u_base_hashtag, u_time, relevant_docs_keybert, docs_preprocessed)\n",
        "print('Mean Average Recall KeyBERT Model: {}'.format(mean_average_recall_hashtag_keybert))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZVm61NCUm0Ju",
        "outputId": "fa659dd9-58cd-4b69-9645-1398adc9555c"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rank: 1 Relevant Document: ('hijab_288.json', 0.010416666666666666, '2022-02-19')\n",
            "Rank: 2 Relevant Document: ('hijab_289.json', 0.0058823529411764705, '2022-02-19')\n",
            "Rank: 3 Relevant Document: ('hijab_282.json', 0.00558659217877095, '2022-02-19')\n",
            "Rank: 4 Relevant Document: ('hijab_287.json', 0.005494505494505495, '2022-02-19')\n",
            "Rank: 5 Relevant Document: ('hijab_283.json', 0.005235602094240838, '2022-02-19')\n",
            "Rank: 6 Relevant Document: ('hijab_281.json', 0.004672897196261682, '2022-02-19')\n",
            "Rank: 7 Relevant Document: ('hijab_290.json', 0.004672897196261682, '2022-02-19')\n",
            "Rank: 8 Relevant Document: ('hijab_285.json', 0.0037735849056603774, '2022-02-19')\n",
            "Rank: 9 Relevant Document: ('hijab_286.json', 0.003205128205128205, '2022-02-19')\n",
            "Rank: 10 Relevant Document: ('hijab_284.json', 0.0027472527472527475, '2022-02-19')\n",
            "Rank: 11 Relevant Document: ('SuperBowl_10.json', 0.0, '2022-02-20')\n",
            "Rank: 12 Relevant Document: ('MultiverseOfMadness_16.json', 0.0, '2022-02-18')\n",
            "Rank: 13 Relevant Document: ('Eminem_23.json', 0.0, '2022-02-18')\n",
            "Rank: 14 Relevant Document: ('JohnsonOut21_44.json', 0.0, '2022-02-19')\n",
            "Rank: 15 Relevant Document: ('JohnsonOut21_45.json', 0.0, '2022-02-20')\n",
            "Rank: 16 Relevant Document: ('JohnsonOut21_47.json', 0.0, '2022-02-20')\n",
            "Rank: 17 Relevant Document: ('JohnsonOut21_50.json', 0.0, '2022-02-20')\n",
            "Rank: 18 Relevant Document: ('Cyberpunk2077_57.json', 0.0, '2022-02-18')\n",
            "Rank: 19 Relevant Document: ('Cyberpunk2077_58.json', 0.0, '2022-02-18')\n",
            "Rank: 20 Relevant Document: ('DeepSidhu_76.json', 0.0, '2022-02-19')\n",
            "\n",
            "Mean Average Precision KeyBERT Model: 0.834385701587714\n",
            "Mean Average Recall KeyBERT Model: 0.3875\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Across all hashtags**"
      ],
      "metadata": {
        "id": "mo9GL9WDh4Dm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# global_list = [['narendramodi', '2022-02-14', 'india'],['UkraineRussiaCrisis', '2022-02-14', 'ukraine'],['IPL', '2022-02-14', 'india'],['TaylorSwift', '2022-02-14', 'USA'],['IndiaFightsCorona', '2022-02-14', 'india'],['narendramodi', '2022-02-15', 'india'],['UkraineRussiaCrisis', '2022-02-15', 'ukraine'],['IPL', '2022-02-15', 'india'],['TaylorSwift', '2022-02-15', 'USA'],['IndiaFightsCorona', '2022-02-15', 'india'],['narendramodi', '2022-02-16', 'india'],['UkraineRussiaCrisis', '2022-02-16', 'ukraine'],['IPL', '2022-02-16', 'india'],['TaylorSwift', '2022-02-16', 'USA'],['IndiaFightsCorona', '2022-02-16', 'india'],['narendramodi', '2022-02-17', 'india'],['UkraineRussiaCrisis', '2022-02-17', 'ukraine'],['IPL', '2022-02-17', 'india'],['TaylorSwift', '2022-02-17', 'USA'],['IndiaFightsCorona', '2022-02-17', 'india'],['narendramodi', '2022-02-18', 'india'],['UkraineRussiaCrisis', '2022-02-18', 'ukraine'],['IPL', '2022-02-18', 'india'],['TaylorSwift', '2022-02-18', 'USA'],['IndiaFightsCorona', '2022-02-18', 'india'],['narendramodi', '2022-02-19', 'india'],['UkraineRussiaCrisis', '2022-02-19', 'ukraine'],['IPL', '2022-02-19', 'india'],['hijab', '2022-02-19', 'india'],['vaccine', '2022-02-19', 'india'],['MillionAtIndiaPavilion', '2022-02-14', 'UAE'],['PunjabPanjeNaal', '2022-02-14', 'India'],['Euphoria', '2022-02-14', 'World'],['OscarsFanFavorite', '2022-02-14', 'World'],['ShameOnBirenSingh', '2022-02-14', 'india'],['BappiLahiri', '2022-02-16', 'india'],['BlandDoritos', '2022-02-16', 'USA'],['VERZUZ', '2022-02-16', 'USA'],['DragRaceUK', '2022-02-16', 'United Kingdom'],['BoycottWalgreens', '2022-02-18', 'USA'],['PunjabElections2022', '2022-02-20', 'india'],['WriddhimanSaha', '2022-02-20', 'india'],['stormfranklin', '2022-02-20', 'USA'],['QueenElizabeth', '2022-02-20', 'United Kingdom'],['ScottyFromWelding', '2022-02-20', 'Australia'],['CarabaoCupFinal', '2022-02-27', 'London'],['NZvSA', '2022-02-28', 'New Zealand'],['IPCC', '2022-02-28', 'Worldwide'],['SuperBowl', '2022-02-14', 'USA'],['MultiverseOfMadness', '2022-02-14', 'USA'],['Eminem', '2022-02-14', 'USA'],['IPLAuction', '2022-02-14', 'india'],['JohnsonOut21', '2022-02-14', 'United Kingdom'],['Cyberpunk2077', '2022-02-15', 'Worldwide'],['Wordle242', '2022-02-15', 'Worldwide'],['DeepSidhu', '2022-02-15', 'india'],['CanadaHasFallen', '2022-02-15', 'canada'],['IStandWithTrudeau', '2022-02-15', 'canada'],['CNNPHVPDebate', '2022-02-26', 'philippines'],['qldfloods', '2022-02-26', 'australia'],['Eurovision', '2022-02-26', 'worldwide'],['IndiansInUkraine', '2022-02-26', 'india'],['PritiPatel', '2022-02-26', 'united kingdom'],['TaylorCatterall', '2022-02-27', 'united kingdom'],['PSLFinal', '2022-02-27', 'pakistan'],['AustraliaDecides', '2022-02-27', 'australia'],['WorldNGODay', '2022-02-27', 'worldwide'],['TheBatman', '2022-02-28', 'USA'],['NationalScienceDay', '2022-02-28', 'india'],['msdtrong', '2022-02-14', 'india'],['Boycott_ChennaiSuperKings', '2022-02-14', 'india'],['GlanceJio', '2022-02-14', 'india'],['ArabicKuthu', '2022-02-14', 'india'],['Djokovic', '2022-02-15', 'australia'],['Real Madrid', '2022-02-15', 'santiago'],['bighit', '2022-02-15', 'korea'],['Maxwell', '2022-02-15', 'australia'],['mafsau', '2022-02-16', 'australia'],['channi', '2022-02-16', 'punjab'],['ayalaan', '2022-02-16', 'india'],['jkbose', '2022-02-16', 'india'],['HappyBirthdayPrinceSK', '2022-02-16', 'india'],['RandomActsOfKindnessDay', '2022-02-17', 'worldwide'],['happybirthdayjhope', '2022-02-17', 'korea'],['mohsinbaig', '2022-02-17', 'pakistan'],['aewdynamite', '2022-02-17', 'worldwide'],['aaraattu', '2022-02-17', 'india'],['ShivajiJayanti', '2022-02-18', 'india'],['PlotToKillModi', '2022-02-18', 'india'],['NationalDrinkWineDay', '2022-02-18', 'usa'],['HorizonForbiddenWest', '2022-02-18', 'worldwide'],['BoycottWalgreens', '2022-02-18', 'usa'],['CallTheMidwife', '2022-02-20', 'worldwide'],['OperationDudula', '2022-02-20', 'south africa'],['truthsocial', '2022-02-21', 'usa'],['nbaallstar', '2022-02-21', 'usa'],['shivamogga', '2022-02-21', 'india'],['HalftimeShow', '2022-02-14', 'usa'],['OttawaStrong', '2022-02-14', 'canada'],['DrDre', '2022-02-14', 'usa'],['BattleOfBillingsBridge', '2022-02-14', 'usa'],['FullyFaltooNFTdrop', '2022-02-14', 'worldwide'],['AK61', '2022-02-15', 'india'],['sandhyamukherjee', '2022-02-15', 'india'],['MUNBHA', '2022-02-15', 'worldwide'],['nursesstrike', '2022-02-15', 'australia'],['Realme9ProPlus', '2022-02-16', 'worldwide'],['KarnatakaHijabControversy', '2022-02-16', 'india'],['BJPwinningUP', '2022-02-16', 'india'],['Punjab_With_Modi', '2022-02-16', 'india'],['PushpaTheRule', '2022-02-16', 'india'],['RehmanMalik', '2022-02-22', 'india'],['harisrauf', '2022-02-22', 'pakistan'],['Rosettenville', '2022-02-22', 'south africa'],['NFU22', '2022-02-22', 'worldwide'],['justiceforharsha', '2022-02-22', 'india'],['wordle251', '2022-02-24', 'worldwide'],['ARSWOL', '2022-02-24', 'worldwide'],['stopwar', '2022-02-24', 'worldwide'],['PrayForPeace', '2022-02-24', 'worldwide'],['StopPutinNOW', '2022-02-24', 'worldwide'],['TeamGirlsCup', '2022-02-25', 'worldwide'],['Canucks', '2022-02-25', 'worldwide'],['PinkShirtDay', '2022-02-25', 'canada'],['superrugbypacific', '2022-02-25', 'australia']]"
      ],
      "metadata": {
        "id": "3GXOVUfQh2sw"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# global_average_mean_average_precision = []\n",
        "# global_mean_average_recall = []\n",
        "\n",
        "# for iter in tqdm(range(len(global_list))):\n",
        "#   u_base_hashtag = global_list[iter][0]\n",
        "#   u_time = global_list[iter][1]\n",
        "#   u_location = global_list[iter][2]\n",
        "#   tweet_query = []\n",
        "#   format = '%Y-%m-%d'\n",
        "#   u_present_date = datetime.datetime.strptime(u_time, format)\n",
        "#   u_prev_date = u_present_date - datetime.timedelta(days=1)\n",
        "#   u_next_date = u_present_date + datetime.timedelta(days=1)\n",
        "#   df_query = df.loc[df['hashtags'].str.contains(u_base_hashtag) & df['Date_Only'].isin([str(u_present_date.date()), str(u_prev_date.date()), str(u_next_date.date())])]\n",
        "\n",
        "#   for tweet in df_query['Preprocessed_Data']:\n",
        "#     tweet_query.extend(tweet)\n",
        "  \n",
        "#   tweet_keywords = []\n",
        "#   kw_extractor = yake.KeywordExtractor(top=20, stopwords=None)\n",
        "#   keywords = kw_extractor.extract_keywords(' '.join(tweet_query))\n",
        "#   for kw, v in keywords:\n",
        "#     #print(\"Keyphrase: \",kw, \": score\", v)\n",
        "#     for key in kw.split():\n",
        "#       if(key not in tweet_keywords):\n",
        "#         tweet_keywords.append(key)\n",
        "  \n",
        "#   docs_preprocessed = []\n",
        "\n",
        "#   total_documents = 0\n",
        "#   path = '/content/drive/MyDrive/Tweelink_Dataset/Tweelink_Articles_Processed'\n",
        "#   for filename in glob(os.path.join(path, '*')):\n",
        "#     with open(os.path.join(os.getcwd(), filename), 'r', encoding = 'utf-8',errors = 'ignore') as f:\n",
        "#       filename = os.path.basename(f.name)\n",
        "#       data = json.load(f)\n",
        "#       d_date = data[\"Date\"]\n",
        "#       if(d_date==\"\" or d_date==\"Date\"):\n",
        "#         continue\n",
        "#       format = '%Y-%m-%d'\n",
        "  \n",
        "#       d_present_date = datetime.datetime.strptime(d_date, format)\n",
        "  \n",
        "#       if(str(d_present_date.date()) not in [str(u_present_date.date()), str(u_prev_date.date()), str(u_next_date.date())]):\n",
        "#         continue\n",
        "    \n",
        "#       docs_preprocessed.append({'Name':filename, 'Data':data})\n",
        "#       total_documents+=1\n",
        "  \n",
        "#   # with yake\n",
        "#   # relevant_docs = find_relevant_documents(docs_preprocessed, tweet_keywords)\n",
        "\n",
        "#   # without yake/ keyword extraction\n",
        "#   relevant_docs = find_relevant_documents(docs_preprocessed, tweet_query)\n",
        "\n",
        "#   mean_average_precision_hashtag = mean_average_precision(20, u_base_hashtag, u_time, relevant_docs, docs_preprocessed)\n",
        "#   global_average_mean_average_precision.append(mean_average_precision_hashtag)\n",
        "#   #print(mean_average_precision_hashtag)\n",
        "\n",
        "#   mean_average_recall_hashtag = mean_average_recall(20, u_base_hashtag, u_time, relevant_docs, docs_preprocessed)\n",
        "#   global_mean_average_recall.append(mean_average_recall_hashtag)\n",
        "#   #print(mean_average_recall_hashtag)"
      ],
      "metadata": {
        "id": "tkcWxb70iYqr"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# overall_average_mean_average_precision = sum(global_average_mean_average_precision)/len(global_average_mean_average_precision)\n",
        "# print(overall_average_mean_average_precision)"
      ],
      "metadata": {
        "id": "CDQAIITBtCLf"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# overall_mean_average_recall = sum(global_mean_average_recall)/len(global_mean_average_recall)\n",
        "# print(overall_mean_average_recall)"
      ],
      "metadata": {
        "id": "xlqnJFdUtgf4"
      },
      "execution_count": 30,
      "outputs": []
    }
  ]
}