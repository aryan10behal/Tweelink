{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "final_results_TF_IDF_raw_count.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "RqDe6xYgIsiJ",
        "uNI-Iyv5JFEw",
        "N7WnKuv6JSAp",
        "R9aQPrqHKGhX",
        "TaJY3wKoMyb3",
        "o_8Ca8euL2Kx",
        "kugqQGpIVaoK",
        "xj1XeLTyfE59",
        "6Vw7UZm_fkxN",
        "htbzsPDnf5IU",
        "06kAPEuQgIAM"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Import Statements"
      ],
      "metadata": {
        "id": "RqDe6xYgIsiJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3aALpjA5Iiri",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7cbbd087-1572-4c73-9a9c-1840ae5f68ca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Error loading corpus: Package 'corpus' not found in index\n",
            "Collecting git+https://github.com/LIAAD/yake\n",
            "  Cloning https://github.com/LIAAD/yake to /tmp/pip-req-build-9_8_iqsr\n",
            "  Running command git clone -q https://github.com/LIAAD/yake /tmp/pip-req-build-9_8_iqsr\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.7/dist-packages (from yake==0.4.8) (0.8.9)\n",
            "Requirement already satisfied: click>=6.0 in /usr/local/lib/python3.7/dist-packages (from yake==0.4.8) (7.1.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from yake==0.4.8) (1.21.6)\n",
            "Collecting segtok\n",
            "  Downloading segtok-1.5.11-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.7/dist-packages (from yake==0.4.8) (2.6.3)\n",
            "Collecting jellyfish\n",
            "  Downloading jellyfish-0.9.0.tar.gz (132 kB)\n",
            "\u001b[K     |████████████████████████████████| 132 kB 6.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from segtok->yake==0.4.8) (2019.12.20)\n",
            "Building wheels for collected packages: yake, jellyfish\n",
            "  Building wheel for yake (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for yake: filename=yake-0.4.8-py2.py3-none-any.whl size=62565 sha256=4760feae55ad47ccee89fddf3aa582317f5f7934e29eee79b9707467c25ba9a1\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-6pga_43t/wheels/52/79/f4/dae9309f60266aa3767a4381405002b6f2955fbcf038d804da\n",
            "  Building wheel for jellyfish (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for jellyfish: filename=jellyfish-0.9.0-cp37-cp37m-linux_x86_64.whl size=73966 sha256=08341b7149276765e0b07595799e77beaf23cfb876c47e00fc66281e4c4f9168\n",
            "  Stored in directory: /root/.cache/pip/wheels/fe/99/4e/646ce766df0d070b0ef04db27aa11543e2767fda3075aec31b\n",
            "Successfully built yake jellyfish\n",
            "Installing collected packages: segtok, jellyfish, yake\n",
            "Successfully installed jellyfish-0.9.0 segtok-1.5.11 yake-0.4.8\n",
            "Collecting multi_rake\n",
            "  Downloading multi_rake-0.0.2-py3-none-any.whl (31 kB)\n",
            "Requirement already satisfied: numpy>=1.14.4 in /usr/local/lib/python3.7/dist-packages (from multi_rake) (1.21.6)\n",
            "Collecting pycld2>=0.41\n",
            "  Downloading pycld2-0.41.tar.gz (41.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 41.4 MB 1.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyrsistent>=0.14.2 in /usr/local/lib/python3.7/dist-packages (from multi_rake) (0.18.1)\n",
            "Requirement already satisfied: regex>=2018.6.6 in /usr/local/lib/python3.7/dist-packages (from multi_rake) (2019.12.20)\n",
            "Building wheels for collected packages: pycld2\n",
            "  Building wheel for pycld2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pycld2: filename=pycld2-0.41-cp37-cp37m-linux_x86_64.whl size=9834248 sha256=27be7c3b48fde850e4df033bba8a5ff16a04e1accf8a0d48b778dc53dee4a5a5\n",
            "  Stored in directory: /root/.cache/pip/wheels/ed/e4/58/ed2e9f43c07d617cc81fe7aff0fc6e42b16c9cf6afe960b614\n",
            "Successfully built pycld2\n",
            "Installing collected packages: pycld2, multi-rake\n",
            "Successfully installed multi-rake-0.0.2 pycld2-0.41\n",
            "Collecting summa\n",
            "  Downloading summa-1.2.0.tar.gz (54 kB)\n",
            "\u001b[K     |████████████████████████████████| 54 kB 1.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy>=0.19 in /usr/local/lib/python3.7/dist-packages (from summa) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from scipy>=0.19->summa) (1.21.6)\n",
            "Building wheels for collected packages: summa\n",
            "  Building wheel for summa (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for summa: filename=summa-1.2.0-py3-none-any.whl size=54412 sha256=29de8ae143bb4afbc423ea43394ff4f8d89e0a812a217e9a7f3f223cd5a8b30b\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/64/ac/7b443477588d365ef37ada30d456bdf5f07dc5be9f6324cb6e\n",
            "Successfully built summa\n",
            "Installing collected packages: summa\n",
            "Successfully installed summa-1.2.0\n",
            "Collecting keybert\n",
            "  Downloading keybert-0.5.1.tar.gz (19 kB)\n",
            "Collecting sentence-transformers>=0.3.8\n",
            "  Downloading sentence-transformers-2.2.0.tar.gz (79 kB)\n",
            "\u001b[K     |████████████████████████████████| 79 kB 3.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scikit-learn>=0.22.2 in /usr/local/lib/python3.7/dist-packages (from keybert) (1.0.2)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.7/dist-packages (from keybert) (1.21.6)\n",
            "Collecting rich>=10.4.0\n",
            "  Downloading rich-12.2.0-py3-none-any.whl (229 kB)\n",
            "\u001b[K     |████████████████████████████████| 229 kB 35.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions<5.0,>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from rich>=10.4.0->keybert) (4.1.1)\n",
            "Collecting commonmark<0.10.0,>=0.9.0\n",
            "  Downloading commonmark-0.9.1-py2.py3-none-any.whl (51 kB)\n",
            "\u001b[K     |████████████████████████████████| 51 kB 5.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pygments<3.0.0,>=2.6.0 in /usr/local/lib/python3.7/dist-packages (from rich>=10.4.0->keybert) (2.6.1)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.22.2->keybert) (1.4.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.22.2->keybert) (1.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.22.2->keybert) (3.1.0)\n",
            "Collecting transformers<5.0.0,>=4.6.0\n",
            "  Downloading transformers-4.18.0-py3-none-any.whl (4.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.0 MB 47.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from sentence-transformers>=0.3.8->keybert) (4.64.0)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from sentence-transformers>=0.3.8->keybert) (1.10.0+cu111)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from sentence-transformers>=0.3.8->keybert) (0.11.1+cu111)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from sentence-transformers>=0.3.8->keybert) (3.2.5)\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 53.5 MB/s \n",
            "\u001b[?25hCollecting huggingface-hub\n",
            "  Downloading huggingface_hub-0.5.1-py3-none-any.whl (77 kB)\n",
            "\u001b[K     |████████████████████████████████| 77 kB 4.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers>=0.3.8->keybert) (2.23.0)\n",
            "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
            "  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.6 MB 35.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers>=0.3.8->keybert) (21.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers>=0.3.8->keybert) (2019.12.20)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.49-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 63.1 MB/s \n",
            "\u001b[?25hCollecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 64.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers>=0.3.8->keybert) (3.6.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers>=0.3.8->keybert) (4.11.3)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers<5.0.0,>=4.6.0->sentence-transformers>=0.3.8->keybert) (3.0.8)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers<5.0.0,>=4.6.0->sentence-transformers>=0.3.8->keybert) (3.8.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk->sentence-transformers>=0.3.8->keybert) (1.15.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers>=0.3.8->keybert) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers>=0.3.8->keybert) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers>=0.3.8->keybert) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers>=0.3.8->keybert) (2.10)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers<5.0.0,>=4.6.0->sentence-transformers>=0.3.8->keybert) (7.1.2)\n",
            "Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->sentence-transformers>=0.3.8->keybert) (7.1.2)\n",
            "Building wheels for collected packages: keybert, sentence-transformers\n",
            "  Building wheel for keybert (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keybert: filename=keybert-0.5.1-py3-none-any.whl size=21332 sha256=bbbbc8428905048afacd1760340641e1a99ded86273f8dde0d5a983bdd4cd319\n",
            "  Stored in directory: /root/.cache/pip/wheels/8e/95/c5/f5ceed2a9f9e80bc1a706a10a6fb03d726df7a3dd11800a58b\n",
            "  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sentence-transformers: filename=sentence_transformers-2.2.0-py3-none-any.whl size=120747 sha256=1af0ac6cff472cbb82ae7831f38fb7b22798af7d011fd312e9e30dd5d1d2342c\n",
            "  Stored in directory: /root/.cache/pip/wheels/83/c0/df/b6873ab7aac3f2465aa9144b6b4c41c4391cfecc027c8b07e7\n",
            "Successfully built keybert sentence-transformers\n",
            "Installing collected packages: pyyaml, tokenizers, sacremoses, huggingface-hub, transformers, sentencepiece, commonmark, sentence-transformers, rich, keybert\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed commonmark-0.9.1 huggingface-hub-0.5.1 keybert-0.5.1 pyyaml-6.0 rich-12.2.0 sacremoses-0.0.49 sentence-transformers-2.2.0 sentencepiece-0.1.96 tokenizers-0.12.1 transformers-4.18.0\n"
          ]
        }
      ],
      "source": [
        "#Importing essential libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import csv\n",
        "import json\n",
        "from itertools import islice\n",
        "from collections import OrderedDict\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import os\n",
        "import nltk\n",
        "from glob import glob\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "from tqdm import tqdm\n",
        "import pickle\n",
        "import math\n",
        "import re\n",
        "from sklearn.model_selection import train_test_split\n",
        "import operator\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('corpus')\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from scipy import stats\n",
        "! pip install git+https://github.com/LIAAD/yake\n",
        "import yake\n",
        "! pip install multi_rake\n",
        "from multi_rake import Rake\n",
        "! pip install summa\n",
        "from summa import keywords as summa_keywords\n",
        "! pip install keybert\n",
        "from keybert import KeyBERT"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "id": "l0oFppN1I3-S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "741c6c3d-834f-4cc0-ddf4-0112e70183d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "AcEEIHJjI5gk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6c40a32c-fdb0-4273-8e95-a5e4eaaeb4b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "file1 = open(\"/content/drive/MyDrive/Tweelink_Dataset/twitter_base_preprocessed.pkl\", \"rb\")\n",
        "df = pickle.load(file1)\n",
        "file1.close()"
      ],
      "metadata": {
        "id": "GCcUm5luI54-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Input"
      ],
      "metadata": {
        "id": "uNI-Iyv5JFEw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "u_base_hashtag = input(\"Enter base hashtag: \")\n",
        "u_time = input(\"Enter time: \")\n",
        "u_location = input(\"Enter Location: \")"
      ],
      "metadata": {
        "id": "iE_ciW51JBA_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f37d663-8bd4-42b0-f08c-8f4f3003d1bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter base hashtag: hijab\n",
            "Enter time: 2022-02-19\n",
            "Enter Location: india\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Processing"
      ],
      "metadata": {
        "id": "N7WnKuv6JSAp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import datetime\n",
        "tweet_query = []\n",
        "format = '%Y-%m-%d'\n",
        "u_present_date = datetime.datetime.strptime(u_time, format)\n",
        "u_prev_date = u_present_date - datetime.timedelta(days=1)\n",
        "u_next_date = u_present_date + datetime.timedelta(days=1)\n",
        "df_query = df.loc[df['hashtags'].str.contains(u_base_hashtag) & df['Date_Only'].isin([str(u_present_date.date()), str(u_prev_date.date()), str(u_next_date.date())])]\n",
        "print(df_query.shape[0])\n",
        "if df_query.shape[0]<50:\n",
        "  df_query = df.loc[df['Date_Only'].isin([str(u_present_date.date()), str(u_prev_date.date()), str(u_next_date.date())])]\n",
        "  df_query = df.iloc[:min(df_query.shape[0],1000),:]"
      ],
      "metadata": {
        "id": "Ro6ffcW_JpQQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0e446d04-ca52-4567-9f01-27234c88acdd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "264\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def keyword_extractor(dataset):\n",
        "  preprocessed_vocabulary = dict()\n",
        "\n",
        "  #Converting to lowercase\n",
        "  def to_lower_case(text):\n",
        "    text = text.lower()\n",
        "    return text\n",
        "\n",
        "  def remove_at_word(text):\n",
        "    data = text.split()\n",
        "    data = [d for d in data if d[0]!='@']\n",
        "    text = ' '.join(data)\n",
        "    return text\n",
        "\n",
        "  def remove_hashtag(text):\n",
        "    data = text.split()\n",
        "    data = [d if (d[0]!='#' or len(d) == 1) else d[1:] for d in data]\n",
        "    data = [d for d in data if d[0]!='#']\n",
        "    text = ' '.join(data)\n",
        "    return text\n",
        "\n",
        "  def remove_URL(text):\n",
        "    text = re.sub(r\"http\\S+\", \"\", text)\n",
        "    text = re.sub(r'bit.ly\\S+', '', text, flags=re.MULTILINE)\n",
        "    return text\n",
        "\n",
        "  #Removing stopwords\n",
        "  def remove_stopwords(text):\n",
        "    stopword = stopwords.words('english')\n",
        "    new_list = [x for x in text.split() if x not in stopword]\n",
        "    return ' '.join(new_list)\n",
        "\n",
        "  #Removing punctuations\n",
        "  def remove_punctuations(text):\n",
        "    punctuations = '''!()-[|]`{};:'\"\\,<>./?@#$=+%^&*_~'''\n",
        "    new_list = ['' if x in punctuations else x for x in text.split()]\n",
        "    new_list_final = []\n",
        "    for token in new_list:\n",
        "      new_token=\"\"\n",
        "      for char in token:\n",
        "        if(char not in punctuations):\n",
        "          new_token+=char\n",
        "      if(len(new_token)!=0):\n",
        "        new_list_final.append(new_token)\n",
        "    return ' '.join(new_list_final)\n",
        "\n",
        "  #Tokenization\n",
        "  def tokenization(text):\n",
        "    return word_tokenize(text)\n",
        "\n",
        "  def pre_process(text):\n",
        "    text = to_lower_case(text)\n",
        "    text = remove_at_word(text)\n",
        "    text = remove_hashtag(text)\n",
        "    text = remove_URL(text)\n",
        "    text = remove_stopwords(text)\n",
        "    text = remove_punctuations(text)\n",
        "    text = tokenization(text)\n",
        "    for token in text:\n",
        "      if token in preprocessed_vocabulary.keys():\n",
        "        preprocessed_vocabulary[token] += 1\n",
        "      else:\n",
        "        preprocessed_vocabulary[token] = 1\n",
        "    return text\n",
        "  \n",
        "  preprocessed_data = [pre_process(text) for text in dataset]\n",
        "\n",
        "  #print(preprocessed_vocabulary)\n",
        "\n",
        "  AOF_coefficient = sum(preprocessed_vocabulary.values())/len(preprocessed_vocabulary)\n",
        "  vocabulary = {token.strip():preprocessed_vocabulary[token] for token in preprocessed_vocabulary.keys() if preprocessed_vocabulary[token] > AOF_coefficient and len(token.strip())}\n",
        "\n",
        "  #print(vocabulary)\n",
        "\n",
        "  final_tokens_per_tweet = []\n",
        "  for data in preprocessed_data:\n",
        "    final_tokens_per_tweet.append([token for token in data if token in vocabulary.keys()])\n",
        "\n",
        "  #print(preprocessed_data)\n",
        "  #print(final_tokens_per_tweet)\n",
        "\n",
        "  word2id = dict()\n",
        "  id2word = dict()\n",
        "  vocabulary_size = len(vocabulary)\n",
        "  count = 0\n",
        "  for token in vocabulary.keys():\n",
        "    word2id[token] = count\n",
        "    id2word[count] = token\n",
        "    count += 1\n",
        "\n",
        "  #print(word2id)\n",
        "  #print(id2word)\n",
        "\n",
        "  directed_graph_adjacency_matrix = np.zeros((vocabulary_size, vocabulary_size))\n",
        "  edge_weight_matrix = np.zeros((vocabulary_size, vocabulary_size))\n",
        "  first_frequency = dict()\n",
        "  last_frequency = dict()\n",
        "  term_frequency = vocabulary\n",
        "  strength = dict()\n",
        "  degree = dict()\n",
        "  selective_centraility = dict()\n",
        "\n",
        "\n",
        "  for tweet in final_tokens_per_tweet:\n",
        "    if len(tweet)<1:\n",
        "      continue\n",
        "\n",
        "    if tweet[0] in first_frequency.keys():\n",
        "      first_frequency[tweet[0]] += 1\n",
        "    else:\n",
        "      first_frequency[tweet[0]] = 1\n",
        "\n",
        "    if tweet[-1] in last_frequency.keys():\n",
        "      last_frequency[tweet[-1]] += 1\n",
        "    else:\n",
        "      last_frequency[tweet[-1]] = 1\n",
        "    \n",
        "\n",
        "\n",
        "    for i in range(len(tweet)-1):\n",
        "      if tweet[i] == tweet[i+1]:\n",
        "        continue\n",
        "      x = word2id[tweet[i]]\n",
        "      y = word2id[tweet[i+1]]\n",
        "      directed_graph_adjacency_matrix[x][y] += 1\n",
        "\n",
        "  for tweet in final_tokens_per_tweet:\n",
        "    for i in range(len(tweet)-1):\n",
        "\n",
        "\n",
        "      if tweet[i] == tweet[i+1]:\n",
        "        continue\n",
        "      x = word2id[tweet[i]]\n",
        "      y = word2id[tweet[i+1]]\n",
        "\n",
        "    # Updating degree..\n",
        "      if tweet[i] in degree.keys():\n",
        "        degree[tweet[i]] += 1\n",
        "      else:\n",
        "        degree[tweet[i]] = 1\n",
        "        \n",
        "      if tweet[i+1] in degree.keys():\n",
        "        degree[tweet[i+1]] += 1\n",
        "      else:\n",
        "        degree[tweet[i+1]] = 1\n",
        "\n",
        "      edge_weight_matrix[x][y] = directed_graph_adjacency_matrix[x][y]/(vocabulary[tweet[i]] + vocabulary[tweet[i+1]] - directed_graph_adjacency_matrix[x][y])\n",
        "\n",
        "      if tweet[i] in strength.keys():\n",
        "        strength[tweet[i]] += edge_weight_matrix[x][y]\n",
        "      else:\n",
        "        strength[tweet[i]] = edge_weight_matrix[x][y]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  first_frequency = {token:(first_frequency[token]/vocabulary[token] if token in first_frequency else 0) for token in vocabulary.keys()}\n",
        "  last_frequency = {token:(last_frequency[token]/vocabulary[token] if token in last_frequency else 0) for token in vocabulary.keys()}\n",
        "  degree = {token:(degree[token] if token in degree else 0) for token in vocabulary.keys()}\n",
        "  strength = {token:(strength[token] if token in strength else 0) for token in vocabulary.keys()}\n",
        "  selective_centraility = {token:(strength[token]/degree[token] if degree[token]!=0 else 0) for token in vocabulary.keys()}\n",
        "\n",
        "  #print(degree)\n",
        "  #print(vocabulary)\n",
        "\n",
        "  maxdegree = max(degree.items(), key=lambda x: x[1])[1]\n",
        "  max_degree_nodes_with_freq = {key:term_frequency[key] for key in degree.keys() if degree[key] == maxdegree}\n",
        "  maxfreq = max(max_degree_nodes_with_freq.items(), key=lambda x: x[1])[1]\n",
        "  central_node_name = [key for key in max_degree_nodes_with_freq.keys() if max_degree_nodes_with_freq[key] == maxfreq][0]\n",
        "  #print(\"central node: \", central_node_name)\n",
        "\n",
        "  # bfs\n",
        "  distance_from_central_node = dict()\n",
        "  central_node_id = word2id[central_node_name]\n",
        "  q = [(central_node_id, 0)]\n",
        "\n",
        "  # Set source as visited\n",
        "  distance_from_central_node[central_node_name] = 0\n",
        "\n",
        "  while q:\n",
        "      vis = q[0]\n",
        "      # Print current node\n",
        "      #print(id2word[vis[0]], vis[1])\n",
        "      q.pop(0)\n",
        "        \n",
        "      # For every adjacent vertex to\n",
        "      # the current vertex\n",
        "      for i in range(len(directed_graph_adjacency_matrix[vis[0]])):\n",
        "          if (directed_graph_adjacency_matrix[vis[0]][i] == 1 and (id2word[i] not in distance_from_central_node.keys())):\n",
        "              # Push the adjacent node\n",
        "              # in the queue\n",
        "              q.append((i, vis[1]+1))\n",
        "              distance_from_central_node[id2word[i]] = vis[1]+1\n",
        "\n",
        "  #print(distance_from_central_node)\n",
        "  inverse_distance_from_central_node = {token:(1/distance_from_central_node[token] if token in distance_from_central_node and token != central_node_name else 0) for token in vocabulary.keys()}\n",
        "  inverse_distance_from_central_node[central_node_name] = 1.0\n",
        "  #print(inverse_distance_from_central_node)\n",
        "\n",
        "  neighbour_importance = dict()\n",
        "\n",
        "  for i in range(len(directed_graph_adjacency_matrix)):\n",
        "    neighbours = set()\n",
        "\n",
        "    # traversing outgoing edges\n",
        "    for j in range(len(directed_graph_adjacency_matrix)):\n",
        "      if i == j:\n",
        "        continue\n",
        "      if directed_graph_adjacency_matrix[i][j] > 0:\n",
        "        neighbours.add(j)\n",
        "    for j in range(len(directed_graph_adjacency_matrix)):\n",
        "      if i == j:\n",
        "        continue\n",
        "      if directed_graph_adjacency_matrix[j][i] > 0:\n",
        "          neighbours.add(j)\n",
        "    if len(neighbours) != 0:\n",
        "      neighbour_importance[id2word[i]] = sum([strength[id2word[j]] for j in neighbours])/len(neighbours)\n",
        "    else:\n",
        "      neighbour_importance[id2word[i]] = 0\n",
        "      \n",
        "  #print(neighbour_importance)\n",
        "\n",
        "  unnormalized_node_weight = {node: (first_frequency[node] + last_frequency[node] + term_frequency[node] + selective_centraility[node] + inverse_distance_from_central_node[node] + neighbour_importance[node]) for node in vocabulary.keys()}\n",
        "  max_node_weight = max(unnormalized_node_weight.items(), key=lambda x: x[1])[1]\n",
        "  min_node_weight = min(unnormalized_node_weight.items(), key=lambda x: x[1])[1]\n",
        "  #print(\"max node weight: \", max_node_weight, \"min node weight: \", min_node_weight)\n",
        "  normalized_node_weight = {node: ((unnormalized_node_weight[node] - min_node_weight)/(max_node_weight - min_node_weight) if max_node_weight != min_node_weight else unnormalized_node_weight[node]) for node in unnormalized_node_weight.keys()}\n",
        "  #print(\"Unnormalized score: \", unnormalized_node_weight)\n",
        "  #print(\"Normalized score: \", normalized_node_weight)\n",
        "\n",
        "  damping_factor = 0.85\n",
        "  relevance_of_node = {node: np.random.uniform(0,1,1)[0] for node in vocabulary.keys()}\n",
        "  threshold = 0.000000001\n",
        "\n",
        "\n",
        "  #print(relevance_of_node)\n",
        "\n",
        "  count = 0\n",
        "  while True:\n",
        "    count += 1\n",
        "    current_relevance_of_node = dict()\n",
        "    for node in vocabulary.keys():\n",
        "      outer_sum = 0\n",
        "      node_idx = word2id[node]\n",
        "      for j in range(len(directed_graph_adjacency_matrix)):\n",
        "        if j == node_idx:\n",
        "          continue\n",
        "        if directed_graph_adjacency_matrix[j][node_idx] > 0:\n",
        "          den_sum = 0\n",
        "          for k in range(len(directed_graph_adjacency_matrix)):\n",
        "            if k == j:\n",
        "              continue\n",
        "            den_sum += directed_graph_adjacency_matrix[j][k]\n",
        "          outer_sum += ((directed_graph_adjacency_matrix[j][node_idx]/den_sum) * relevance_of_node[id2word[j]])\n",
        "      current_relevance_of_node[node] = (1-damping_factor)*normalized_node_weight[node] + damping_factor*normalized_node_weight[node]*outer_sum\n",
        "    \n",
        "\n",
        "    # checking convergence..\n",
        "    sq_error = sum([(current_relevance_of_node[node] - relevance_of_node[node])**2 for node in vocabulary.keys()])\n",
        "    relevance_of_node = current_relevance_of_node\n",
        "    if sq_error < threshold:\n",
        "      break\n",
        "\n",
        "  #print(relevance_of_node)\n",
        "  #print(count)\n",
        "\n",
        "  degree_centrality  = {node: 0 for node in vocabulary.keys()}\n",
        "\n",
        "  if len(directed_graph_adjacency_matrix) > 1:\n",
        "    for i in range(len(directed_graph_adjacency_matrix)):\n",
        "      count = 0\n",
        "      for j in range(len(directed_graph_adjacency_matrix)):\n",
        "        if i == j:\n",
        "          continue\n",
        "        if directed_graph_adjacency_matrix[j][i] > 0:\n",
        "          count += 1\n",
        "      degree_centrality[id2word[i]] = count / (len(directed_graph_adjacency_matrix)-1)\n",
        "\n",
        "  #print(degree_centrality)\n",
        "\n",
        "  final_keyword_rank = [{'node': node, 'NE_rank': relevance_of_node[node], 'Degree': degree_centrality[node]} for node in vocabulary.keys()]\n",
        "\n",
        "  #print(\"-----------\")\n",
        "  final_keyword_rank = sorted(final_keyword_rank, key = lambda i: (i['NE_rank'], i['Degree']), reverse = True)\n",
        "\n",
        "  final_keywords = [keyword['node'] for keyword in final_keyword_rank]\n",
        "\n",
        "  return final_keywords"
      ],
      "metadata": {
        "id": "qyYTFrfXJrko"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for tweet in df_query['Preprocessed_Data']:\n",
        "  tweet_query.extend(tweet)"
      ],
      "metadata": {
        "id": "MA4RjZJiJtlw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "keyword_dataset = df_query['tweet'].tolist()\n",
        "tweet_query_keyword_extractor = keyword_extractor(keyword_dataset)"
      ],
      "metadata": {
        "id": "Esocp_vkJvYA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tweet_keywords_yake = []\n",
        "kw_extractor = yake.KeywordExtractor(top=20, stopwords=None)\n",
        "keywords = kw_extractor.extract_keywords(' '.join(tweet_query))\n",
        "#keywords = kw_extractor.extract_keywords(' '.join(df_query['tweet'].tolist()))\n",
        "for kw, v in keywords:\n",
        "  print(\"Keyphrase: \",kw, \": score\", v)\n",
        "  for key in kw.split():\n",
        "    if(key.lower() not in tweet_keywords_yake):\n",
        "      tweet_keywords_yake.append(key.lower())"
      ],
      "metadata": {
        "id": "15Zm6j2bJxYg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eed70e6b-9844-414d-e085-27baf6003b1f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keyphrase:  hijab hijab hijab : score 7.93098251569731e-07\n",
            "Keyphrase:  hijab karnataka india : score 1.5697747232782718e-06\n",
            "Keyphrase:  muslims hijab started : score 1.6442080038241414e-06\n",
            "Keyphrase:  persecution muslims hijab : score 1.8404918384483932e-06\n",
            "Keyphrase:  muslims islam muslim : score 1.917070007737707e-06\n",
            "Keyphrase:  hijab started india : score 2.0983196417080144e-06\n",
            "Keyphrase:  india muslim world : score 2.278438414831808e-06\n",
            "Keyphrase:  wear hijab hijab : score 2.381368646009185e-06\n",
            "Keyphrase:  started india muslim : score 2.6379691133901388e-06\n",
            "Keyphrase:  wear hijab schools : score 2.6722056209623396e-06\n",
            "Keyphrase:  muslims hijab hijab : score 2.9907514385679128e-06\n",
            "Keyphrase:  hijab hijabisfundamentalright india : score 3.092358441392823e-06\n",
            "Keyphrase:  wearing hijab hijab : score 3.2729984537171845e-06\n",
            "Keyphrase:  hijab schools colleges : score 3.3492161131480414e-06\n",
            "Keyphrase:  islam muslim allah : score 3.642941755792867e-06\n",
            "Keyphrase:  hijab abaya muslim : score 4.0776510485973575e-06\n",
            "Keyphrase:  muslim women india : score 4.122753760151645e-06\n",
            "Keyphrase:  muslims india hijab : score 4.796942600854159e-06\n",
            "Keyphrase:  hijab muslims india : score 4.796942600854159e-06\n",
            "Keyphrase:  girls wear hijab : score 5.01291274452287e-06\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tweet_keywords_rake = []\n",
        "rake = Rake()\n",
        "rake_keywords = rake.apply(' '.join(tweet_query).encode('ascii', 'ignore').decode())\n",
        "for kw,score in rake_keywords[:20]:\n",
        "  for key in kw.split():\n",
        "    if(key.lower() not in tweet_keywords_yake):\n",
        "      tweet_keywords_rake.append(key.lower())"
      ],
      "metadata": {
        "id": "iRzj4aklJ0Hw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tweet_keywords_text_rank = []\n",
        "TR_keywords = summa_keywords.keywords(' '.join(tweet_query), scores=True)\n",
        "for kw,score in TR_keywords[:20]:\n",
        "  for key in kw.split():\n",
        "    if(key.lower() not in tweet_keywords_text_rank):\n",
        "      tweet_keywords_text_rank.append(key.lower())\n",
        "print(tweet_keywords_text_rank)"
      ],
      "metadata": {
        "id": "HjnU20IkJ3OD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "433a6172-17ae-4232-da79-28fc5eb656e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['hijabers', 'hijabs', 'hijab', 'hate', 'womens', 'india', 'voice', 'muslim', 'women', 'wearing', 'wears', 'schools', 'indias', 'islamic', 'amp', 'like', 'likes', 'karnatakas', 'protest', 'local', 'dallas', 'muslims', 'girl', 'posts', 'hijabrow', 'educated', 'girls', 'suddenly', 'wanted', 'bans']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# uncomment later\n",
        "# keybert_model = KeyBERT(model='all-mpnet-base-v2')\n",
        "# keybert_keywords = keybert_model.extract_keywords(' '.join(tweet_query), keyphrase_ngram_range=(1,1), stop_words='english', highlight=False, top_n=20)\n",
        "# tweet_keywords_keybert = list(dict(keybert_keywords).keys())\n",
        "# print(tweet_keywords_keybert)"
      ],
      "metadata": {
        "id": "RtXmVDsVJ3nX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Helpful Functions"
      ],
      "metadata": {
        "id": "R9aQPrqHKGhX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "docs_preprocessed = []"
      ],
      "metadata": {
        "id": "M7uQcsyrJJrH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Storing file name and data\n",
        "total_documents = 0\n",
        "path = '/content/drive/MyDrive/Tweelink_Dataset/Tweelink_Articles_Processed'\n",
        "for filename in glob(os.path.join(path, '*')):\n",
        "   with open(os.path.join(os.getcwd(), filename), 'r', encoding = 'utf-8',errors = 'ignore') as f:\n",
        "     filename = os.path.basename(f.name)\n",
        "     data = json.load(f)\n",
        "     d_date = data[\"Date\"]\n",
        "     if(d_date==\"\" or d_date==\"Date\"):\n",
        "       continue\n",
        "     format = '%Y-%m-%d'\n",
        " \n",
        "     try:\n",
        "       d_present_date = datetime.datetime.strptime(d_date, format)\n",
        "     except:\n",
        "       continue\n",
        " \n",
        "     if(str(d_present_date.date()) not in [str(u_present_date.date()), str(u_prev_date.date()), str(u_next_date.date())]):\n",
        "       continue\n",
        "   \n",
        "     docs_preprocessed.append({'Name':filename, 'Data':data})\n",
        "     total_documents+=1\n",
        "print(total_documents)"
      ],
      "metadata": {
        "id": "GV6IjZhlJ6fY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a0065778-8682-40e1-b867-ec40db571d23"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "398\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_relevant_docs_list_for_base_hashtag(base_hashtag, base_date, docs_preprocessed):\n",
        "  relevant_docs_list = []\n",
        "  for doc in docs_preprocessed:\n",
        "    if doc['Data']['Base Hashtag']==base_hashtag:\n",
        "      current_date = datetime.datetime.strptime(base_date, format)\n",
        "      prev_date = current_date - datetime.timedelta(days=1)\n",
        "      next_date = current_date + datetime.timedelta(days=1)\n",
        "      #if(doc['Data']['Date'] in [str(prev_date.date()), str(current_date.date()), str(next_date.date())]):\n",
        "      #  relevant_docs_list.append((doc['Name'], doc['Data']['Location'].lower()))\n",
        "      if(doc['Data']['Date']==str(str(current_date.date()))):\n",
        "        relevant_docs_list.append((doc['Name'], doc['Data']['Location'].lower(), 1.0))\n",
        "      elif(doc['Data']['Date'] in [str(prev_date.date()), str(next_date.date())]):\n",
        "        relevant_docs_list.append((doc['Name'], doc['Data']['Location'].lower(), 0.5))\n",
        "  \n",
        "  # prioritize location\n",
        "  location_relevant = []\n",
        "  location_irrelevant = []\n",
        "  for x in relevant_docs_list:\n",
        "    if u_location.lower() in x[1]:\n",
        "      location_relevant.append(x)\n",
        "    else:\n",
        "      location_irrelevant.append(x)\n",
        "  relevant_docs_list = location_relevant + location_irrelevant\n",
        "  return relevant_docs_list"
      ],
      "metadata": {
        "id": "IXTP86qiK-0I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def nDCG(base_hashtag, base_date, prediction_list, docs_preprocessed):\n",
        "  ground_truth = get_relevant_docs_list_for_base_hashtag(base_hashtag, base_date, docs_preprocessed)\n",
        "  ground_truth_scores = {}\n",
        "  ground_truth_scores_list = []\n",
        "  prediction_list_scores = []\n",
        "  for gt in ground_truth:\n",
        "    ground_truth_scores[gt[0]] = gt[2]\n",
        "    ground_truth_scores_list.append(gt[2])\n",
        "  for x in prediction_list:\n",
        "    if x[0] in ground_truth_scores.keys():\n",
        "      prediction_list_scores.append(ground_truth_scores[x[0]])\n",
        "    else:\n",
        "      prediction_list_scores.append(0.0)\n",
        "  \n",
        "  DCG = prediction_list_scores[0] + sum([prediction_list_scores[i]/np.log2(i+1) for i in range(1,len(prediction_list_scores))])\n",
        "  ideal_DCG = ground_truth_scores_list[0] + sum([ground_truth_scores_list[i]/np.log2(i+1) for i in range(1,len(ground_truth_scores_list))])\n",
        "  if ideal_DCG==0:\n",
        "    return DCG\n",
        "  return DCG/ideal_DCG"
      ],
      "metadata": {
        "id": "rtoNHdl3LjUY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def precision_at_k(k, base_hashtag, base_date, prediction_list, docs_preprocessed):\n",
        "  relevant_docs_list = get_relevant_docs_list_for_base_hashtag(base_hashtag, base_date, docs_preprocessed)\n",
        "  num_of_relevant_results=0\n",
        "  list_of_rel_doc_names = []\n",
        "  for x in relevant_docs_list:\n",
        "    list_of_rel_doc_names.append(x[0])\n",
        "  for itr in range(k):\n",
        "    if (prediction_list[itr][0] in list_of_rel_doc_names):\n",
        "      num_of_relevant_results+=1\n",
        "  return num_of_relevant_results/k"
      ],
      "metadata": {
        "id": "PNanfJBQLl5w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def mean_average_precision(max_k, base_hashtag, base_date, relevant_docs, docs_preprocessed):\n",
        "  average_precision=0\n",
        "  ctr=0\n",
        "  relevant_docs_list = get_relevant_docs_list_for_base_hashtag(base_hashtag, base_date, docs_preprocessed)\n",
        "  print(len(relevant_docs_list))\n",
        "  for k_val in range(1,len(relevant_docs_list)+1):\n",
        "    ctr+=1\n",
        "    if k_val>len(relevant_docs_list):\n",
        "      break\n",
        "    precision_at_k_val = precision_at_k(k_val, base_hashtag, base_date, relevant_docs, docs_preprocessed)\n",
        "    #print('Hashtag: {}   Precision@{}: {}'.format(base_hashtag, k_val, precision_at_k_val))\n",
        "    average_precision += precision_at_k_val\n",
        "  return average_precision/ctr"
      ],
      "metadata": {
        "id": "DIdUyKG5Ln6w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def recall_at_k(k, base_hashtag, base_date, prediction_list, docs_preprocessed):\n",
        "  relevant_docs_list = get_relevant_docs_list_for_base_hashtag(base_hashtag, base_date, docs_preprocessed)\n",
        "  current_num_of_relevant_results=0\n",
        "  list_of_rel_doc_names = []\n",
        "  for x in relevant_docs_list:\n",
        "    list_of_rel_doc_names.append(x[0])\n",
        "  for itr in range(k):\n",
        "    if (prediction_list[itr][0] in list_of_rel_doc_names):\n",
        "      current_num_of_relevant_results+=1\n",
        "  if(len(relevant_docs_list)==0):\n",
        "    return 0\n",
        "  return current_num_of_relevant_results/len(relevant_docs_list)"
      ],
      "metadata": {
        "id": "Vt8J5Q57LqKg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def mean_average_recall(max_k, base_hashtag, base_date, relevant_docs, docs_preprocessed):\n",
        "  average_recall=0\n",
        "  ctr=0\n",
        "  relevant_docs_list = get_relevant_docs_list_for_base_hashtag(base_hashtag, base_date, docs_preprocessed)\n",
        "  print(len(relevant_docs_list))\n",
        "  for k_val in range(1,len(relevant_docs_list)+1):\n",
        "    ctr+=1\n",
        "    if k_val>len(relevant_docs_list):\n",
        "      break\n",
        "    recall_at_k_val = recall_at_k(k_val, base_hashtag, base_date, relevant_docs, docs_preprocessed)\n",
        "    #print('Hashtag: {}   Recall@{}: {}'.format(base_hashtag, k_val, recall_at_k_val))\n",
        "    average_recall += recall_at_k_val\n",
        "  return average_recall/ctr"
      ],
      "metadata": {
        "id": "hsjM7y96LsQ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TF-IDF Raw Count"
      ],
      "metadata": {
        "id": "TaJY3wKoMyb3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Finding vocabulary of our dataset\n",
        "all_words = []\n",
        "for doc in docs_preprocessed:\n",
        "  #print(doc)\n",
        "  tokens = doc['Data']['Body_processed']\n",
        "  all_words.extend(tokens)\n",
        "\n",
        "# finding all distinct words in dataset\n",
        "vocabulary = list(set(all_words))\n",
        "word2id = {}\n",
        "id2word = {}\n",
        "\n",
        "k = 0\n",
        "for word in vocabulary: \n",
        "  word2id[word] = k\n",
        "  k += 1\n",
        "for word in word2id.keys():\n",
        "  id2word[word2id[word]] = word"
      ],
      "metadata": {
        "id": "pIKygn0ILuUY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Word-doc matrix (for storing frequency of each word of vocabulary in document)\n",
        "inverted_index = {}\n",
        "total_words_in_doc = []\n",
        "word_doc_matrix = np.zeros((total_documents, len(vocabulary)), dtype=float)\n",
        "for i in tqdm(range(len(docs_preprocessed))):\n",
        "  # total_words_in_doc.append(len(docs_preprocessed[i]['Text']))\n",
        "  for j, word in enumerate(docs_preprocessed[i]['Data']['Body_processed']):\n",
        "    if(word not in inverted_index):\n",
        "        inverted_index[word] = [i]\n",
        "    else:\n",
        "      if inverted_index[word][-1]!=i:\n",
        "        inverted_index[word].append(i)\n",
        "    word_doc_matrix[i][word2id[word]] += 1\n",
        "  total_words_in_doc.append(sum(word_doc_matrix[i]))"
      ],
      "metadata": {
        "id": "Cnb1lmHLMGEY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1eea7ef0-1e1f-4f3d-c95d-b1882e7231f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 398/398 [00:00<00:00, 512.44it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "max_freq_word_in_doc = []\n",
        "for words_in_doc in word_doc_matrix:\n",
        "  max_freq_word_in_doc.append(np.max(words_in_doc))"
      ],
      "metadata": {
        "id": "suQ_oCbYMqgp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "IDF = {}\n",
        "inverted_index_final = {}\n",
        "for key in inverted_index:\n",
        "  inverted_index_final[key] = {\"Frequency\": len(inverted_index[key]), \"doc_id\":inverted_index[key]}\n",
        "  IDF[key] = np.log10(total_documents/(len(inverted_index[key]) +1))\n",
        "\n",
        "#  sorting according to words\n",
        "inverted_index_final = dict(sorted(inverted_index_final.items()))"
      ],
      "metadata": {
        "id": "_6G4gLg3NvCi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def Raw_count_tf_idf_scheme(word_doc_matrix):\n",
        "  return word_doc_matrix"
      ],
      "metadata": {
        "id": "uBuoPWhwN1wz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Raw_count_tf_idf = Raw_count_tf_idf_scheme(np.copy(word_doc_matrix))"
      ],
      "metadata": {
        "id": "A91KiRSgOE4s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in tqdm(range(len(vocabulary))):\n",
        "  IDF_factor = IDF[id2word[i]]\n",
        "  Raw_count_tf_idf[:,i] *= IDF_factor"
      ],
      "metadata": {
        "id": "im_uZPD3OJeE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bdf207fb-fccc-4641-8245-6c612318dea5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 15010/15010 [00:00<00:00, 169846.99it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def find_relevant_documents_tfidf_raw_count(isPlain, docs_preprocessed, processed_query):\n",
        "  max_list_size = len(get_relevant_docs_list_for_base_hashtag(u_base_hashtag, u_time, docs_preprocessed))\n",
        "  \n",
        "  query_frequency = dict()\n",
        "  for query_token in processed_query:\n",
        "      if query_token in query_frequency:\n",
        "          query_frequency[query_token] += 1\n",
        "      else:\n",
        "        query_frequency[query_token] = 1\n",
        "\n",
        "  query_frequency_vector = np.zeros((len(vocabulary),))\n",
        "\n",
        "  for token in query_frequency.keys():\n",
        "    if token in vocabulary:\n",
        "      query_frequency_vector[word2id[token]] = query_frequency[token]\n",
        "\n",
        "  max_freq_token = query_frequency[max(query_frequency, key=query_frequency.get)]\n",
        "  total_tokens = len(processed_query)\n",
        "\n",
        "  f = query_frequency_vector\n",
        "  query_raw_count = Raw_count_tf_idf_scheme(np.copy(query_frequency_vector))\n",
        "\n",
        "  for i in tqdm(range(len(vocabulary))):\n",
        "    IDF_factor = IDF[id2word[i]]\n",
        "    query_raw_count[i] *= IDF_factor\n",
        "\n",
        "  if isPlain:\n",
        "    query_raw_count = np.where(query_raw_count >= np.sort(query_raw_count)[-10], query_raw_count, 0)\n",
        "  \n",
        "  Raw_count_tf_idf_score = np.dot(Raw_count_tf_idf, query_raw_count)\n",
        "  Raw_count_tf_idf_score = {docs_preprocessed[i]['Name']:Raw_count_tf_idf_score[i] for i in range(len(Raw_count_tf_idf_score))}\n",
        "  Raw_Count_relevant_docs = list(sorted(Raw_count_tf_idf_score.items(), key=operator.itemgetter(1),reverse=True))[:max_list_size]\n",
        "\n",
        "  for i in range(len(Raw_Count_relevant_docs)):\n",
        "    for j in range(len(docs_preprocessed)):\n",
        "      if(Raw_Count_relevant_docs[i][0] == docs_preprocessed[j]['Name']):\n",
        "        Raw_Count_relevant_docs[i] = (Raw_Count_relevant_docs[i][0], Raw_Count_relevant_docs[i][1], docs_preprocessed[j]['Data']['Date'], docs_preprocessed[j]['Data']['Location'].lower(), docs_preprocessed[j]['Data']['Link'])\n",
        "\n",
        "  # prioritize location\n",
        "  location_relevant = []\n",
        "  location_irrelevant = []\n",
        "  for x in Raw_Count_relevant_docs:\n",
        "    if u_location.lower() in x[3]:\n",
        "      location_relevant.append(x)\n",
        "    else:\n",
        "      location_irrelevant.append(x) \n",
        "  Raw_Count_relevant_docs = location_relevant + location_irrelevant\n",
        "  return Raw_Count_relevant_docs"
      ],
      "metadata": {
        "id": "6qFVHUCNRfoZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Plain Model (TF-IDF Raw Count)"
      ],
      "metadata": {
        "id": "o_8Ca8euL2Kx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "relevant_docs_tfidf_raw_count_plain = find_relevant_documents_tfidf_raw_count(True, docs_preprocessed, tweet_query)\n",
        "\n",
        "for rank, doc in enumerate(relevant_docs_tfidf_raw_count_plain):\n",
        "  print('Rank: {} Relevant Document: {}'.format(rank+1,doc))\n",
        "\n",
        "print()\n",
        "\n",
        "mean_average_precision_hashtag_tfidf_raw_count_plain = mean_average_precision(20, u_base_hashtag, u_time, relevant_docs_tfidf_raw_count_plain, docs_preprocessed)\n",
        "print('Mean Average Precision Plain Model (TF-IDF Raw Count) : {}'.format(mean_average_precision_hashtag_tfidf_raw_count_plain))\n",
        "\n",
        "mean_average_recall_hashtag_tfidf_raw_count_plain = mean_average_recall(20, u_base_hashtag, u_time, relevant_docs_tfidf_raw_count_plain, docs_preprocessed)\n",
        "print('Mean Average Recall Plain Model (TF-IDF Raw Count) : {}'.format(mean_average_recall_hashtag_tfidf_raw_count_plain))\n",
        "\n",
        "nDCG_hashtag_tfidf_raw_count_plain = nDCG(u_base_hashtag, u_time, relevant_docs_tfidf_raw_count_plain, docs_preprocessed)\n",
        "print('nDCG Plain Model (TF-IDF Raw Count) : {}'.format(nDCG_hashtag_tfidf_raw_count_plain))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O2g0_vWoTQ_Q",
        "outputId": "dd1e3506-d8d2-45aa-e56f-d2fd380180fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 15010/15010 [00:00<00:00, 861815.76it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rank: 1 Relevant Document: ('hijab_332.json', 10502.037447174953, '2022-02-20', 'new delhi, india', 'https://thewire.in/communalism/karnataka-muslim-women-students-face-case-suspension-for-wearing-hijab-protesting')\n",
            "Rank: 2 Relevant Document: ('hijab_338.json', 9633.05112020975, '2022-02-20', 'new delhi, india', 'https://news.abplive.com/karnataka/hijab-row-conspiracy-of-bjp-against-muslim-girls-former-k-taka-cm-siddaramaiah-1514360')\n",
            "Rank: 3 Relevant Document: ('hijab_285.json', 8689.296422473528, '2022-02-19', 'new delhi, india', 'https://news.abplive.com/karnataka/karnataka-hijab-row-govt-order-dated-feb-5-does-not-ban-hijab-ag-tells-high-court-1514213')\n",
            "Rank: 4 Relevant Document: ('hijab_340.json', 8526.54260365689, '2022-02-20', 'hyderabad, india', 'https://www.siasat.com/hijab-row-bangladesh-stands-in-solidarity-with-muslim-women-2279139/')\n",
            "Rank: 5 Relevant Document: ('hijab_333.json', 7427.372275805748, '2022-02-20', 'kerala, india', 'https://www.theweek.in/news/entertainment/2022/02/20/hijab-not-a-choice-but-an-obligation-in-islam-says-zaira-wasim.html')\n",
            "Rank: 6 Relevant Document: ('hijab_283.json', 5110.3216359168855, '2022-02-19', 'new delhi, india', 'https://www.newindianexpress.com/nation/2022/feb/19/education-more-important-than-hijab-rss-linked-muslim-rashtriya-manch-to-community-2421449.html')\n",
            "Rank: 7 Relevant Document: ('hijab_337.json', 756.0094952390868, '2022-02-20', 'new delhi, india', 'https://www.hindustantimes.com/opinion/hijab-row-restriction-on-clothing-is-political-101645367225790.html')\n",
            "Rank: 8 Relevant Document: ('hijab_334.json', 16324.86168340986, '2022-02-20', 'massachusetts, united states of america', 'https://scroll.in/article/1017784/why-urbane-educated-muslim-women-are-wearing-the-hijab-in-india')\n",
            "Rank: 9 Relevant Document: ('hijab_339.json', 16261.665097368812, '2022-02-20', 'aluva, kerala', 'https://thefederal.com/states/south/tamil-nadu/discrimination-violence-drove-40-dalits-in-tn-village-to-embrace-islam/')\n",
            "Rank: 10 Relevant Document: ('hijab_336.json', 10224.356963036133, '2022-02-20', 'mumbai, maharashtra', 'https://www.indiatoday.in/movies/celebrities/story/hijab-isn-t-a-choice-but-an-obligation-in-islam-says-zaira-wasim-1915351-2022-02-20')\n",
            "Rank: 11 Relevant Document: ('hijab_286.json', 10006.99621845407, '2022-02-19', 'bangalore, karnataka', 'https://thefederal.com/states/south/karnataka/uneasy-bjp-central-command-wants-to-wind-down-hijab-row-in-karnataka/')\n",
            "Rank: 12 Relevant Document: ('hijab_282.json', 8854.630437849512, '2022-02-19', 'bangalore, karnataka', 'https://www.hindustantimes.com/entertainment/bollywood/zaira-wasim-reacts-to-hijab-row-i-as-a-woman-who-wears-hijab-resent-this-entire-system-101645288361058.html')\n",
            "Rank: 13 Relevant Document: ('hijab_281.json', 8423.78843010069, '2022-02-19', 'bangalore, karnataka', 'https://www.thehindu.com/news/national/karnataka/hijab-row-polarises-classrooms-breaks-friendships/article65063188.ece')\n",
            "Rank: 14 Relevant Document: ('hijab_288.json', 7812.905748652735, '2022-02-19', 'mysore', 'https://thekashmirwalla.com/mysuru-college-cancels-uniform-order-allows-students-to-wear-hijab/')\n",
            "Rank: 15 Relevant Document: ('hijab_287.json', 7342.172472584456, '2022-02-19', 'shivamogga', 'https://www.outlookindia.com/national/karnataka-hijab-row-58-students-suspended-in-shivamogga-for-wearing-headscarves-holding-protest-news-183162')\n",
            "Rank: 16 Relevant Document: ('hijab_290.json', 6831.636476731654, '2022-02-19', 'bangalore, karnataka', 'https://english.varthabharati.in/karnataka/hijab-row-feb-5-government-order-doesnt-ban-hijab-insists-ag-in-karnataka-hc')\n",
            "Rank: 17 Relevant Document: ('hijab_289.json', 6165.866515518828, '2022-02-19', 'karnataka', 'https://www.freepressjournal.in/india/karnataka-hijab-row-seen-to-be-soft-cm-basavaraj-bommaiturns-heat-on-hijab-protesters')\n",
            "Rank: 18 Relevant Document: ('hijab_331.json', 6148.07569698089, '2022-02-20', 'chennai, tamil nadu', 'https://www.thehindu.com/data/data-hijab-row-why-the-ban-is-a-double-blow-for-muslim-girl-students/article65066546.ece')\n",
            "Rank: 19 Relevant Document: ('hijab_284.json', 5196.315960969416, '2022-02-19', 'bangalore, karnataka', 'https://www.sundayguardianlive.com/news/hijab-controversy-manufactured-create-anarchy')\n",
            "Rank: 20 Relevant Document: ('hijab_335.json', 3683.94009505245, '2022-02-20', 'bangalore, karnataka', 'https://indianexpress.com/article/cities/bangalore/karnataka-bengaluru-live-updates-hijab-row-protests-court-hearing-covid-7765244/')\n",
            "\n",
            "20\n",
            "Mean Average Precision Plain Model (TF-IDF Raw Count) : 1.0\n",
            "20\n",
            "Mean Average Recall Plain Model (TF-IDF Raw Count) : 0.5249999999999999\n",
            "nDCG Plain Model (TF-IDF Raw Count) : 0.9008203286974528\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model with Keyword Extractor (TF-IDF Raw Count)"
      ],
      "metadata": {
        "id": "kugqQGpIVaoK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "relevant_docs_tfidf_raw_count_keyword_extractor = find_relevant_documents_tfidf_raw_count(False, docs_preprocessed, tweet_query_keyword_extractor[:20])\n",
        "\n",
        "for rank, doc in enumerate(relevant_docs_tfidf_raw_count_keyword_extractor):\n",
        "  print('Rank: {} Relevant Document: {}'.format(rank+1,doc))\n",
        "\n",
        "print()\n",
        "\n",
        "mean_average_precision_hashtag_tfidf_raw_count_keyword_extractor = mean_average_precision(20, u_base_hashtag, u_time, relevant_docs_tfidf_raw_count_keyword_extractor, docs_preprocessed)\n",
        "print('Mean Average Precision Keyword Extractor Model (TF-IDF Raw Count) : {}'.format(mean_average_precision_hashtag_tfidf_raw_count_keyword_extractor))\n",
        "\n",
        "mean_average_recall_hashtag_tfidf_raw_count_keyword_extractor = mean_average_recall(20, u_base_hashtag, u_time, relevant_docs_tfidf_raw_count_keyword_extractor, docs_preprocessed)\n",
        "print('Mean Average Recall Keyword Extractor Model (TF-IDF Raw Count) : {}'.format(mean_average_recall_hashtag_tfidf_raw_count_keyword_extractor))\n",
        "\n",
        "nDCG_hashtag_tfidf_raw_count_keyword_extractor = nDCG(u_base_hashtag, u_time, relevant_docs_tfidf_raw_count_keyword_extractor, docs_preprocessed)\n",
        "print('nDCG Keyword Extractor Model (TF-IDF Raw Count) : {}'.format(nDCG_hashtag_tfidf_raw_count_keyword_extractor))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QrsAamJfVZHQ",
        "outputId": "e9240904-e50a-44d8-cc8c-5cc46ca46826"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 15010/15010 [00:00<00:00, 503205.18it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rank: 1 Relevant Document: ('hijab_285.json', 57.92549972157805, '2022-02-19', 'new delhi, india', 'https://news.abplive.com/karnataka/karnataka-hijab-row-govt-order-dated-feb-5-does-not-ban-hijab-ag-tells-high-court-1514213')\n",
            "Rank: 2 Relevant Document: ('hijab_338.json', 57.82940448776149, '2022-02-20', 'new delhi, india', 'https://news.abplive.com/karnataka/hijab-row-conspiracy-of-bjp-against-muslim-girls-former-k-taka-cm-siddaramaiah-1514360')\n",
            "Rank: 3 Relevant Document: ('hijab_332.json', 51.12343180517411, '2022-02-20', 'new delhi, india', 'https://thewire.in/communalism/karnataka-muslim-women-students-face-case-suspension-for-wearing-hijab-protesting')\n",
            "Rank: 4 Relevant Document: ('hijab_340.json', 39.09596578997147, '2022-02-20', 'hyderabad, india', 'https://www.siasat.com/hijab-row-bangladesh-stands-in-solidarity-with-muslim-women-2279139/')\n",
            "Rank: 5 Relevant Document: ('hijab_333.json', 36.38204461806112, '2022-02-20', 'kerala, india', 'https://www.theweek.in/news/entertainment/2022/02/20/hijab-not-a-choice-but-an-obligation-in-islam-says-zaira-wasim.html')\n",
            "Rank: 6 Relevant Document: ('hijab_283.json', 34.954858306602404, '2022-02-19', 'new delhi, india', 'https://www.newindianexpress.com/nation/2022/feb/19/education-more-important-than-hijab-rss-linked-muslim-rashtriya-manch-to-community-2421449.html')\n",
            "Rank: 7 Relevant Document: ('InternalAssessmentForAll2022_1707.json', 12.212576345041986, '2022-02-19', 'india', 'https://indianexpress.com/article/cities/delhi/haryana-board-exams-class-8-students-parents-protest-7782756/')\n",
            "Rank: 8 Relevant Document: ('hijab_339.json', 104.81521122653827, '2022-02-20', 'aluva, kerala', 'https://thefederal.com/states/south/tamil-nadu/discrimination-violence-drove-40-dalits-in-tn-village-to-embrace-islam/')\n",
            "Rank: 9 Relevant Document: ('hijab_334.json', 102.92071731656698, '2022-02-20', 'massachusetts, united states of america', 'https://scroll.in/article/1017784/why-urbane-educated-muslim-women-are-wearing-the-hijab-in-india')\n",
            "Rank: 10 Relevant Document: ('hijab_286.json', 58.86957561414852, '2022-02-19', 'bangalore, karnataka', 'https://thefederal.com/states/south/karnataka/uneasy-bjp-central-command-wants-to-wind-down-hijab-row-in-karnataka/')\n",
            "Rank: 11 Relevant Document: ('hijab_331.json', 54.9248599881621, '2022-02-20', 'chennai, tamil nadu', 'https://www.thehindu.com/data/data-hijab-row-why-the-ban-is-a-double-blow-for-muslim-girl-students/article65066546.ece')\n",
            "Rank: 12 Relevant Document: ('hijab_288.json', 51.35346929491691, '2022-02-19', 'mysore', 'https://thekashmirwalla.com/mysuru-college-cancels-uniform-order-allows-students-to-wear-hijab/')\n",
            "Rank: 13 Relevant Document: ('hijab_336.json', 47.63602221094082, '2022-02-20', 'mumbai, maharashtra', 'https://www.indiatoday.in/movies/celebrities/story/hijab-isn-t-a-choice-but-an-obligation-in-islam-says-zaira-wasim-1915351-2022-02-20')\n",
            "Rank: 14 Relevant Document: ('hijab_281.json', 44.74798507892372, '2022-02-19', 'bangalore, karnataka', 'https://www.thehindu.com/news/national/karnataka/hijab-row-polarises-classrooms-breaks-friendships/article65063188.ece')\n",
            "Rank: 15 Relevant Document: ('hijab_282.json', 42.45818910410061, '2022-02-19', 'bangalore, karnataka', 'https://www.hindustantimes.com/entertainment/bollywood/zaira-wasim-reacts-to-hijab-row-i-as-a-woman-who-wears-hijab-resent-this-entire-system-101645288361058.html')\n",
            "Rank: 16 Relevant Document: ('hijab_290.json', 39.05847558191219, '2022-02-19', 'bangalore, karnataka', 'https://english.varthabharati.in/karnataka/hijab-row-feb-5-government-order-doesnt-ban-hijab-insists-ag-in-karnataka-hc')\n",
            "Rank: 17 Relevant Document: ('hijab_287.json', 36.482827346198775, '2022-02-19', 'shivamogga', 'https://www.outlookindia.com/national/karnataka-hijab-row-58-students-suspended-in-shivamogga-for-wearing-headscarves-holding-protest-news-183162')\n",
            "Rank: 18 Relevant Document: ('hijab_284.json', 33.05094299695584, '2022-02-19', 'bangalore, karnataka', 'https://www.sundayguardianlive.com/news/hijab-controversy-manufactured-create-anarchy')\n",
            "Rank: 19 Relevant Document: ('hijab_335.json', 19.281788180954727, '2022-02-20', 'bangalore, karnataka', 'https://indianexpress.com/article/cities/bangalore/karnataka-bengaluru-live-updates-hijab-row-protests-court-hearing-covid-7765244/')\n",
            "Rank: 20 Relevant Document: ('hijab_289.json', 18.92316861024775, '2022-02-19', 'karnataka', 'https://www.freepressjournal.in/india/karnataka-hijab-row-seen-to-be-soft-cm-basavaraj-bommaiturns-heat-on-hijab-protesters')\n",
            "\n",
            "20\n",
            "Mean Average Precision Keyword Extractor Model (TF-IDF Raw Count) : 0.9426130171428155\n",
            "20\n",
            "Mean Average Recall Keyword Extractor Model (TF-IDF Raw Count) : 0.48999999999999994\n",
            "nDCG Keyword Extractor Model (TF-IDF Raw Count) : 0.9000258789002469\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model with YAKE (TF-IDF Raw Count)"
      ],
      "metadata": {
        "id": "xj1XeLTyfE59"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "relevant_docs_tfidf_raw_count_yake = find_relevant_documents_tfidf_raw_count(False, docs_preprocessed, tweet_keywords_yake)\n",
        "\n",
        "for rank, doc in enumerate(relevant_docs_tfidf_raw_count_yake):\n",
        "  print('Rank: {} Relevant Document: {}'.format(rank+1,doc))\n",
        "\n",
        "print()\n",
        "\n",
        "mean_average_precision_hashtag_tfidf_raw_count_yake = mean_average_precision(20, u_base_hashtag, u_time, relevant_docs_tfidf_raw_count_yake, docs_preprocessed)\n",
        "print('Mean Average Precision Keyword YAKE Model (TF-IDF Raw Count) : {}'.format(mean_average_precision_hashtag_tfidf_raw_count_yake))\n",
        "\n",
        "mean_average_recall_hashtag_tfidf_raw_count_yake = mean_average_recall(20, u_base_hashtag, u_time, relevant_docs_tfidf_raw_count_yake, docs_preprocessed)\n",
        "print('Mean Average Recall Keyword YAKE Model (TF-IDF Raw Count) : {}'.format(mean_average_recall_hashtag_tfidf_raw_count_yake))\n",
        "\n",
        "nDCG_hashtag_tfidf_raw_count_yake = nDCG(u_base_hashtag, u_time, relevant_docs_tfidf_raw_count_yake, docs_preprocessed)\n",
        "print('nDCG Keyword YAKE Model (TF-IDF Raw Count) : {}'.format(nDCG_hashtag_tfidf_raw_count_yake))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WXLr02MqfAOf",
        "outputId": "1f6f4e5e-d537-4a83-f44a-df6c64597da5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 15010/15010 [00:00<00:00, 602587.20it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rank: 1 Relevant Document: ('hijab_338.json', 55.79397509692116, '2022-02-20', 'new delhi, india', 'https://news.abplive.com/karnataka/hijab-row-conspiracy-of-bjp-against-muslim-girls-former-k-taka-cm-siddaramaiah-1514360')\n",
            "Rank: 2 Relevant Document: ('hijab_332.json', 50.10571710975394, '2022-02-20', 'new delhi, india', 'https://thewire.in/communalism/karnataka-muslim-women-students-face-case-suspension-for-wearing-hijab-protesting')\n",
            "Rank: 3 Relevant Document: ('hijab_285.json', 49.64260777707677, '2022-02-19', 'new delhi, india', 'https://news.abplive.com/karnataka/karnataka-hijab-row-govt-order-dated-feb-5-does-not-ban-hijab-ag-tells-high-court-1514213')\n",
            "Rank: 4 Relevant Document: ('hijab_340.json', 39.17940906101972, '2022-02-20', 'hyderabad, india', 'https://www.siasat.com/hijab-row-bangladesh-stands-in-solidarity-with-muslim-women-2279139/')\n",
            "Rank: 5 Relevant Document: ('hijab_333.json', 34.978033563787584, '2022-02-20', 'kerala, india', 'https://www.theweek.in/news/entertainment/2022/02/20/hijab-not-a-choice-but-an-obligation-in-islam-says-zaira-wasim.html')\n",
            "Rank: 6 Relevant Document: ('hijab_283.json', 33.937143611182236, '2022-02-19', 'new delhi, india', 'https://www.newindianexpress.com/nation/2022/feb/19/education-more-important-than-hijab-rss-linked-muslim-rashtriya-manch-to-community-2421449.html')\n",
            "Rank: 7 Relevant Document: ('narendramodi_253.json', 9.1184590208495, '2022-02-19', 'new delhi, india', 'https://www.newindianexpress.com/nation/2022/feb/19/india-is-your-home-pm-narendra-modi-meets-afghan-sikh-hindu-delegation-2421442.html')\n",
            "Rank: 8 Relevant Document: ('hijab_334.json', 101.32689798350573, '2022-02-20', 'massachusetts, united states of america', 'https://scroll.in/article/1017784/why-urbane-educated-muslim-women-are-wearing-the-hijab-in-india')\n",
            "Rank: 9 Relevant Document: ('hijab_339.json', 97.78685948062152, '2022-02-20', 'aluva, kerala', 'https://thefederal.com/states/south/tamil-nadu/discrimination-violence-drove-40-dalits-in-tn-village-to-embrace-islam/')\n",
            "Rank: 10 Relevant Document: ('hijab_286.json', 53.16179165145706, '2022-02-19', 'bangalore, karnataka', 'https://thefederal.com/states/south/karnataka/uneasy-bjp-central-command-wants-to-wind-down-hijab-row-in-karnataka/')\n",
            "Rank: 11 Relevant Document: ('hijab_288.json', 45.10600674125597, '2022-02-19', 'mysore', 'https://thekashmirwalla.com/mysuru-college-cancels-uniform-order-allows-students-to-wear-hijab/')\n",
            "Rank: 12 Relevant Document: ('hijab_336.json', 43.81028540697358, '2022-02-20', 'mumbai, maharashtra', 'https://www.indiatoday.in/movies/celebrities/story/hijab-isn-t-a-choice-but-an-obligation-in-islam-says-zaira-wasim-1915351-2022-02-20')\n",
            "Rank: 13 Relevant Document: ('hijab_281.json', 42.09334520249275, '2022-02-19', 'bangalore, karnataka', 'https://www.thehindu.com/news/national/karnataka/hijab-row-polarises-classrooms-breaks-friendships/article65063188.ece')\n",
            "Rank: 14 Relevant Document: ('hijab_331.json', 41.93969092541336, '2022-02-20', 'chennai, tamil nadu', 'https://www.thehindu.com/data/data-hijab-row-why-the-ban-is-a-double-blow-for-muslim-girl-students/article65066546.ece')\n",
            "Rank: 15 Relevant Document: ('hijab_282.json', 38.632452300133366, '2022-02-19', 'bangalore, karnataka', 'https://www.hindustantimes.com/entertainment/bollywood/zaira-wasim-reacts-to-hijab-row-i-as-a-woman-who-wears-hijab-resent-this-entire-system-101645288361058.html')\n",
            "Rank: 16 Relevant Document: ('hijab_287.json', 35.46511265077861, '2022-02-19', 'shivamogga', 'https://www.outlookindia.com/national/karnataka-hijab-row-58-students-suspended-in-shivamogga-for-wearing-headscarves-holding-protest-news-183162')\n",
            "Rank: 17 Relevant Document: ('hijab_290.json', 31.793298332831082, '2022-02-19', 'bangalore, karnataka', 'https://english.varthabharati.in/karnataka/hijab-row-feb-5-government-order-doesnt-ban-hijab-insists-ag-in-karnataka-hc')\n",
            "Rank: 18 Relevant Document: ('hijab_284.json', 30.94818956247051, '2022-02-19', 'bangalore, karnataka', 'https://www.sundayguardianlive.com/news/hijab-controversy-manufactured-create-anarchy')\n",
            "Rank: 19 Relevant Document: ('hijab_335.json', 19.281788180954727, '2022-02-20', 'bangalore, karnataka', 'https://indianexpress.com/article/cities/bangalore/karnataka-bengaluru-live-updates-hijab-row-protests-court-hearing-covid-7765244/')\n",
            "Rank: 20 Relevant Document: ('hijab_289.json', 17.905453914827586, '2022-02-19', 'karnataka', 'https://www.freepressjournal.in/india/karnataka-hijab-row-seen-to-be-soft-cm-basavaraj-bommaiturns-heat-on-hijab-protesters')\n",
            "\n",
            "20\n",
            "Mean Average Precision Keyword YAKE Model (TF-IDF Raw Count) : 0.9426130171428155\n",
            "20\n",
            "Mean Average Recall Keyword YAKE Model (TF-IDF Raw Count) : 0.48999999999999994\n",
            "nDCG Keyword YAKE Model (TF-IDF Raw Count) : 0.8710340781135755\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model with RAKE (TF-IDF Raw Count)"
      ],
      "metadata": {
        "id": "6Vw7UZm_fkxN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "relevant_docs_tfidf_raw_count_rake = find_relevant_documents_tfidf_raw_count(False, docs_preprocessed, tweet_keywords_rake)\n",
        "\n",
        "for rank, doc in enumerate(relevant_docs_tfidf_raw_count_rake):\n",
        "  print('Rank: {} Relevant Document: {}'.format(rank+1,doc))\n",
        "\n",
        "print()\n",
        "\n",
        "mean_average_precision_hashtag_tfidf_raw_count_rake = mean_average_precision(20, u_base_hashtag, u_time, relevant_docs_tfidf_raw_count_rake, docs_preprocessed)\n",
        "print('Mean Average Precision Keyword RAKE Model (TF-IDF Raw Count) : {}'.format(mean_average_precision_hashtag_tfidf_raw_count_rake))\n",
        "\n",
        "mean_average_recall_hashtag_tfidf_raw_count_rake = mean_average_recall(20, u_base_hashtag, u_time, relevant_docs_tfidf_raw_count_rake, docs_preprocessed)\n",
        "print('Mean Average Recall Keyword RAKE Model (TF-IDF Raw Count) : {}'.format(mean_average_recall_hashtag_tfidf_raw_count_rake))\n",
        "\n",
        "nDCG_hashtag_tfidf_raw_count_rake = nDCG(u_base_hashtag, u_time, relevant_docs_tfidf_raw_count_rake, docs_preprocessed)\n",
        "print('nDCG Keyword RAKE Model (TF-IDF Raw Count) : {}'.format(nDCG_hashtag_tfidf_raw_count_rake))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T8YtHJHrfo0t",
        "outputId": "42f8e20c-b4bd-45a2-cfd5-ab959a393473"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 15010/15010 [00:00<00:00, 743762.29it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rank: 1 Relevant Document: ('hijab_285.json', 42.234386309757504, '2022-02-19', 'new delhi, india', 'https://news.abplive.com/karnataka/karnataka-hijab-row-govt-order-dated-feb-5-does-not-ban-hijab-ag-tells-high-court-1514213')\n",
            "Rank: 2 Relevant Document: ('kejriwalvsall_1655.json', 38.33705991443404, '2022-02-18', 'india', 'https://www.ndtv.com/opinion/it-could-be-kejriwal-vs-adityanath-in-2029-by-sagarika-ghose-2814030')\n",
            "Rank: 3 Relevant Document: ('kejriwalvsall_1661.json', 23.074822735114793, '2022-02-18', 'india', 'https://www.hindustantimes.com/cities/delhi-news/is-kejriwal-for-municipal-reforms-or-not-bjp-asks-as-unification-of-mcds-imminent-101646853880629.html')\n",
            "Rank: 4 Relevant Document: ('hijab_338.json', 22.796542562460992, '2022-02-20', 'new delhi, india', 'https://news.abplive.com/karnataka/hijab-row-conspiracy-of-bjp-against-muslim-girls-former-k-taka-cm-siddaramaiah-1514360')\n",
            "Rank: 5 Relevant Document: ('hijab_283.json', 21.493872193500465, '2022-02-19', 'new delhi, india', 'https://www.newindianexpress.com/nation/2022/feb/19/education-more-important-than-hijab-rss-linked-muslim-rashtriya-manch-to-community-2421449.html')\n",
            "Rank: 6 Relevant Document: ('kejriwalvsall_1663.json', 19.05005879665086, '2022-02-18', 'india', 'https://theprint.in/politics/not-modi-mamata-or-kcr-kejriwal-bigger-threat-to-congress-now-3-takeaways-from-assembly-polls/867014/')\n",
            "Rank: 7 Relevant Document: ('hijab_286.json', 62.172137212369734, '2022-02-19', 'bangalore, karnataka', 'https://thefederal.com/states/south/karnataka/uneasy-bjp-central-command-wants-to-wind-down-hijab-row-in-karnataka/')\n",
            "Rank: 8 Relevant Document: ('UkraineRussiaCrisis_319.json', 44.04296799393131, '2022-02-20', 'new york, united states of america', 'https://www.bloomberg.com/news/articles/2022-04-16/ukraine-update-un-calls-for-unimpeded-access-to-besieged-cities')\n",
            "Rank: 9 Relevant Document: ('hijab_339.json', 40.81694815506048, '2022-02-20', 'aluva, kerala', 'https://thefederal.com/states/south/tamil-nadu/discrimination-violence-drove-40-dalits-in-tn-village-to-embrace-islam/')\n",
            "Rank: 10 Relevant Document: ('UltimateDraft_1687.json', 35.380849434113316, '2022-02-19', 'united states of america', 'https://basketballforever.com/2022/02/20/nba-ultimate-draft-highlights-dominique-wilkins-shocks-panel-with-first-pick')\n",
            "Rank: 11 Relevant Document: ('hijab_281.json', 34.5160351836277, '2022-02-19', 'bangalore, karnataka', 'https://www.thehindu.com/news/national/karnataka/hijab-row-polarises-classrooms-breaks-friendships/article65063188.ece')\n",
            "Rank: 12 Relevant Document: ('zeynep_1627.json', 33.31938463526135, '2022-02-18', 'united states of america', 'https://www.nytimes.com/2022/03/11/opinion/covid-health-pandemic.html')\n",
            "Rank: 13 Relevant Document: ('narendramodi_305.json', 30.669487483379342, '2022-02-20', 'hardoi, uttar pradesh', 'https://www.indiatoday.in/elections/uttar-pradesh-assembly-polls-2022/story/pm-modi-addresses-rally-up-hardoi-1915489-2022-02-20')\n",
            "Rank: 14 Relevant Document: ('UkraineRussiaCrisis_314.json', 28.6434656784951, '2022-02-20', 'washington, united states of america', 'https://edition.cnn.com/europe/live-news/ukraine-russia-news-02-20-22-intl/index.html')\n",
            "Rank: 15 Relevant Document: ('UkraineRussiaCrisis_315.json', 27.521015546625133, '2022-02-20', 'london, united kingdom', 'https://fivebooks.com/best-books/russia-ukraine-serhii-plokhy/')\n",
            "Rank: 16 Relevant Document: ('hijab_290.json', 25.322492679289667, '2022-02-19', 'bangalore, karnataka', 'https://english.varthabharati.in/karnataka/hijab-row-feb-5-government-order-doesnt-ban-hijab-insists-ag-in-karnataka-hc')\n",
            "Rank: 17 Relevant Document: ('zeynep_1746.json', 24.679478354442185, '2022-02-20', 'germany', 'https://www.artemis.bm/news/billion-plus-storm-losses-expected-from-dudley-ylenia-eunice-zeynep/')\n",
            "Rank: 18 Relevant Document: ('OperationDudula_999.json', 24.45257466744852, '2022-02-20', 'johannesburg, south africa', 'https://www.dailymaverick.co.za/article/2022-02-21-xenophobia-accused-operation-dudula-returns-to-hillbrow/')\n",
            "Rank: 19 Relevant Document: ('zeynep_1630.json', 23.77224263589374, '2022-02-18', 'united states of america', 'https://blogs.lse.ac.uk/usappblog/2022/02/27/book-review-politics-and-expertise-how-to-use-science-in-a-democratic-society-by-zeynep-pamuk/')\n",
            "Rank: 20 Relevant Document: ('EFFvsAfriforum_1652.json', 21.060003620169176, '2022-02-18', 'south africa', 'https://tdpelmedia.com/african-literature-scholar-professor-elizabeth-gunner-to-testify-in-malema-hate-speech-case')\n",
            "\n",
            "20\n",
            "Mean Average Precision Keyword RAKE Model (TF-IDF Raw Count) : 0.49262282716423583\n",
            "20\n",
            "Mean Average Recall Keyword RAKE Model (TF-IDF Raw Count) : 0.23499999999999996\n",
            "nDCG Keyword RAKE Model (TF-IDF Raw Count) : 0.45112928314370543\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model with TextRank (TF-IDF Raw Count)"
      ],
      "metadata": {
        "id": "htbzsPDnf5IU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "relevant_docs_tfidf_raw_count_text_rank = find_relevant_documents_tfidf_raw_count(False, docs_preprocessed, tweet_keywords_text_rank)\n",
        "\n",
        "for rank, doc in enumerate(relevant_docs_tfidf_raw_count_text_rank):\n",
        "  print('Rank: {} Relevant Document: {}'.format(rank+1,doc))\n",
        "\n",
        "print()\n",
        "\n",
        "mean_average_precision_hashtag_tfidf_raw_count_text_rank = mean_average_precision(20, u_base_hashtag, u_time, relevant_docs_tfidf_raw_count_text_rank, docs_preprocessed)\n",
        "print('Mean Average Precision Keyword TextRank Model (TF-IDF Raw Count) : {}'.format(mean_average_precision_hashtag_tfidf_raw_count_text_rank))\n",
        "\n",
        "mean_average_recall_hashtag_tfidf_raw_count_text_rank = mean_average_recall(20, u_base_hashtag, u_time, relevant_docs_tfidf_raw_count_text_rank, docs_preprocessed)\n",
        "print('Mean Average Recall Keyword TextRank Model (TF-IDF Raw Count) : {}'.format(mean_average_recall_hashtag_tfidf_raw_count_text_rank))\n",
        "\n",
        "nDCG_hashtag_tfidf_raw_count_text_rank = nDCG(u_base_hashtag, u_time, relevant_docs_tfidf_raw_count_text_rank, docs_preprocessed)\n",
        "print('nDCG Keyword TextRank Model (TF-IDF Raw Count) : {}'.format(nDCG_hashtag_tfidf_raw_count_text_rank))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o4IPmTFFf0qL",
        "outputId": "aa7d6825-e99e-448a-cb31-2ac2365b8365"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 15010/15010 [00:00<00:00, 501413.71it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rank: 1 Relevant Document: ('hijab_332.json', 58.45044256443269, '2022-02-20', 'new delhi, india', 'https://thewire.in/communalism/karnataka-muslim-women-students-face-case-suspension-for-wearing-hijab-protesting')\n",
            "Rank: 2 Relevant Document: ('hijab_338.json', 55.87603281962558, '2022-02-20', 'new delhi, india', 'https://news.abplive.com/karnataka/hijab-row-conspiracy-of-bjp-against-muslim-girls-former-k-taka-cm-siddaramaiah-1514360')\n",
            "Rank: 3 Relevant Document: ('hijab_285.json', 44.85006042106423, '2022-02-19', 'new delhi, india', 'https://news.abplive.com/karnataka/karnataka-hijab-row-govt-order-dated-feb-5-does-not-ban-hijab-ag-tells-high-court-1514213')\n",
            "Rank: 4 Relevant Document: ('hijab_283.json', 36.82647786650092, '2022-02-19', 'new delhi, india', 'https://www.newindianexpress.com/nation/2022/feb/19/education-more-important-than-hijab-rss-linked-muslim-rashtriya-manch-to-community-2421449.html')\n",
            "Rank: 5 Relevant Document: ('hijab_340.json', 35.82443716628397, '2022-02-20', 'hyderabad, india', 'https://www.siasat.com/hijab-row-bangladesh-stands-in-solidarity-with-muslim-women-2279139/')\n",
            "Rank: 6 Relevant Document: ('hijab_333.json', 19.589096735113504, '2022-02-20', 'kerala, india', 'https://www.theweek.in/news/entertainment/2022/02/20/hijab-not-a-choice-but-an-obligation-in-islam-says-zaira-wasim.html')\n",
            "Rank: 7 Relevant Document: ('hijab_334.json', 108.08722687509925, '2022-02-20', 'massachusetts, united states of america', 'https://scroll.in/article/1017784/why-urbane-educated-muslim-women-are-wearing-the-hijab-in-india')\n",
            "Rank: 8 Relevant Document: ('hijab_339.json', 88.47181955968374, '2022-02-20', 'aluva, kerala', 'https://thefederal.com/states/south/tamil-nadu/discrimination-violence-drove-40-dalits-in-tn-village-to-embrace-islam/')\n",
            "Rank: 9 Relevant Document: ('hijab_284.json', 49.07898839151345, '2022-02-19', 'bangalore, karnataka', 'https://www.sundayguardianlive.com/news/hijab-controversy-manufactured-create-anarchy')\n",
            "Rank: 10 Relevant Document: ('hijab_331.json', 48.16431776224172, '2022-02-20', 'chennai, tamil nadu', 'https://www.thehindu.com/data/data-hijab-row-why-the-ban-is-a-double-blow-for-muslim-girl-students/article65066546.ece')\n",
            "Rank: 11 Relevant Document: ('hijab_286.json', 45.88178540367577, '2022-02-19', 'bangalore, karnataka', 'https://thefederal.com/states/south/karnataka/uneasy-bjp-central-command-wants-to-wind-down-hijab-row-in-karnataka/')\n",
            "Rank: 12 Relevant Document: ('hijab_281.json', 39.74188606863638, '2022-02-19', 'bangalore, karnataka', 'https://www.thehindu.com/news/national/karnataka/hijab-row-polarises-classrooms-breaks-friendships/article65063188.ece')\n",
            "Rank: 13 Relevant Document: ('hijab_287.json', 35.45957624133942, '2022-02-19', 'shivamogga', 'https://www.outlookindia.com/national/karnataka-hijab-row-58-students-suspended-in-shivamogga-for-wearing-headscarves-holding-protest-news-183162')\n",
            "Rank: 14 Relevant Document: ('hijab_288.json', 31.688847659161738, '2022-02-19', 'mysore', 'https://thekashmirwalla.com/mysuru-college-cancels-uniform-order-allows-students-to-wear-hijab/')\n",
            "Rank: 15 Relevant Document: ('hijab_282.json', 30.365246702469378, '2022-02-19', 'bangalore, karnataka', 'https://www.hindustantimes.com/entertainment/bollywood/zaira-wasim-reacts-to-hijab-row-i-as-a-woman-who-wears-hijab-resent-this-entire-system-101645288361058.html')\n",
            "Rank: 16 Relevant Document: ('hijab_290.json', 28.22855051377119, '2022-02-19', 'bangalore, karnataka', 'https://english.varthabharati.in/karnataka/hijab-row-feb-5-government-order-doesnt-ban-hijab-insists-ag-in-karnataka-hc')\n",
            "Rank: 17 Relevant Document: ('hijab_336.json', 26.11879564681801, '2022-02-20', 'mumbai, maharashtra', 'https://www.indiatoday.in/movies/celebrities/story/hijab-isn-t-a-choice-but-an-obligation-in-islam-says-zaira-wasim-1915351-2022-02-20')\n",
            "Rank: 18 Relevant Document: ('hijab_289.json', 19.83232487945895, '2022-02-19', 'karnataka', 'https://www.freepressjournal.in/india/karnataka-hijab-row-seen-to-be-soft-cm-basavaraj-bommaiturns-heat-on-hijab-protesters')\n",
            "Rank: 19 Relevant Document: ('narendramodi_310.json', 13.402324329893947, '2022-02-20', 'amritsar, punjab', 'https://www.aljazeera.com/news/2022/2/20/farmer-anger-will-test-modi-as-indias-grain-bowl-votes')\n",
            "Rank: 20 Relevant Document: ('TrudeauTyranny_1738.json', 12.822142586661943, '2022-02-20', 'united states of america', 'https://www.mediaite.com/tv/fcking-liars-canadian-convoy-protesters-invade-msnbc-live-report-yelling-freedom-at-top-of-lungs/')\n",
            "\n",
            "20\n",
            "Mean Average Precision Keyword TextRank Model (TF-IDF Raw Count) : 0.9923684210526315\n",
            "20\n",
            "Mean Average Recall Keyword TextRank Model (TF-IDF Raw Count) : 0.5175\n",
            "nDCG Keyword TextRank Model (TF-IDF Raw Count) : 0.8778468476072812\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model with KeyBERT (TF-IDF Binary)"
      ],
      "metadata": {
        "id": "06kAPEuQgIAM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# relevant_docs_tfidf_raw_count_keybert = find_relevant_documents_tfidf_raw_count(False, docs_preprocessed, tweet_keywords_keybert)\n",
        "\n",
        "# for rank, doc in enumerate(relevant_docs_tfidf_raw_count_keybert):\n",
        "#   print('Rank: {} Relevant Document: {}'.format(rank+1,doc))\n",
        "\n",
        "# print()\n",
        "\n",
        "# mean_average_precision_hashtag_tfidf_raw_count_keybert = mean_average_precision(20, u_base_hashtag, u_time, relevant_docs_tfidf_raw_count_keybert, docs_preprocessed)\n",
        "# print('Mean Average Precision Keyword KeyBERT Model (TF-IDF Raw Count) : {}'.format(mean_average_precision_hashtag_tfidf_raw_count_keybert))\n",
        "\n",
        "# mean_average_recall_hashtag_tfidf_raw_count_keybert = mean_average_recall(20, u_base_hashtag, u_time, relevant_docs_tfidf_raw_count_keybert, docs_preprocessed)\n",
        "# print('Mean Average Recall Keyword KeyBERT Model (TF-IDF Raw Count) : {}'.format(mean_average_recall_hashtag_tfidf_raw_count_keybert))\n",
        "\n",
        "# nDCG_hashtag_tfidf_raw_count_keybert = nDCG(u_base_hashtag, u_time, relevant_docs_tfidf_raw_count_keybert, docs_preprocessed)\n",
        "# print('nDCG Keyword KeyBERT Model (TF-IDF Raw Count) : {}'.format(nDCG_hashtag_tfidf_raw_count_keybert))"
      ],
      "metadata": {
        "id": "cq3e2Dn2gGKk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}