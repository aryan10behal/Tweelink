{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A-6HD8Pe-_Sp",
        "outputId": "7771adfa-4534-4cdf-b10e-07131de1fbdc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EjXXowTxCgx7",
        "outputId": "1785047c-f187-479b-e438-a32bb04a2cb4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/LIAAD/yake\n",
            "  Cloning https://github.com/LIAAD/yake to /tmp/pip-req-build-cbpn_xlw\n",
            "  Running command git clone -q https://github.com/LIAAD/yake /tmp/pip-req-build-cbpn_xlw\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.7/dist-packages (from yake==0.4.8) (0.8.9)\n",
            "Requirement already satisfied: click>=6.0 in /usr/local/lib/python3.7/dist-packages (from yake==0.4.8) (7.1.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from yake==0.4.8) (1.21.5)\n",
            "Requirement already satisfied: segtok in /usr/local/lib/python3.7/dist-packages (from yake==0.4.8) (1.5.11)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.7/dist-packages (from yake==0.4.8) (2.6.3)\n",
            "Requirement already satisfied: jellyfish in /usr/local/lib/python3.7/dist-packages (from yake==0.4.8) (0.9.0)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from segtok->yake==0.4.8) (2019.12.20)\n"
          ]
        }
      ],
      "source": [
        "! pip install git+https://github.com/LIAAD/yake\n",
        "import yake"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "from math import log, sqrt\n",
        "import re\n",
        "import numpy as np\n",
        "import sys\n",
        "from copy import deepcopy\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import csv\n",
        "import json\n",
        "from itertools import islice\n",
        "from collections import OrderedDict\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import os\n",
        "import nltk\n",
        "from glob import glob\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "from tqdm import tqdm\n",
        "import pickle\n",
        "import math\n",
        "import re\n",
        "from sklearn.model_selection import train_test_split\n",
        "import operator\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "import datetime\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('corpus')"
      ],
      "metadata": {
        "id": "G49FIo8uLD8j",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7d67610a-8c2c-47a2-cdb5-674cbaed4c8d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Error loading corpus: Package 'corpus' not found in index\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "gShHsbJb_jDW"
      },
      "outputs": [],
      "source": [
        "file1 = open(\"/content/drive/MyDrive/Tweelink_Dataset/twitter_base_preprocessed.pkl\", \"rb\")\n",
        "df = pickle.load(file1)\n",
        "file1.close()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_relevant_docs_list_for_base_hashtag(base_hashtag, base_date, docs_preprocessed):\n",
        "  relevant_docs_list = []\n",
        "  for doc in docs_preprocessed:\n",
        "    if doc['Data']['Base Hashtag']==base_hashtag:\n",
        "      current_date = datetime.datetime.strptime(base_date, format)\n",
        "      prev_date = current_date - datetime.timedelta(days=1)\n",
        "      next_date = current_date + datetime.timedelta(days=1)\n",
        "      if(doc['Data']['Date'] in [str(prev_date.date()), str(current_date.date()), str(next_date.date())]):\n",
        "        relevant_docs_list.append(doc['Name'])\n",
        "  return relevant_docs_list"
      ],
      "metadata": {
        "id": "FRVSYuVjT3cm"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def precision_at_k(k, base_hashtag, base_date, prediction_list, docs_preprocessed):\n",
        "  relevant_docs_list = get_relevant_docs_list_for_base_hashtag(base_hashtag, base_date, docs_preprocessed)\n",
        "  num_of_relevant_results=0\n",
        "  for itr in range(k):\n",
        "    if (prediction_list[itr] in relevant_docs_list):\n",
        "      num_of_relevant_results+=1\n",
        "  return num_of_relevant_results/k"
      ],
      "metadata": {
        "id": "uiu_FtqqT4GS"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def mean_average_precision(max_k, base_hashtag, base_date, relevant_docs, docs_preprocessed):\n",
        "  average_precision=0\n",
        "  ctr=0\n",
        "  for k_val in range(1,max_k+1):\n",
        "    ctr+=1\n",
        "    precision_at_k_val = precision_at_k(k_val, base_hashtag, base_date, relevant_docs, docs_preprocessed)\n",
        "    #print('Hashtag: {}   Precision@{}: {}'.format(base_hashtag, k_val, precision_at_k_val))\n",
        "    average_precision += precision_at_k_val\n",
        "  return average_precision/ctr"
      ],
      "metadata": {
        "id": "4Q6W2l8-T6G3"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def recall_at_k(k, base_hashtag, base_date, prediction_list, docs_preprocessed):\n",
        "  relevant_docs_list = get_relevant_docs_list_for_base_hashtag(base_hashtag, base_date, docs_preprocessed)\n",
        "  current_num_of_relevant_results=0\n",
        "  for itr in range(k):\n",
        "    if (prediction_list[itr] in relevant_docs_list):\n",
        "      current_num_of_relevant_results+=1\n",
        "  if(len(relevant_docs_list)==0):\n",
        "    return 0\n",
        "  return current_num_of_relevant_results/len(relevant_docs_list)"
      ],
      "metadata": {
        "id": "cihke9X6T7oh"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def mean_average_recall(max_k, base_hashtag, base_date, relevant_docs, docs_preprocessed):\n",
        "  average_recall=0\n",
        "  ctr=0\n",
        "  for k_val in range(1,max_k+1):\n",
        "    ctr+=1\n",
        "    recall_at_k_val = recall_at_k(k_val, base_hashtag, base_date, relevant_docs, docs_preprocessed)\n",
        "    #print('Hashtag: {}   Recall@{}: {}'.format(base_hashtag, k_val, recall_at_k_val))\n",
        "    average_recall += recall_at_k_val\n",
        "  return average_recall/ctr"
      ],
      "metadata": {
        "id": "B52GnqVwT9Sf"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def keyword_extractor(dataset):\n",
        "  preprocessed_vocabulary = dict()\n",
        "\n",
        "  #Converting to lowercase\n",
        "  def to_lower_case(text):\n",
        "    text = text.lower()\n",
        "    return text\n",
        "\n",
        "  def remove_at_word(text):\n",
        "    data = text.split()\n",
        "    data = [d for d in data if d[0]!='@']\n",
        "    text = ' '.join(data)\n",
        "    return text\n",
        "\n",
        "  def remove_hashtag(text):\n",
        "    data = text.split()\n",
        "    data = [d if (d[0]!='#' or len(d) == 1) else d[1:] for d in data]\n",
        "    data = [d for d in data if d[0]!='#']\n",
        "    text = ' '.join(data)\n",
        "    return text\n",
        "\n",
        "  def remove_URL(text):\n",
        "    text = re.sub(r\"http\\S+\", \"\", text)\n",
        "    text = re.sub(r'bit.ly\\S+', '', text, flags=re.MULTILINE)\n",
        "    return text\n",
        "\n",
        "  #Removing stopwords\n",
        "  def remove_stopwords(text):\n",
        "    stopword = stopwords.words('english')\n",
        "    new_list = [x for x in text.split() if x not in stopword]\n",
        "    return ' '.join(new_list)\n",
        "\n",
        "  #Removing punctuations\n",
        "  def remove_punctuations(text):\n",
        "    punctuations = '''!()-[|]`{};:'\"\\,<>./?@#$=+%^&*_~'''\n",
        "    new_list = ['' if x in punctuations else x for x in text.split()]\n",
        "    new_list_final = []\n",
        "    for token in new_list:\n",
        "      new_token=\"\"\n",
        "      for char in token:\n",
        "        if(char not in punctuations):\n",
        "          new_token+=char\n",
        "      if(len(new_token)!=0):\n",
        "        new_list_final.append(new_token)\n",
        "    return ' '.join(new_list_final)\n",
        "\n",
        "  #Tokenization\n",
        "  def tokenization(text):\n",
        "    return word_tokenize(text)\n",
        "\n",
        "  def pre_process(text):\n",
        "    text = to_lower_case(text)\n",
        "    text = remove_at_word(text)\n",
        "    text = remove_hashtag(text)\n",
        "    text = remove_URL(text)\n",
        "    text = remove_stopwords(text)\n",
        "    text = remove_punctuations(text)\n",
        "    text = tokenization(text)\n",
        "    for token in text:\n",
        "      if token in preprocessed_vocabulary.keys():\n",
        "        preprocessed_vocabulary[token] += 1\n",
        "      else:\n",
        "        preprocessed_vocabulary[token] = 1\n",
        "    return text\n",
        "  \n",
        "  preprocessed_data = [pre_process(text) for text in dataset]\n",
        "\n",
        "  #print(preprocessed_vocabulary)\n",
        "\n",
        "  AOF_coefficient = sum(preprocessed_vocabulary.values())/len(preprocessed_vocabulary)\n",
        "  vocabulary = {token.strip():preprocessed_vocabulary[token] for token in preprocessed_vocabulary.keys() if preprocessed_vocabulary[token] > AOF_coefficient and len(token.strip())}\n",
        "\n",
        "  #print(vocabulary)\n",
        "\n",
        "  final_tokens_per_tweet = []\n",
        "  for data in preprocessed_data:\n",
        "    final_tokens_per_tweet.append([token for token in data if token in vocabulary.keys()])\n",
        "\n",
        "  #print(preprocessed_data)\n",
        "  #print(final_tokens_per_tweet)\n",
        "\n",
        "  word2id = dict()\n",
        "  id2word = dict()\n",
        "  vocabulary_size = len(vocabulary)\n",
        "  count = 0\n",
        "  for token in vocabulary.keys():\n",
        "    word2id[token] = count\n",
        "    id2word[count] = token\n",
        "    count += 1\n",
        "\n",
        "  #print(word2id)\n",
        "  #print(id2word)\n",
        "\n",
        "  directed_graph_adjacency_matrix = np.zeros((vocabulary_size, vocabulary_size))\n",
        "  edge_weight_matrix = np.zeros((vocabulary_size, vocabulary_size))\n",
        "  first_frequency = dict()\n",
        "  last_frequency = dict()\n",
        "  term_frequency = vocabulary\n",
        "  strength = dict()\n",
        "  degree = dict()\n",
        "  selective_centraility = dict()\n",
        "\n",
        "\n",
        "  for tweet in final_tokens_per_tweet:\n",
        "\n",
        "    if tweet[0] in first_frequency.keys():\n",
        "      first_frequency[tweet[0]] += 1\n",
        "    else:\n",
        "      first_frequency[tweet[0]] = 1\n",
        "\n",
        "    if tweet[-1] in last_frequency.keys():\n",
        "      last_frequency[tweet[-1]] += 1\n",
        "    else:\n",
        "      last_frequency[tweet[-1]] = 1\n",
        "    \n",
        "\n",
        "\n",
        "    for i in range(len(tweet)-1):\n",
        "      if tweet[i] == tweet[i+1]:\n",
        "        continue\n",
        "      x = word2id[tweet[i]]\n",
        "      y = word2id[tweet[i+1]]\n",
        "      directed_graph_adjacency_matrix[x][y] += 1\n",
        "\n",
        "  for tweet in final_tokens_per_tweet:\n",
        "    for i in range(len(tweet)-1):\n",
        "\n",
        "\n",
        "      if tweet[i] == tweet[i+1]:\n",
        "        continue\n",
        "      x = word2id[tweet[i]]\n",
        "      y = word2id[tweet[i+1]]\n",
        "\n",
        "    # Updating degree..\n",
        "      if tweet[i] in degree.keys():\n",
        "        degree[tweet[i]] += 1\n",
        "      else:\n",
        "        degree[tweet[i]] = 1\n",
        "        \n",
        "      if tweet[i+1] in degree.keys():\n",
        "        degree[tweet[i+1]] += 1\n",
        "      else:\n",
        "        degree[tweet[i+1]] = 1\n",
        "\n",
        "      edge_weight_matrix[x][y] = directed_graph_adjacency_matrix[x][y]/(vocabulary[tweet[i]] + vocabulary[tweet[i+1]] - directed_graph_adjacency_matrix[x][y])\n",
        "\n",
        "      if tweet[i] in strength.keys():\n",
        "        strength[tweet[i]] += edge_weight_matrix[x][y]\n",
        "      else:\n",
        "        strength[tweet[i]] = edge_weight_matrix[x][y]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  first_frequency = {token:(first_frequency[token]/vocabulary[token] if token in first_frequency else 0) for token in vocabulary.keys()}\n",
        "  last_frequency = {token:(last_frequency[token]/vocabulary[token] if token in last_frequency else 0) for token in vocabulary.keys()}\n",
        "  degree = {token:(degree[token] if token in degree else 0) for token in vocabulary.keys()}\n",
        "  strength = {token:(strength[token] if token in strength else 0) for token in vocabulary.keys()}\n",
        "  selective_centraility = {token:(strength[token]/degree[token] if degree[token]!=0 else 0) for token in vocabulary.keys()}\n",
        "\n",
        "  #print(degree)\n",
        "  #print(vocabulary)\n",
        "\n",
        "  maxdegree = max(degree.items(), key=lambda x: x[1])[1]\n",
        "  max_degree_nodes_with_freq = {key:term_frequency[key] for key in degree.keys() if degree[key] == maxdegree}\n",
        "  maxfreq = max(max_degree_nodes_with_freq.items(), key=lambda x: x[1])[1]\n",
        "  central_node_name = [key for key in max_degree_nodes_with_freq.keys() if max_degree_nodes_with_freq[key] == maxfreq][0]\n",
        "  #print(\"central node: \", central_node_name)\n",
        "\n",
        "  # bfs\n",
        "  distance_from_central_node = dict()\n",
        "  central_node_id = word2id[central_node_name]\n",
        "  q = [(central_node_id, 0)]\n",
        "\n",
        "  # Set source as visited\n",
        "  distance_from_central_node[central_node_name] = 0\n",
        "\n",
        "  while q:\n",
        "      vis = q[0]\n",
        "      # Print current node\n",
        "      #print(id2word[vis[0]], vis[1])\n",
        "      q.pop(0)\n",
        "        \n",
        "      # For every adjacent vertex to\n",
        "      # the current vertex\n",
        "      for i in range(len(directed_graph_adjacency_matrix[vis[0]])):\n",
        "          if (directed_graph_adjacency_matrix[vis[0]][i] == 1 and (id2word[i] not in distance_from_central_node.keys())):\n",
        "              # Push the adjacent node\n",
        "              # in the queue\n",
        "              q.append((i, vis[1]+1))\n",
        "              distance_from_central_node[id2word[i]] = vis[1]+1\n",
        "\n",
        "  #print(distance_from_central_node)\n",
        "  inverse_distance_from_central_node = {token:(1/distance_from_central_node[token] if token in distance_from_central_node and token != central_node_name else 0) for token in vocabulary.keys()}\n",
        "  inverse_distance_from_central_node[central_node_name] = 1.0\n",
        "  #print(inverse_distance_from_central_node)\n",
        "\n",
        "  neighbour_importance = dict()\n",
        "\n",
        "  for i in range(len(directed_graph_adjacency_matrix)):\n",
        "    neighbours = set()\n",
        "\n",
        "    # traversing outgoing edges\n",
        "    for j in range(len(directed_graph_adjacency_matrix)):\n",
        "      if i == j:\n",
        "        continue\n",
        "      if directed_graph_adjacency_matrix[i][j] > 0:\n",
        "        neighbours.add(j)\n",
        "    for j in range(len(directed_graph_adjacency_matrix)):\n",
        "      if i == j:\n",
        "        continue\n",
        "      if directed_graph_adjacency_matrix[j][i] > 0:\n",
        "          neighbours.add(j)\n",
        "    if len(neighbours) != 0:\n",
        "      neighbour_importance[id2word[i]] = sum([strength[id2word[j]] for j in neighbours])/len(neighbours)\n",
        "    else:\n",
        "      neighbour_importance[id2word[i]] = 0\n",
        "      \n",
        "  #print(neighbour_importance)\n",
        "\n",
        "  unnormalized_node_weight = {node: (first_frequency[node] + last_frequency[node] + term_frequency[node] + selective_centraility[node] + inverse_distance_from_central_node[node] + neighbour_importance[node]) for node in vocabulary.keys()}\n",
        "  max_node_weight = max(unnormalized_node_weight.items(), key=lambda x: x[1])[1]\n",
        "  min_node_weight = min(unnormalized_node_weight.items(), key=lambda x: x[1])[1]\n",
        "  #print(\"max node weight: \", max_node_weight, \"min node weight: \", min_node_weight)\n",
        "  normalized_node_weight = {node: ((unnormalized_node_weight[node] - min_node_weight)/(max_node_weight - min_node_weight) if max_node_weight != min_node_weight else unnormalized_node_weight[node]) for node in unnormalized_node_weight.keys()}\n",
        "  #print(\"Unnormalized score: \", unnormalized_node_weight)\n",
        "  #print(\"Normalized score: \", normalized_node_weight)\n",
        "\n",
        "  damping_factor = 0.85\n",
        "  relevance_of_node = {node: np.random.uniform(0,1,1)[0] for node in vocabulary.keys()}\n",
        "  threshold = 0.000000001\n",
        "\n",
        "\n",
        "  #print(relevance_of_node)\n",
        "\n",
        "  count = 0\n",
        "  while True:\n",
        "    count += 1\n",
        "    current_relevance_of_node = dict()\n",
        "    for node in vocabulary.keys():\n",
        "      outer_sum = 0\n",
        "      node_idx = word2id[node]\n",
        "      for j in range(len(directed_graph_adjacency_matrix)):\n",
        "        if j == node_idx:\n",
        "          continue\n",
        "        if directed_graph_adjacency_matrix[j][node_idx] > 0:\n",
        "          den_sum = 0\n",
        "          for k in range(len(directed_graph_adjacency_matrix)):\n",
        "            if k == j:\n",
        "              continue\n",
        "            den_sum += directed_graph_adjacency_matrix[j][k]\n",
        "          outer_sum += ((directed_graph_adjacency_matrix[j][node_idx]/den_sum) * relevance_of_node[id2word[j]])\n",
        "      current_relevance_of_node[node] = (1-damping_factor)*normalized_node_weight[node] + damping_factor*normalized_node_weight[node]*outer_sum\n",
        "    \n",
        "\n",
        "    # checking convergence..\n",
        "    sq_error = sum([(current_relevance_of_node[node] - relevance_of_node[node])**2 for node in vocabulary.keys()])\n",
        "    relevance_of_node = current_relevance_of_node\n",
        "    if sq_error < threshold:\n",
        "      break\n",
        "\n",
        "  #print(relevance_of_node)\n",
        "  #print(count)\n",
        "\n",
        "  degree_centrality  = {node: 0 for node in vocabulary.keys()}\n",
        "\n",
        "  if len(directed_graph_adjacency_matrix) > 1:\n",
        "    for i in range(len(directed_graph_adjacency_matrix)):\n",
        "      count = 0\n",
        "      for j in range(len(directed_graph_adjacency_matrix)):\n",
        "        if i == j:\n",
        "          continue\n",
        "        if directed_graph_adjacency_matrix[j][i] > 0:\n",
        "          count += 1\n",
        "      degree_centrality[id2word[i]] = count / (len(directed_graph_adjacency_matrix)-1)\n",
        "\n",
        "  #print(degree_centrality)\n",
        "\n",
        "  final_keyword_rank = [{'node': node, 'NE_rank': relevance_of_node[node], 'Degree': degree_centrality[node]} for node in vocabulary.keys()]\n",
        "\n",
        "  #print(\"-----------\")\n",
        "  final_keyword_rank = sorted(final_keyword_rank, key = lambda i: (i['NE_rank'], i['Degree']), reverse = True)\n",
        "\n",
        "  final_keywords = [keyword['node'] for keyword in final_keyword_rank]\n",
        "\n",
        "  return final_keywords"
      ],
      "metadata": {
        "id": "PC2WawicN08A"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "npSDU9no-G68"
      },
      "outputs": [],
      "source": [
        "# ## Import the data ad create the inverted index\n",
        "\n",
        "def import_dataset():\n",
        "    \"\"\"\n",
        "    This function import all the articles in the TIME corpus,\n",
        "    returning list of lists where each sub-list contains all the\n",
        "    terms present in the document as a string.\n",
        "    \"\"\"\n",
        "    # articles = []\n",
        "    # with open('TIME.ALL', 'r') as f:\n",
        "    #     tmp = []\n",
        "    #     for row in f:\n",
        "    #         if row.startswith(\"*TEXT\"):\n",
        "    #             if tmp != []:\n",
        "    #                 articles.append(tmp)\n",
        "    #             tmp = []\n",
        "    #         else:\n",
        "    #             row = re.sub(r'[^a-zA-Z\\s]+', '', row)\n",
        "    #             tmp += row.split()\n",
        "    # return articles\n",
        "\n",
        "    docs_preprocessed_metrics = []\n",
        "    docs_preprocessed = []\n",
        "    docs_preprocessed_with_names = []\n",
        "    path = '/content/drive/MyDrive/Tweelink_Dataset/Tweelink_Articles_Processed'\n",
        "    for filename in glob(os.path.join(path, '*')):\n",
        "      with open(os.path.join(os.getcwd(), filename), 'r', encoding = 'utf-8',errors = 'ignore') as f:\n",
        "        filename = os.path.basename(f.name)\n",
        "        data = json.load(f)\n",
        "        d_date = data[\"Date\"]\n",
        "        if(d_date==\"\" or d_date==\"Date\"):\n",
        "          continue\n",
        "        format = '%Y-%m-%d'\n",
        "    \n",
        "        d_present_date = datetime.datetime.strptime(d_date, format)\n",
        "    \n",
        "        if(str(d_present_date.date()) not in [str(u_present_date.date()), str(u_prev_date.date()), str(u_next_date.date())]):\n",
        "          continue\n",
        "      \n",
        "        docs_preprocessed_metrics.append({'Name':filename, 'Data':data})\n",
        "        docs_preprocessed.append(data['Body_processed'])\n",
        "        docs_preprocessed_with_names.append(filename)\n",
        "\n",
        "    return docs_preprocessed, docs_preprocessed_with_names, docs_preprocessed_metrics\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# def remove_stop_words(corpus):\n",
        "#    '''\n",
        "#    This function removes from the corpus all the stop words present in the file TIME.STP\n",
        "#    '''\n",
        "#    stop_w = [line.rstrip('\\n') for line in open('TIME.STP')]\n",
        "#    stop_w=list(filter(None, stop_w))\n",
        "#    for i in range(0,len(corpus)):\n",
        "#        corpus[i] = [x for x in corpus[i] if x not in stop_w]\n",
        "#    return corpus \n",
        "\n",
        "\n",
        "def make_inverted_index(corpus):\n",
        "    \"\"\"\n",
        "    This function builds an inverted index as an hash table (dictionary)\n",
        "    where the keys are the terms and the values are ordered lists of\n",
        "    docIDs containing the term.\n",
        "    \"\"\"\n",
        "    # corpus = remove_stop_words(corpus)\n",
        "    index = defaultdict(set)\n",
        "    for docid, article in enumerate(corpus):\n",
        "        for term in article:\n",
        "            index[term].add(docid)\n",
        "    return index\n",
        "\n",
        "\n",
        "# ### Union of two posting lists\n",
        "\n",
        "\n",
        "def posting_lists_union(pl1, pl2):\n",
        "        \"\"\"\n",
        "        Returns a new posting list resulting from the union of the\n",
        "        two lists passed as arguments.\n",
        "        \"\"\"\n",
        "        pl1 = sorted(list(pl1))\n",
        "        pl2 = sorted(list(pl2))\n",
        "        union = []\n",
        "        i = 0\n",
        "        j = 0\n",
        "        while (i < len(pl1) and j < len(pl2)):\n",
        "            if (pl1[i] == pl2[j]):\n",
        "                union.append(pl1[i])\n",
        "                i += 1\n",
        "                j += 1\n",
        "            elif (pl1[i] < pl2[j]):\n",
        "                union.append(pl1[i])\n",
        "                i += 1\n",
        "            else:\n",
        "                union.append(pl2[j])\n",
        "                j += 1\n",
        "        for k in range(i, len(pl1)):\n",
        "            union.append(pl1[k])\n",
        "        for k in range(j, len(pl2)):\n",
        "            union.append(pl2[k])\n",
        "        return union\n",
        "\n",
        "\n",
        "# ## Precomputing weights\n",
        "\n",
        "\n",
        "def DF(term, index):\n",
        "    '''\n",
        "    Function computing Document Frequency for a term.\n",
        "    '''\n",
        "    return len(index[term])\n",
        "\n",
        "\n",
        "def IDF(term, index, corpus):\n",
        "    '''\n",
        "    Function computing Inverse Document Frequency for a term.\n",
        "    '''\n",
        "    return log(len(corpus)/DF(term, index))\n",
        "\n",
        "\n",
        "def RSV_weights(corpus,index):\n",
        "    '''\n",
        "    This function precomputes the Retrieval Status Value weights\n",
        "    for each term in the index\n",
        "    '''\n",
        "    N = len(corpus)\n",
        "    w = {}\n",
        "    for term in index.keys():\n",
        "        p = DF(term, index)/(N+0.5)  \n",
        "        w[term] = IDF(term, index, corpus) + log(p/(1-p))\n",
        "    return w\n",
        "    \n",
        "\n",
        "\n",
        "# ## BIM Class\n",
        "\n",
        "\n",
        "class BIM():\n",
        "    '''\n",
        "    Binary Independence Model class\n",
        "    '''\n",
        "    \n",
        "    def __init__(self, corpus, articles_with_name):\n",
        "        self.original_corpus = deepcopy(corpus)\n",
        "        self.articles_with_name = articles_with_name\n",
        "        self.articles = corpus\n",
        "        self.index = make_inverted_index(self.articles)\n",
        "        self.weights = RSV_weights(self.articles, self.index)\n",
        "        self.ranked = []\n",
        "        self.query_text = ''\n",
        "        self.N_retrieved = 0\n",
        "    \n",
        "        \n",
        "    def RSV_doc_query(self, doc_id, query):\n",
        "        '''\n",
        "        This function computes the Retrieval Status Value for a given couple document - query\n",
        "        using the precomputed weights\n",
        "        '''\n",
        "        score = 0\n",
        "        doc = self.articles[doc_id]\n",
        "        for term in doc:\n",
        "            if term in query:\n",
        "                score += self.weights[term]     \n",
        "        return score\n",
        "\n",
        "    \n",
        "        \n",
        "    def ranking(self, query):\n",
        "        '''\n",
        "        Auxiliary function for the function answer_query. Computes the score only for documents\n",
        "        that are in the posting list of al least one term in the query\n",
        "        '''\n",
        "\n",
        "        docs = []\n",
        "        for term in self.index: \n",
        "            # print(term in query, query, term)\n",
        "            if term in query:\n",
        "                docs = posting_lists_union(docs, self.index[term])\n",
        "\n",
        "        scores = []\n",
        "        for doc in docs:\n",
        "            scores.append((doc, self.RSV_doc_query(doc, query)))\n",
        "        \n",
        "        self.ranked = sorted(scores, key=lambda x: x[1], reverse = True)\n",
        "        return self.ranked\n",
        "    \n",
        "    \n",
        "    \n",
        "    def recompute_weights(self, relevant_idx, query):\n",
        "        '''\n",
        "        Auxiliary function for relevance_feedback function and\n",
        "        for the pseudo relevance feedback in answer_query function.\n",
        "        Recomputes the weights, only for the terms in the query\n",
        "        based on a set of relevant documents.\n",
        "        '''\n",
        "        \n",
        "        relevant_docs = []\n",
        "        for idx in relevant_idx:\n",
        "            doc_id = self.ranked[idx-1][0]\n",
        "            relevant_docs.append(self.articles[doc_id])\n",
        "        \n",
        "        N = len(self.articles)\n",
        "        N_rel = len(relevant_idx)\n",
        "        \n",
        "        for term in query:\n",
        "            if term in self.weights.keys():\n",
        "                vri = 0\n",
        "                for doc in relevant_docs:\n",
        "                    if term in doc:\n",
        "                        vri += 1\n",
        "                p = (vri + 0.5) /( N_rel + 1)\n",
        "                u = (DF(term, self.index) - vri + 0.5) / (N - N_rel +1)\n",
        "                self.weights[term] = log((1-u)/u) + log(p/(1-p))\n",
        "\n",
        "            \n",
        "    \n",
        "    def answer_query(self, query_text):\n",
        "        '''\n",
        "        Function to answer a free text query. Shows the first 30 words of the\n",
        "        15 most relevant documents. \n",
        "        Also implements the pseudo relevance feedback with k = 5\n",
        "        '''\n",
        "        \n",
        "        self.query_text = query_text\n",
        "        query =  query_text.lower().split()\n",
        "        ranking = self.ranking(query)\n",
        "\n",
        "        ## pseudo relevance feedback \n",
        "        i = 0\n",
        "        new_ranking=[]\n",
        "        while i<10 and ranking != new_ranking:\n",
        "            self.recompute_weights([1,2,3,4,5], query)\n",
        "            new_ranking = self.ranking(query)\n",
        "            i+=1\n",
        "\n",
        "\n",
        "        ranking = new_ranking\n",
        "        \n",
        "        self.N_retrieved = 20\n",
        "        \n",
        "        ## print retrieved documents\n",
        "        ret_list = []\n",
        "        for i in range(0, self.N_retrieved):\n",
        "            article = self.original_corpus[ranking[i][0]]\n",
        "            if (len(article) > 30):\n",
        "                article = article[0:30]\n",
        "            text = \" \".join(article)\n",
        "            print(f\"Article {self.articles_with_name[self.ranked[i][0]]}, score: {ranking[i][1]}\")\n",
        "            ret_list.append(self.articles_with_name[self.ranked[i][0]])\n",
        "            # print(text, '\\n')\n",
        "\n",
        "        self.weights = RSV_weights(self.articles, self.index)\n",
        "        return ret_list\n",
        "\n",
        "\n",
        "            \n",
        "    def relevance_feedback(self, *args):\n",
        "        '''\n",
        "        Function that implements relevance feedback for the last query formulated.\n",
        "        The set of relevant documents is given by the user through a set of indexes\n",
        "        '''\n",
        "        if(self.query_text == ''):\n",
        "            print('Cannot get feedback before a query is formulated.')\n",
        "            return\n",
        "        \n",
        "        relevant_idx = list(args)\n",
        "        \n",
        "        if(isinstance(relevant_idx[0], list)):\n",
        "            relevant_idx = relevant_idx[0]\n",
        "        \n",
        "        query = self.query_text.upper().split()\n",
        "        self.recompute_weights(relevant_idx,query)\n",
        "        \n",
        "        self.answer_query(self.query_text)\n",
        "    \n",
        "    \n",
        "    def read_document(self,rank_num):\n",
        "        '''\n",
        "        Function that allows the user to select a document among the ones returned \n",
        "        by answer_query and read the whole text\n",
        "        '''\n",
        "        if(self.query_text == ''):\n",
        "            print('Cannot select a document before a query is formulated.')\n",
        "            return\n",
        "            \n",
        "        article = self.original_corpus[self.ranked[rank_num - 1][0]]\n",
        "        text = \" \".join(article)\n",
        "        print(f\"Article {rank_num}, score: {self.ranked[rank_num][1]}\")\n",
        "        print(text, '\\n')\n",
        "        \n",
        "        \n",
        "    def show_more(self):\n",
        "        '''\n",
        "        Function that allows the user to see more 10 retrieved documents\n",
        "        '''\n",
        "        \n",
        "        if(self.N_retrieved + 10 > len(self.ranked)):\n",
        "            print('No more documents available')\n",
        "            return \n",
        "        \n",
        "        for i in range(self.N_retrieved, self.N_retrieved+10):\n",
        "            article = self.original_corpus[self.ranked[i][0]]\n",
        "            if (len(article) > 30):\n",
        "                article = article[0:30]\n",
        "            text = \" \".join(article)\n",
        "            print(f\"Article {self.articles_with_name[self.ranked[i][0]]}, score: {self.ranked[i][1]}\")\n",
        "            print(text, '\\n')\n",
        "        \n",
        "        self.N_retrieved += 10 \n",
        "        \n",
        "\n",
        "\n",
        "\n",
        "# Example of usage\n",
        "\n",
        "# articles, articles_with_name = import_dataset()\n",
        "# bim  = BIM(articles, articles_with_name)\n",
        "# bim.answer_query('Italy and Great Britain fight the enemy')\n",
        "# bim.relevance_feedback(5,6,8)\n",
        "# bim.show_more()\n",
        "# bim.read_document(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FqZ4Z7JT_oIz",
        "outputId": "b45a7ecc-b253-458b-80e9-6a17e2e968ac"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter base hashtag: narendramodi\n",
            "Enter time: 2022-02-20\n",
            "Enter Location: india\n"
          ]
        }
      ],
      "source": [
        "u_base_hashtag = input(\"Enter base hashtag: \")\n",
        "u_time = input(\"Enter time: \")\n",
        "u_location = input(\"Enter Location: \")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "yGrY-qLq_qSc"
      },
      "outputs": [],
      "source": [
        "tweet_query = []\n",
        "format = '%Y-%m-%d'\n",
        "u_present_date = datetime.datetime.strptime(u_time, format)\n",
        "u_prev_date = u_present_date - datetime.timedelta(days=1)\n",
        "u_next_date = u_present_date + datetime.timedelta(days=1)\n",
        "df_query = df.loc[df['hashtags'].str.contains(u_base_hashtag) & df['Date_Only'].isin([str(u_present_date.date()), str(u_prev_date.date()), str(u_next_date.date())])]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "pz867JVX_rxA"
      },
      "outputs": [],
      "source": [
        "for tweet in df_query['Preprocessed_Data']:\n",
        "  tweet_query.extend(tweet)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "keyword_dataset = df_query['tweet'].tolist()\n",
        "tweet_query_keyword_extractor = keyword_extractor(keyword_dataset)"
      ],
      "metadata": {
        "id": "536uef14OI7I"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wdCaO1wPClna",
        "outputId": "38ff3de1-6f53-4ae9-9118-e364f8616434"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keyphrase:  india bjp modi : score 3.124693022092362e-05\n",
            "Keyphrase:  narendramodi india bjp : score 6.037166731317872e-05\n",
            "Keyphrase:  bjp india narendramodi : score 6.037166731317872e-05\n",
            "Keyphrase:  india bjp narendramodi : score 6.037166731317872e-05\n",
            "Keyphrase:  bjp narendramodi india : score 6.037166731317872e-05\n",
            "Keyphrase:  modi india bjp : score 6.249386044184723e-05\n",
            "Keyphrase:  bjp narendramodi congress : score 6.461576268648576e-05\n",
            "Keyphrase:  congress bjp narendramodi : score 6.461576268648576e-05\n",
            "Keyphrase:  india narendramodi modi : score 6.581080854383824e-05\n",
            "Keyphrase:  narendramodi india modi : score 6.581080854383824e-05\n",
            "Keyphrase:  priyankagandhi rahulgandhi congress : score 6.790842832286181e-05\n",
            "Keyphrase:  bjp modi amitshah : score 6.851854888827196e-05\n",
            "Keyphrase:  rahulgandhi congress bjp : score 6.94561200328927e-05\n",
            "Keyphrase:  narendramodi modi congress : score 7.043825244061592e-05\n",
            "Keyphrase:  india modi amitshah : score 7.487064473666578e-05\n",
            "Keyphrase:  modi amitshah rahulgandhi : score 7.756272047694059e-05\n",
            "Keyphrase:  modi congress amitshah : score 8.013691280835908e-05\n",
            "Keyphrase:  prime minister modi : score 8.260812317526487e-05\n",
            "Keyphrase:  soniagandhi india bjp : score 8.933244596185653e-05\n",
            "Keyphrase:  modi amitshah politics : score 8.997304334446483e-05\n",
            "['india', 'bjp', 'modi', 'narendramodi', 'congress', 'priyankagandhi', 'rahulgandhi', 'amitshah', 'prime', 'minister', 'soniagandhi', 'politics']\n"
          ]
        }
      ],
      "source": [
        "tweet_keywords_yake = []\n",
        "kw_extractor = yake.KeywordExtractor(top=20, stopwords=None)\n",
        "keywords = kw_extractor.extract_keywords(' '.join(tweet_query))\n",
        "#keywords = kw_extractor.extract_keywords(' '.join(df_query['tweet'].tolist()))\n",
        "for kw, v in keywords:\n",
        "  print(\"Keyphrase: \",kw, \": score\", v)\n",
        "  for key in kw.split():\n",
        "    if(key.lower() not in tweet_keywords_yake):\n",
        "      tweet_keywords_yake.append(key.lower())\n",
        "\n",
        "print(tweet_keywords_yake)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example of usage\n",
        "articles, articles_with_name, docs_preprocessed_metrics = import_dataset()"
      ],
      "metadata": {
        "id": "u_GxOwk2G9-5"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plain Model (Binary Independence Model)"
      ],
      "metadata": {
        "id": "RjQVrV-cOcCG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#results with normal query\n",
        "bim  = BIM(articles, articles_with_name)\n",
        "relevant_docs_plain = bim.answer_query(\" \".join(tweet_query))\n",
        "\n",
        "# bim.relevance_feedback(5,6,8)\n",
        "# bim.show_more()\n",
        "# bim.read_document(2)\n",
        "\n",
        "print()\n",
        "\n",
        "mean_average_precision_hashtag_bim_plain = mean_average_precision(20, u_base_hashtag, u_time, relevant_docs_plain, docs_preprocessed_metrics)\n",
        "print('Mean Average Precision Plain Model (BIM) : {}'.format(mean_average_precision_hashtag_bim_plain))\n",
        "\n",
        "mean_average_recall_hashtag_bim_plain = mean_average_recall(20, u_base_hashtag, u_time, relevant_docs_plain, docs_preprocessed_metrics)\n",
        "print('Mean Average Recall Plain Model (BIM) : {}'.format(mean_average_recall_hashtag_bim_plain))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oK89bse1GWVn",
        "outputId": "7407d7fe-cf17-4ad4-a8e4-e9e087d89745"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Article PunjabElections2022_127.json, score: 851.822652980454\n",
            "Article PunjabElections2022_124.json, score: 447.1987543445336\n",
            "Article narendramodi_305.json, score: 372.3686959203364\n",
            "Article WriddhimanSaha_133.json, score: 311.070795969913\n",
            "Article narendramodi_256.json, score: 306.0638537661412\n",
            "Article PunjabElections2022_123.json, score: 255.53453475975095\n",
            "Article PunjabElections2022_121.json, score: 240.12392229968475\n",
            "Article narendramodi_303.json, score: 234.656032946595\n",
            "Article narendramodi_307.json, score: 228.21348893511322\n",
            "Article narendramodi_306.json, score: 228.02769184049515\n",
            "Article narendramodi_304.json, score: 197.77697697389016\n",
            "Article narendramodi_260.json, score: 183.86487484557077\n",
            "Article PunjabElections2022_125.json, score: 182.33913582029274\n",
            "Article PunjabElections2022_122.json, score: 172.5482501887701\n",
            "Article nbaallstar_286.json, score: 172.11136229422772\n",
            "Article hijab_286.json, score: 169.2184334596351\n",
            "Article narendramodi_253.json, score: 165.32434684995448\n",
            "Article narendramodi_302.json, score: 165.30221582449258\n",
            "Article PunjabElections2022_126.json, score: 160.35599969368172\n",
            "Article WriddhimanSaha_134.json, score: 139.29201156404824\n",
            "\n",
            "Mean Average Precision Plain Model (BIM) : 0.3943756963280957\n",
            "Mean Average Recall Plain Model (BIM) : 0.2285714285714286\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model with Keyword Extractor (Binary Independence Model)"
      ],
      "metadata": {
        "id": "WkyDKXEoU0Ch"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# results with top k keywords\n",
        "bim  = BIM(articles, articles_with_name)\n",
        "relevant_doce_yake = bim.answer_query(\" \".join(tweet_keywords_yake))\n",
        "\n",
        "# bim.relevance_feedback(5,6,8)\n",
        "# bim.show_more()\n",
        "# bim.read_document(2)\n",
        "\n",
        "print()\n",
        "\n",
        "mean_average_precision_hashtag_bim_keyword_extractor = mean_average_precision(20, u_base_hashtag, u_time, relevant_doce_yake, docs_preprocessed_metrics)\n",
        "print('Mean Average Precision Keyword Extractor Model (BIM) : {}'.format(mean_average_precision_hashtag_bim_keyword_extractor))\n",
        "\n",
        "mean_average_recall_hashtag_bim_keyword_extractor = mean_average_recall(20, u_base_hashtag, u_time, relevant_doce_yake, docs_preprocessed_metrics)\n",
        "print('Mean Average Recall Keyword Extractor Model (BIM) : {}'.format(mean_average_recall_hashtag_bim_keyword_extractor))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U_VagX4qU6Y9",
        "outputId": "ca5ccb31-0705-4032-a3ee-60570708fca5"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Article PunjabElections2022_127.json, score: 237.17816728316586\n",
            "Article narendramodi_253.json, score: 164.03729029589556\n",
            "Article narendramodi_305.json, score: 154.9973112219869\n",
            "Article narendramodi_306.json, score: 107.0378079638308\n",
            "Article narendramodi_256.json, score: 95.14273664320255\n",
            "Article narendramodi_260.json, score: 70.11842175694022\n",
            "Article PunjabElections2022_121.json, score: 68.36557348119001\n",
            "Article PunjabElections2022_123.json, score: 68.36557348119001\n",
            "Article narendramodi_302.json, score: 67.50878296262857\n",
            "Article UNWFPtosavesoil_153.json, score: 67.12815300153467\n",
            "Article narendramodi_307.json, score: 67.03615261198105\n",
            "Article hijab_286.json, score: 66.53316732419071\n",
            "Article narendramodi_254.json, score: 66.34696509453492\n",
            "Article narendramodi_303.json, score: 61.64186375019813\n",
            "Article WriddhimanSaha_133.json, score: 56.7718189960597\n",
            "Article PunjabElections2022_124.json, score: 53.06291345389182\n",
            "Article narendramodi_304.json, score: 52.5969315051052\n",
            "Article UNWFPtosavesoil_159.json, score: 52.2107856678603\n",
            "Article ScottyFromWelding_161.json, score: 52.02015646694464\n",
            "Article narendramodi_251.json, score: 49.42757394711146\n",
            "\n",
            "Mean Average Precision Keyword Extractor Model (BIM) : 0.6008248874018998\n",
            "Mean Average Recall Keyword Extractor Model (BIM) : 0.30238095238095236\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model with YAKE (Binary Independence Model)"
      ],
      "metadata": {
        "id": "ePByFPGvT1hW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PblKJeWT_8YP",
        "outputId": "fc7fe5ce-82b5-45f9-9b52-049c46c17be7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Article PunjabElections2022_127.json, score: 237.17816728316586\n",
            "Article narendramodi_253.json, score: 164.03729029589556\n",
            "Article narendramodi_305.json, score: 154.9973112219869\n",
            "Article narendramodi_306.json, score: 107.0378079638308\n",
            "Article narendramodi_256.json, score: 95.14273664320255\n",
            "Article narendramodi_260.json, score: 70.11842175694022\n",
            "Article PunjabElections2022_121.json, score: 68.36557348119001\n",
            "Article PunjabElections2022_123.json, score: 68.36557348119001\n",
            "Article narendramodi_302.json, score: 67.50878296262857\n",
            "Article UNWFPtosavesoil_153.json, score: 67.12815300153467\n",
            "Article narendramodi_307.json, score: 67.03615261198105\n",
            "Article hijab_286.json, score: 66.53316732419071\n",
            "Article narendramodi_254.json, score: 66.34696509453492\n",
            "Article narendramodi_303.json, score: 61.64186375019813\n",
            "Article WriddhimanSaha_133.json, score: 56.7718189960597\n",
            "Article PunjabElections2022_124.json, score: 53.06291345389182\n",
            "Article narendramodi_304.json, score: 52.5969315051052\n",
            "Article UNWFPtosavesoil_159.json, score: 52.2107856678603\n",
            "Article ScottyFromWelding_161.json, score: 52.02015646694464\n",
            "Article narendramodi_251.json, score: 49.42757394711146\n",
            "\n",
            "Mean Average Precision Plain Model (BIM) : 0.6008248874018998\n",
            "Mean Average Recall Plain Model (BIM) : 0.30238095238095236\n"
          ]
        }
      ],
      "source": [
        "# results with top k keywords\n",
        "bim  = BIM(articles, articles_with_name)\n",
        "relevant_docs_yake = bim.answer_query(\" \".join(tweet_keywords_yake))\n",
        "\n",
        "# bim.relevance_feedback(5,6,8)\n",
        "# bim.show_more()\n",
        "# bim.read_document(2)\n",
        "\n",
        "print()\n",
        "\n",
        "mean_average_precision_hashtag_bim_yake = mean_average_precision(20, u_base_hashtag, u_time, relevant_docs_yake, docs_preprocessed_metrics)\n",
        "print('Mean Average Precision Plain Model (BIM) : {}'.format(mean_average_precision_hashtag_bim_yake))\n",
        "\n",
        "mean_average_recall_hashtag_bim_yake = mean_average_recall(20, u_base_hashtag, u_time, relevant_docs_yake, docs_preprocessed_metrics)\n",
        "print('Mean Average Recall Plain Model (BIM) : {}'.format(mean_average_recall_hashtag_bim_yake))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# global_list = [['narendramodi', '2022-02-14', 'india'],['UkraineRussiaCrisis', '2022-02-14', 'ukraine'],['IPL', '2022-02-14', 'india'],['TaylorSwift', '2022-02-14', 'USA'],['IndiaFightsCorona', '2022-02-14', 'india'],['narendramodi', '2022-02-15', 'india'],['UkraineRussiaCrisis', '2022-02-15', 'ukraine'],['IPL', '2022-02-15', 'india'],['TaylorSwift', '2022-02-15', 'USA'],['IndiaFightsCorona', '2022-02-15', 'india'],['narendramodi', '2022-02-16', 'india'],['UkraineRussiaCrisis', '2022-02-16', 'ukraine'],['IPL', '2022-02-16', 'india'],['TaylorSwift', '2022-02-16', 'USA'],['IndiaFightsCorona', '2022-02-16', 'india'],['narendramodi', '2022-02-17', 'india'],['UkraineRussiaCrisis', '2022-02-17', 'ukraine'],['IPL', '2022-02-17', 'india'],['TaylorSwift', '2022-02-17', 'USA'],['IndiaFightsCorona', '2022-02-17', 'india'],['narendramodi', '2022-02-18', 'india'],['UkraineRussiaCrisis', '2022-02-18', 'ukraine'],['IPL', '2022-02-18', 'india'],['TaylorSwift', '2022-02-18', 'USA'],['IndiaFightsCorona', '2022-02-18', 'india'],['narendramodi', '2022-02-19', 'india'],['UkraineRussiaCrisis', '2022-02-19', 'ukraine'],['IPL', '2022-02-19', 'india'],['hijab', '2022-02-19', 'india'],['vaccine', '2022-02-19', 'india'],['MillionAtIndiaPavilion', '2022-02-14', 'UAE'],['PunjabPanjeNaal', '2022-02-14', 'India'],['Euphoria', '2022-02-14', 'World'],['OscarsFanFavorite', '2022-02-14', 'World'],['ShameOnBirenSingh', '2022-02-14', 'india'],['BappiLahiri', '2022-02-16', 'india'],['BlandDoritos', '2022-02-16', 'USA'],['VERZUZ', '2022-02-16', 'USA'],['DragRaceUK', '2022-02-16', 'United Kingdom'],['BoycottWalgreens', '2022-02-18', 'USA'],['PunjabElections2022', '2022-02-20', 'india'],['WriddhimanSaha', '2022-02-20', 'india'],['stormfranklin', '2022-02-20', 'USA'],['QueenElizabeth', '2022-02-20', 'United Kingdom'],['ScottyFromWelding', '2022-02-20', 'Australia'],['CarabaoCupFinal', '2022-02-27', 'London'],['NZvSA', '2022-02-28', 'New Zealand'],['IPCC', '2022-02-28', 'Worldwide'],['SuperBowl', '2022-02-14', 'USA'],['MultiverseOfMadness', '2022-02-14', 'USA'],['Eminem', '2022-02-14', 'USA'],['IPLAuction', '2022-02-14', 'india'],['JohnsonOut21', '2022-02-14', 'United Kingdom'],['Cyberpunk2077', '2022-02-15', 'Worldwide'],['Wordle242', '2022-02-15', 'Worldwide'],['DeepSidhu', '2022-02-15', 'india'],['CanadaHasFallen', '2022-02-15', 'canada'],['IStandWithTrudeau', '2022-02-15', 'canada'],['CNNPHVPDebate', '2022-02-26', 'philippines'],['qldfloods', '2022-02-26', 'australia'],['Eurovision', '2022-02-26', 'worldwide'],['IndiansInUkraine', '2022-02-26', 'india'],['PritiPatel', '2022-02-26', 'united kingdom'],['TaylorCatterall', '2022-02-27', 'united kingdom'],['PSLFinal', '2022-02-27', 'pakistan'],['AustraliaDecides', '2022-02-27', 'australia'],['WorldNGODay', '2022-02-27', 'worldwide'],['TheBatman', '2022-02-28', 'USA'],['NationalScienceDay', '2022-02-28', 'india'],['msdtrong', '2022-02-14', 'india'],['Boycott_ChennaiSuperKings', '2022-02-14', 'india'],['GlanceJio', '2022-02-14', 'india'],['ArabicKuthu', '2022-02-14', 'india'],['Djokovic', '2022-02-15', 'australia'],['Real Madrid', '2022-02-15', 'santiago'],['bighit', '2022-02-15', 'korea'],['Maxwell', '2022-02-15', 'australia'],['mafsau', '2022-02-16', 'australia'],['channi', '2022-02-16', 'punjab'],['ayalaan', '2022-02-16', 'india'],['jkbose', '2022-02-16', 'india'],['HappyBirthdayPrinceSK', '2022-02-16', 'india'],['RandomActsOfKindnessDay', '2022-02-17', 'worldwide'],['happybirthdayjhope', '2022-02-17', 'korea'],['mohsinbaig', '2022-02-17', 'pakistan'],['aewdynamite', '2022-02-17', 'worldwide'],['aaraattu', '2022-02-17', 'india'],['ShivajiJayanti', '2022-02-18', 'india'],['PlotToKillModi', '2022-02-18', 'india'],['NationalDrinkWineDay', '2022-02-18', 'usa'],['HorizonForbiddenWest', '2022-02-18', 'worldwide'],['BoycottWalgreens', '2022-02-18', 'usa'],['CallTheMidwife', '2022-02-20', 'worldwide'],['OperationDudula', '2022-02-20', 'south africa'],['truthsocial', '2022-02-21', 'usa'],['nbaallstar', '2022-02-21', 'usa'],['shivamogga', '2022-02-21', 'india'],['HalftimeShow', '2022-02-14', 'usa'],['OttawaStrong', '2022-02-14', 'canada'],['DrDre', '2022-02-14', 'usa'],['BattleOfBillingsBridge', '2022-02-14', 'usa'],['FullyFaltooNFTdrop', '2022-02-14', 'worldwide'],['AK61', '2022-02-15', 'india'],['sandhyamukherjee', '2022-02-15', 'india'],['MUNBHA', '2022-02-15', 'worldwide'],['nursesstrike', '2022-02-15', 'australia'],['Realme9ProPlus', '2022-02-16', 'worldwide'],['KarnatakaHijabControversy', '2022-02-16', 'india'],['BJPwinningUP', '2022-02-16', 'india'],['Punjab_With_Modi', '2022-02-16', 'india'],['PushpaTheRule', '2022-02-16', 'india'],['RehmanMalik', '2022-02-22', 'india'],['harisrauf', '2022-02-22', 'pakistan'],['Rosettenville', '2022-02-22', 'south africa'],['NFU22', '2022-02-22', 'worldwide'],['justiceforharsha', '2022-02-22', 'india'],['wordle251', '2022-02-24', 'worldwide'],['ARSWOL', '2022-02-24', 'worldwide'],['stopwar', '2022-02-24', 'worldwide'],['PrayForPeace', '2022-02-24', 'worldwide'],['StopPutinNOW', '2022-02-24', 'worldwide'],['TeamGirlsCup', '2022-02-25', 'worldwide'],['Canucks', '2022-02-25', 'worldwide'],['PinkShirtDay', '2022-02-25', 'canada'],['superrugbypacific', '2022-02-25', 'australia']]"
      ],
      "metadata": {
        "id": "tO8i_IIiH9PI"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# global_average_mean_average_precision_1 = []\n",
        "# global_mean_average_recall_1 = []\n",
        "\n",
        "# global_average_mean_average_precision_2 = []\n",
        "# global_mean_average_recall_2 = []\n",
        "\n",
        "# for iter in tqdm(range(len(global_list))):\n",
        "#   u_base_hashtag = global_list[iter][0]\n",
        "#   u_time = global_list[iter][1]\n",
        "#   u_location = global_list[iter][2]\n",
        "#   tweet_query = []\n",
        "#   format = '%Y-%m-%d'\n",
        "#   u_present_date = datetime.datetime.strptime(u_time, format)\n",
        "#   u_prev_date = u_present_date - datetime.timedelta(days=1)\n",
        "#   u_next_date = u_present_date + datetime.timedelta(days=1)\n",
        "#   df_query = df.loc[df['hashtags'].str.contains(u_base_hashtag) & df['Date_Only'].isin([str(u_present_date.date()), str(u_prev_date.date()), str(u_next_date.date())])]\n",
        "\n",
        "#   for tweet in df_query['Preprocessed_Data']:\n",
        "#     tweet_query.extend(tweet)\n",
        "  \n",
        "#   tweet_keywords = []\n",
        "#   kw_extractor = yake.KeywordExtractor(top=20, stopwords=None)\n",
        "#   keywords = kw_extractor.extract_keywords(' '.join(tweet_query))\n",
        "#   for kw, v in keywords:\n",
        "#     #print(\"Keyphrase: \",kw, \": score\", v)\n",
        "#     for key in kw.split():\n",
        "#       if(key not in tweet_keywords):\n",
        "#         tweet_keywords.append(key)\n",
        "  \n",
        "#   docs_preprocessed = []\n",
        "\n",
        "#   total_documents = 0\n",
        "#   path = '/content/drive/MyDrive/Tweelink_Dataset/Tweelink_Articles_Processed'\n",
        "#   for filename in glob(os.path.join(path, '*')):\n",
        "#     with open(os.path.join(os.getcwd(), filename), 'r', encoding = 'utf-8',errors = 'ignore') as f:\n",
        "#       filename = os.path.basename(f.name)\n",
        "#       data = json.load(f)\n",
        "#       d_date = data[\"Date\"]\n",
        "#       if(d_date==\"\" or d_date==\"Date\"):\n",
        "#         continue\n",
        "#       format = '%Y-%m-%d'\n",
        "  \n",
        "#       d_present_date = datetime.datetime.strptime(d_date, format)\n",
        "  \n",
        "#       if(str(d_present_date.date()) not in [str(u_present_date.date()), str(u_prev_date.date()), str(u_next_date.date())]):\n",
        "#         continue\n",
        "    \n",
        "#       docs_preprocessed.append({'Name':filename, 'Data':data})\n",
        "#       total_documents+=1\n",
        "  \n",
        "#   articles, articles_with_name, docs_preprocessed_metrics = import_dataset()\n",
        "#   bim  = BIM(articles, articles_with_name)\n",
        "#   relevant_articles_list_1 = bim.answer_query(\" \".join(tweet_query))\n",
        "#   mean_average_precision_hashtag_1 = mean_average_precision(20, u_base_hashtag, u_time, relevant_articles_list_1, docs_preprocessed_metrics)\n",
        "#   global_average_mean_average_precision_1.append(mean_average_precision_hashtag_1)\n",
        "#   mean_average_recall_hashtag_1 = mean_average_recall(20, u_base_hashtag, u_time, relevant_articles_list_1, docs_preprocessed_metrics)\n",
        "#   global_mean_average_recall_1.append(mean_average_recall_hashtag_1)\n",
        "\n",
        "#   bim  = BIM(articles, articles_with_name)\n",
        "#   relevant_articles_list_2 = bim.answer_query(\" \".join(tweet_keywords))\n",
        "#   mean_average_precision_hashtag_2 = mean_average_precision(20, u_base_hashtag, u_time, relevant_articles_list_2, docs_preprocessed_metrics)\n",
        "#   global_average_mean_average_precision_2.append(mean_average_precision_hashtag_2)\n",
        "#   mean_average_recall_hashtag_2 = mean_average_recall(20, u_base_hashtag, u_time, relevant_articles_list_2, docs_preprocessed_metrics)\n",
        "#   global_mean_average_recall_2.append(mean_average_recall_hashtag_2)"
      ],
      "metadata": {
        "id": "V9tAOGJoGY8s"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# overall_average_mean_average_precision_1 = sum(global_average_mean_average_precision_1)/len(global_average_mean_average_precision_1)\n",
        "# print(overall_average_mean_average_precision_1)\n",
        "\n",
        "# overall_mean_average_recall_1 = sum(global_mean_average_recall_1)/len(global_mean_average_recall_1)\n",
        "# print(overall_mean_average_recall_1)"
      ],
      "metadata": {
        "id": "0GxfFKviHra7"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# overall_average_mean_average_precision_2 = sum(global_average_mean_average_precision_2)/len(global_average_mean_average_precision_2)\n",
        "# print(overall_average_mean_average_precision_2)\n",
        "\n",
        "# overall_mean_average_recall_2 = sum(global_mean_average_recall_2)/len(global_mean_average_recall_2)\n",
        "# print(overall_mean_average_recall_2)"
      ],
      "metadata": {
        "id": "rNfhZmJSH0hU"
      },
      "execution_count": 24,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "final_results_binary_independence.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}