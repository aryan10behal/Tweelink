{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "final_results_soft_cosine_similarity.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Import Statements"
      ],
      "metadata": {
        "id": "Ts-fOnw4P9WO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install --upgrade gensim"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eNXARoJ_4SMG",
        "outputId": "8550e185-f0a9-485f-a9ff-20a4bbefb428"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.7/dist-packages (4.1.2)\n",
            "Requirement already satisfied: numpy>=1.17.0 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.21.6)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (5.2.1)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.4.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import csv\n",
        "import json\n",
        "from itertools import islice\n",
        "from collections import OrderedDict\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import os\n",
        "import nltk\n",
        "from glob import glob\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "from tqdm import tqdm\n",
        "import pickle\n",
        "import math\n",
        "import re\n",
        "from sklearn.model_selection import train_test_split\n",
        "import operator\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('corpus')\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from scipy import stats\n",
        "! pip install git+https://github.com/LIAAD/yake\n",
        "import yake\n",
        "stop_words = stopwords.words('english')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IBOv1bdeQBSj",
        "outputId": "22460861-f389-4381-dc1f-c1ae6c61c9ed"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Error loading corpus: Package 'corpus' not found in index\n",
            "Collecting git+https://github.com/LIAAD/yake\n",
            "  Cloning https://github.com/LIAAD/yake to /tmp/pip-req-build-vg7nl240\n",
            "  Running command git clone -q https://github.com/LIAAD/yake /tmp/pip-req-build-vg7nl240\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.7/dist-packages (from yake==0.4.8) (0.8.9)\n",
            "Requirement already satisfied: click>=6.0 in /usr/local/lib/python3.7/dist-packages (from yake==0.4.8) (7.1.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from yake==0.4.8) (1.21.6)\n",
            "Requirement already satisfied: segtok in /usr/local/lib/python3.7/dist-packages (from yake==0.4.8) (1.5.11)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.7/dist-packages (from yake==0.4.8) (2.6.3)\n",
            "Requirement already satisfied: jellyfish in /usr/local/lib/python3.7/dist-packages (from yake==0.4.8) (0.9.0)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from segtok->yake==0.4.8) (2019.12.20)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "import gensim\n",
        "from gensim.corpora import Dictionary\n",
        "from gensim.models import TfidfModel\n",
        "from gensim.similarities import SparseTermSimilarityMatrix, WordEmbeddingSimilarityIndex"
      ],
      "metadata": {
        "id": "brKpa7mn63K6"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import gensim.downloader as api\n",
        "# model = api.load('word2vec-google-news-300')"
      ],
      "metadata": {
        "id": "BAYn7jQO-eRj"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8TyRRoA3QJ-e",
        "outputId": "6190c6e8-80fc-4679-edeb-badf1e7b2ce3"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "file1 = open(\"/content/drive/MyDrive/Tweelink_Dataset/twitter_base_preprocessed.pkl\", \"rb\")\n",
        "df = pickle.load(file1)\n",
        "file1.close()"
      ],
      "metadata": {
        "id": "wZK40Dw9QLda"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 473
        },
        "id": "l18kFcfYTPnJ",
        "outputId": "c4eb1e65-c2f1-4307-808e-7cdacaf35129"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "               author id                 created_at geo country country_code  \\\n",
              "0           1952510090.0  2022-02-14 00:11:44+00:00                            \n",
              "1             20453105.0  2022-02-14 00:04:56+00:00                            \n",
              "2  1492660754998247424.0  2022-02-14 00:39:55+00:00                            \n",
              "3            117812637.0  2022-02-14 00:20:05+00:00                            \n",
              "4  1490044524604928000.0  2022-02-14 00:10:26+00:00                            \n",
              "\n",
              "  place_full_name place_name place_type                     id lang  ...  \\\n",
              "0                                        1493015060943454208.0   en  ...   \n",
              "1                                        1493013352938934272.0   en  ...   \n",
              "2                                        1493022155239534336.0   en  ...   \n",
              "3                                        1493017163485044736.0   en  ...   \n",
              "4                                        1493014734903382016.0   en  ...   \n",
              "\n",
              "                source                                              tweet  \\\n",
              "0  Twitter for Android  ð¥ð¥ð¥ð¥ð¥ð¥ð¥â¤µï¸â¤µï¸â¤µï¸...   \n",
              "1  Twitter for Android  It's too bad these guys are afraid of needles,...   \n",
              "2  Twitter for Android  Cowboy dressed as #Furries now available at th...   \n",
              "3   Twitter for iPhone  We blocked these trucks from entering the dntn...   \n",
              "4   Twitter for iPhone  Krista is very pleased with how the @RCMPONT r...   \n",
              "\n",
              "                                            hashtags sensitive  \\\n",
              "0  CommonSenseGunLaws,GunControlNow,GunSafes,GunS...     False   \n",
              "1          ClownConvoy,FreeDumbConvoy,OttawaOccupied     False   \n",
              "2                    Furries,RamRanch,OttawaOccupied     False   \n",
              "3                           Riverside,OttawaOccupied     False   \n",
              "4  FluTruxKlanGoHome,OttawaOccupied,kkkonvoy,Otta...     False   \n",
              "\n",
              "                      urls context_text context_probability context_type  \\\n",
              "0  https://t.co/LjkEup24Dk                              0.0                \n",
              "1  https://t.co/MmFzuFjIDR                              0.0                \n",
              "2  https://t.co/GBuBCUtpXe                              0.0                \n",
              "3  https://t.co/KzLkBZvjgD                              0.0                \n",
              "4  https://t.co/kAm5KNugdA                              0.0                \n",
              "\n",
              "                                   Preprocessed_Data   Date_Only  \n",
              "0  [ð¥ð¥ð¥ð¥ð¥ð¥ð¥â¤µï¸â¤µï¸â¤µï¸...  2022-02-14  \n",
              "1  [bad, guy, afraid, needle, twinrix, would, pre...  2022-02-14  \n",
              "2  [cowboy, dressed, furries, available, ramranch...  2022-02-14  \n",
              "3  [blocked, truck, entering, dntn, core, riversi...  2022-02-14  \n",
              "4  [krista, pleased, rcmpont, responded, maskless...  2022-02-14  \n",
              "\n",
              "[5 rows x 24 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-16322da3-083c-4174-8fc5-869b7e5e58a9\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>author id</th>\n",
              "      <th>created_at</th>\n",
              "      <th>geo</th>\n",
              "      <th>country</th>\n",
              "      <th>country_code</th>\n",
              "      <th>place_full_name</th>\n",
              "      <th>place_name</th>\n",
              "      <th>place_type</th>\n",
              "      <th>id</th>\n",
              "      <th>lang</th>\n",
              "      <th>...</th>\n",
              "      <th>source</th>\n",
              "      <th>tweet</th>\n",
              "      <th>hashtags</th>\n",
              "      <th>sensitive</th>\n",
              "      <th>urls</th>\n",
              "      <th>context_text</th>\n",
              "      <th>context_probability</th>\n",
              "      <th>context_type</th>\n",
              "      <th>Preprocessed_Data</th>\n",
              "      <th>Date_Only</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1952510090.0</td>\n",
              "      <td>2022-02-14 00:11:44+00:00</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>1493015060943454208.0</td>\n",
              "      <td>en</td>\n",
              "      <td>...</td>\n",
              "      <td>Twitter for Android</td>\n",
              "      <td>ð¥ð¥ð¥ð¥ð¥ð¥ð¥â¤µï¸â¤µï¸â¤µï¸...</td>\n",
              "      <td>CommonSenseGunLaws,GunControlNow,GunSafes,GunS...</td>\n",
              "      <td>False</td>\n",
              "      <td>https://t.co/LjkEup24Dk</td>\n",
              "      <td></td>\n",
              "      <td>0.0</td>\n",
              "      <td></td>\n",
              "      <td>[ð¥ð¥ð¥ð¥ð¥ð¥ð¥â¤µï¸â¤µï¸â¤µï¸...</td>\n",
              "      <td>2022-02-14</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>20453105.0</td>\n",
              "      <td>2022-02-14 00:04:56+00:00</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>1493013352938934272.0</td>\n",
              "      <td>en</td>\n",
              "      <td>...</td>\n",
              "      <td>Twitter for Android</td>\n",
              "      <td>It's too bad these guys are afraid of needles,...</td>\n",
              "      <td>ClownConvoy,FreeDumbConvoy,OttawaOccupied</td>\n",
              "      <td>False</td>\n",
              "      <td>https://t.co/MmFzuFjIDR</td>\n",
              "      <td></td>\n",
              "      <td>0.0</td>\n",
              "      <td></td>\n",
              "      <td>[bad, guy, afraid, needle, twinrix, would, pre...</td>\n",
              "      <td>2022-02-14</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1492660754998247424.0</td>\n",
              "      <td>2022-02-14 00:39:55+00:00</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>1493022155239534336.0</td>\n",
              "      <td>en</td>\n",
              "      <td>...</td>\n",
              "      <td>Twitter for Android</td>\n",
              "      <td>Cowboy dressed as #Furries now available at th...</td>\n",
              "      <td>Furries,RamRanch,OttawaOccupied</td>\n",
              "      <td>False</td>\n",
              "      <td>https://t.co/GBuBCUtpXe</td>\n",
              "      <td></td>\n",
              "      <td>0.0</td>\n",
              "      <td></td>\n",
              "      <td>[cowboy, dressed, furries, available, ramranch...</td>\n",
              "      <td>2022-02-14</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>117812637.0</td>\n",
              "      <td>2022-02-14 00:20:05+00:00</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>1493017163485044736.0</td>\n",
              "      <td>en</td>\n",
              "      <td>...</td>\n",
              "      <td>Twitter for iPhone</td>\n",
              "      <td>We blocked these trucks from entering the dntn...</td>\n",
              "      <td>Riverside,OttawaOccupied</td>\n",
              "      <td>False</td>\n",
              "      <td>https://t.co/KzLkBZvjgD</td>\n",
              "      <td></td>\n",
              "      <td>0.0</td>\n",
              "      <td></td>\n",
              "      <td>[blocked, truck, entering, dntn, core, riversi...</td>\n",
              "      <td>2022-02-14</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1490044524604928000.0</td>\n",
              "      <td>2022-02-14 00:10:26+00:00</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>1493014734903382016.0</td>\n",
              "      <td>en</td>\n",
              "      <td>...</td>\n",
              "      <td>Twitter for iPhone</td>\n",
              "      <td>Krista is very pleased with how the @RCMPONT r...</td>\n",
              "      <td>FluTruxKlanGoHome,OttawaOccupied,kkkonvoy,Otta...</td>\n",
              "      <td>False</td>\n",
              "      <td>https://t.co/kAm5KNugdA</td>\n",
              "      <td></td>\n",
              "      <td>0.0</td>\n",
              "      <td></td>\n",
              "      <td>[krista, pleased, rcmpont, responded, maskless...</td>\n",
              "      <td>2022-02-14</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 24 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-16322da3-083c-4174-8fc5-869b7e5e58a9')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-16322da3-083c-4174-8fc5-869b7e5e58a9 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-16322da3-083c-4174-8fc5-869b7e5e58a9');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "u_base_hashtag = input(\"Enter base hashtag: \")\n",
        "u_time = input(\"Enter time: \")\n",
        "u_location = input(\"Enter Location: \")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0nvReo2xTQIG",
        "outputId": "ddae4610-daac-4c7d-8f9c-9c470e32b7b4"
      },
      "execution_count": 42,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter base hashtag: hijab\n",
            "Enter time: 2022-02-19\n",
            "Enter Location: india\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import datetime\n",
        "tweet_query = []\n",
        "format = '%Y-%m-%d'\n",
        "u_present_date = datetime.datetime.strptime(u_time, format)\n",
        "u_prev_date = u_present_date - datetime.timedelta(days=1)\n",
        "u_next_date = u_present_date + datetime.timedelta(days=1)\n",
        "df_query = df.loc[df['hashtags'].str.contains(u_base_hashtag) & df['Date_Only'].isin([str(u_present_date.date()), str(u_prev_date.date()), str(u_next_date.date())])]"
      ],
      "metadata": {
        "id": "Ih0NQpCHTVVR"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_query.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 577
        },
        "id": "JPM7VJaBTf4b",
        "outputId": "7e81685f-184e-4c52-be13-ca43381355cc"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                 author id                 created_at               geo  \\\n",
              "12546          123851279.0  2022-02-19 12:59:42+00:00                     \n",
              "8192            3187630801  2022-02-20 00:55:03+00:00                     \n",
              "8198            2443976946  2022-02-20 00:14:25+00:00                     \n",
              "8278   1483569403425959939  2022-02-20 01:06:11+00:00  18810aa5b43e76c7   \n",
              "8280              18931596  2022-02-20 01:55:42+00:00                     \n",
              "\n",
              "                country country_code place_full_name place_name place_type  \\\n",
              "12546                                                                        \n",
              "8192                                                                         \n",
              "8198                                                                         \n",
              "8278   Verenigde Staten           US      Dallas, TX     Dallas       city   \n",
              "8280                                                                         \n",
              "\n",
              "                          id lang  ...               source  \\\n",
              "12546  1495020266862174208.0   en  ...   Twitter for iPhone   \n",
              "8192     1495200290210910209   en  ...  Twitter for Android   \n",
              "8198     1495190066112651265   en  ...      Twitter Web App   \n",
              "8278     1495203094224904192   en  ...   Twitter for iPhone   \n",
              "8280     1495215554508075011   en  ...      Twitter Web App   \n",
              "\n",
              "                                                   tweet  \\\n",
              "12546  They are so desperate they have let loose #Yat...   \n",
              "8192   No Woman is Forced to wear and Not to wear Som...   \n",
              "8198   Protests in US cities against #Karnataka #hija...   \n",
              "8278   Protest by local Dallas Muslims at JFK square ...   \n",
              "8280   Feel sorry for #zahirawasim.  Religion must ma...   \n",
              "\n",
              "                                                hashtags sensitive  \\\n",
              "12546             YatiNarsinghanand,PlotToKillModi,hijab     False   \n",
              "8192   hijab,HijabIsOurPride,HijabIsFundamentalRight,...     False   \n",
              "8198                    Karnataka,hijab,Islamophobic,BJP     False   \n",
              "8278                                               hijab     False   \n",
              "8280                                   zahirawasim,hijab     False   \n",
              "\n",
              "                                                    urls context_text  \\\n",
              "12546                                                                   \n",
              "8192                                                                    \n",
              "8198   https://t.co/957kBM8AJT,https://t.co/5gY741DZP...                \n",
              "8278                             https://t.co/KCm413Yb8N                \n",
              "8280                                                                    \n",
              "\n",
              "      context_probability context_type  \\\n",
              "12546                 0.0                \n",
              "8192                  0.0                \n",
              "8198                  0.0                \n",
              "8278                  0.0                \n",
              "8280                  0.0                \n",
              "\n",
              "                                       Preprocessed_Data   Date_Only  \n",
              "12546  [desperate, let, loose, yatinarsinghanand, vio...  2022-02-19  \n",
              "8192   [woman, forced, wear, wear, something, hijab, ...  2022-02-20  \n",
              "8198   [protest, u, city, karnataka, hijab, ban, indi...  2022-02-20  \n",
              "8278   [protest, local, dallas, muslim, jfk, square, ...  2022-02-20  \n",
              "8280   [feel, sorry, zahirawasim, religion, must, mak...  2022-02-20  \n",
              "\n",
              "[5 rows x 24 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-9e62252d-e224-4902-816b-7a9dd0cd30a8\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>author id</th>\n",
              "      <th>created_at</th>\n",
              "      <th>geo</th>\n",
              "      <th>country</th>\n",
              "      <th>country_code</th>\n",
              "      <th>place_full_name</th>\n",
              "      <th>place_name</th>\n",
              "      <th>place_type</th>\n",
              "      <th>id</th>\n",
              "      <th>lang</th>\n",
              "      <th>...</th>\n",
              "      <th>source</th>\n",
              "      <th>tweet</th>\n",
              "      <th>hashtags</th>\n",
              "      <th>sensitive</th>\n",
              "      <th>urls</th>\n",
              "      <th>context_text</th>\n",
              "      <th>context_probability</th>\n",
              "      <th>context_type</th>\n",
              "      <th>Preprocessed_Data</th>\n",
              "      <th>Date_Only</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>12546</th>\n",
              "      <td>123851279.0</td>\n",
              "      <td>2022-02-19 12:59:42+00:00</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>1495020266862174208.0</td>\n",
              "      <td>en</td>\n",
              "      <td>...</td>\n",
              "      <td>Twitter for iPhone</td>\n",
              "      <td>They are so desperate they have let loose #Yat...</td>\n",
              "      <td>YatiNarsinghanand,PlotToKillModi,hijab</td>\n",
              "      <td>False</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>0.0</td>\n",
              "      <td></td>\n",
              "      <td>[desperate, let, loose, yatinarsinghanand, vio...</td>\n",
              "      <td>2022-02-19</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8192</th>\n",
              "      <td>3187630801</td>\n",
              "      <td>2022-02-20 00:55:03+00:00</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>1495200290210910209</td>\n",
              "      <td>en</td>\n",
              "      <td>...</td>\n",
              "      <td>Twitter for Android</td>\n",
              "      <td>No Woman is Forced to wear and Not to wear Som...</td>\n",
              "      <td>hijab,HijabIsOurPride,HijabIsFundamentalRight,...</td>\n",
              "      <td>False</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>0.0</td>\n",
              "      <td></td>\n",
              "      <td>[woman, forced, wear, wear, something, hijab, ...</td>\n",
              "      <td>2022-02-20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8198</th>\n",
              "      <td>2443976946</td>\n",
              "      <td>2022-02-20 00:14:25+00:00</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>1495190066112651265</td>\n",
              "      <td>en</td>\n",
              "      <td>...</td>\n",
              "      <td>Twitter Web App</td>\n",
              "      <td>Protests in US cities against #Karnataka #hija...</td>\n",
              "      <td>Karnataka,hijab,Islamophobic,BJP</td>\n",
              "      <td>False</td>\n",
              "      <td>https://t.co/957kBM8AJT,https://t.co/5gY741DZP...</td>\n",
              "      <td></td>\n",
              "      <td>0.0</td>\n",
              "      <td></td>\n",
              "      <td>[protest, u, city, karnataka, hijab, ban, indi...</td>\n",
              "      <td>2022-02-20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8278</th>\n",
              "      <td>1483569403425959939</td>\n",
              "      <td>2022-02-20 01:06:11+00:00</td>\n",
              "      <td>18810aa5b43e76c7</td>\n",
              "      <td>Verenigde Staten</td>\n",
              "      <td>US</td>\n",
              "      <td>Dallas, TX</td>\n",
              "      <td>Dallas</td>\n",
              "      <td>city</td>\n",
              "      <td>1495203094224904192</td>\n",
              "      <td>en</td>\n",
              "      <td>...</td>\n",
              "      <td>Twitter for iPhone</td>\n",
              "      <td>Protest by local Dallas Muslims at JFK square ...</td>\n",
              "      <td>hijab</td>\n",
              "      <td>False</td>\n",
              "      <td>https://t.co/KCm413Yb8N</td>\n",
              "      <td></td>\n",
              "      <td>0.0</td>\n",
              "      <td></td>\n",
              "      <td>[protest, local, dallas, muslim, jfk, square, ...</td>\n",
              "      <td>2022-02-20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8280</th>\n",
              "      <td>18931596</td>\n",
              "      <td>2022-02-20 01:55:42+00:00</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>1495215554508075011</td>\n",
              "      <td>en</td>\n",
              "      <td>...</td>\n",
              "      <td>Twitter Web App</td>\n",
              "      <td>Feel sorry for #zahirawasim.  Religion must ma...</td>\n",
              "      <td>zahirawasim,hijab</td>\n",
              "      <td>False</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>0.0</td>\n",
              "      <td></td>\n",
              "      <td>[feel, sorry, zahirawasim, religion, must, mak...</td>\n",
              "      <td>2022-02-20</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 24 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9e62252d-e224-4902-816b-7a9dd0cd30a8')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-9e62252d-e224-4902-816b-7a9dd0cd30a8 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-9e62252d-e224-4902-816b-7a9dd0cd30a8');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_query"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 818
        },
        "id": "5w_SRasu2bfA",
        "outputId": "3604826f-8782-4b11-f342-d5dceeb5ce49"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                   author id                 created_at               geo  \\\n",
              "12546            123851279.0  2022-02-19 12:59:42+00:00                     \n",
              "8192              3187630801  2022-02-20 00:55:03+00:00                     \n",
              "8198              2443976946  2022-02-20 00:14:25+00:00                     \n",
              "8278     1483569403425959939  2022-02-20 01:06:11+00:00  18810aa5b43e76c7   \n",
              "8280                18931596  2022-02-20 01:55:42+00:00                     \n",
              "...                      ...                        ...               ...   \n",
              "11300             3022076697  2022-02-20 23:02:11+00:00                     \n",
              "11306    1407681110511964160  2022-02-20 23:36:07+00:00                     \n",
              "18929  1463168868063399936.0  2022-02-18 06:39:52+00:00                     \n",
              "23847            158374641.0  2022-02-19 13:46:22+00:00                     \n",
              "29728             50259839.0  2022-02-19 03:00:40+00:00                     \n",
              "\n",
              "                country country_code place_full_name place_name place_type  \\\n",
              "12546                                                                        \n",
              "8192                                                                         \n",
              "8198                                                                         \n",
              "8278   Verenigde Staten           US      Dallas, TX     Dallas       city   \n",
              "8280                                                                         \n",
              "...                 ...          ...             ...        ...        ...   \n",
              "11300                                                                        \n",
              "11306                                                                        \n",
              "18929                                                                        \n",
              "23847                                                                        \n",
              "29728                                                                        \n",
              "\n",
              "                          id lang  ...               source  \\\n",
              "12546  1495020266862174208.0   en  ...   Twitter for iPhone   \n",
              "8192     1495200290210910209   en  ...  Twitter for Android   \n",
              "8198     1495190066112651265   en  ...      Twitter Web App   \n",
              "8278     1495203094224904192   en  ...   Twitter for iPhone   \n",
              "8280     1495215554508075011   en  ...      Twitter Web App   \n",
              "...                      ...  ...  ...                  ...   \n",
              "11300    1495534273063006214   en  ...          Post Studio   \n",
              "11306    1495542813240565761   en  ...   Twitter for iPhone   \n",
              "18929  1494562291458756608.0   en  ...   Twitter for iPhone   \n",
              "23847  1495032011148660736.0   en  ...      Twitter Web App   \n",
              "29728  1494869517549604864.0   en  ...  Twitter for Android   \n",
              "\n",
              "                                                   tweet  \\\n",
              "12546  They are so desperate they have let loose #Yat...   \n",
              "8192   No Woman is Forced to wear and Not to wear Som...   \n",
              "8198   Protests in US cities against #Karnataka #hija...   \n",
              "8278   Protest by local Dallas Muslims at JFK square ...   \n",
              "8280   Feel sorry for #zahirawasim.  Religion must ma...   \n",
              "...                                                  ...   \n",
              "11300  Looking for an Modest dress for your upcoming ...   \n",
              "11306  Weak Hindus worship Satan, do not have manline...   \n",
              "18929  ðð¿ð²ð®ð ð±ð²ð°ð¶ðð¶ð...   \n",
              "23847  @RajeevRai https://t.co/vIHQ6mIJOJ #abuazmi co...   \n",
              "29728  India (minus and BJPee supporters), I hope tha...   \n",
              "\n",
              "                                                hashtags sensitive  \\\n",
              "12546             YatiNarsinghanand,PlotToKillModi,hijab     False   \n",
              "8192   hijab,HijabIsOurPride,HijabIsFundamentalRight,...     False   \n",
              "8198                    Karnataka,hijab,Islamophobic,BJP     False   \n",
              "8278                                               hijab     False   \n",
              "8280                                   zahirawasim,hijab     False   \n",
              "...                                                  ...       ...   \n",
              "11300                   modestfashion,hijab,abaya,muslim     False   \n",
              "11306                                 India,hijab,Hindus     False   \n",
              "18929  hijab,hijabday,PopularFrontDay,Muslim,Markhor,...     False   \n",
              "23847  abuazmi,Ahmedabadblast2008,AkhileshYadav,Samaj...     False   \n",
              "29728  SabYaadRakkhaJayega,Election2022,PunjabElectio...     False   \n",
              "\n",
              "                                                    urls context_text  \\\n",
              "12546                                                                   \n",
              "8192                                                                    \n",
              "8198   https://t.co/957kBM8AJT,https://t.co/5gY741DZP...                \n",
              "8278                             https://t.co/KCm413Yb8N                \n",
              "8280                                                                    \n",
              "...                                                  ...          ...   \n",
              "11300    https://t.co/8p49EqGJ5z,https://t.co/dCiENpIuaX                \n",
              "11306                                                                   \n",
              "18929                                                                   \n",
              "23847                            https://t.co/vIHQ6mIJOJ                \n",
              "29728                            https://t.co/RK1w7s9kOP                \n",
              "\n",
              "      context_probability context_type  \\\n",
              "12546                 0.0                \n",
              "8192                  0.0                \n",
              "8198                  0.0                \n",
              "8278                  0.0                \n",
              "8280                  0.0                \n",
              "...                   ...          ...   \n",
              "11300                 0.0                \n",
              "11306                 0.0                \n",
              "18929                 0.0                \n",
              "23847                 0.0                \n",
              "29728                 0.0                \n",
              "\n",
              "                                       Preprocessed_Data   Date_Only  \n",
              "12546  [desperate, let, loose, yatinarsinghanand, vio...  2022-02-19  \n",
              "8192   [woman, forced, wear, wear, something, hijab, ...  2022-02-20  \n",
              "8198   [protest, u, city, karnataka, hijab, ban, indi...  2022-02-20  \n",
              "8278   [protest, local, dallas, muslim, jfk, square, ...  2022-02-20  \n",
              "8280   [feel, sorry, zahirawasim, religion, must, mak...  2022-02-20  \n",
              "...                                                  ...         ...  \n",
              "11300  [looking, modest, dress, upcoming, event, ð...  2022-02-20  \n",
              "11306  [weak, hindu, worship, satan, manliness, coura...  2022-02-20  \n",
              "18929  [ðð¿ð²ð®ð, ð±ð²ð°ð¶ðð...  2022-02-18  \n",
              "23847  [rajeevrai, http, tcovihq6mijoj, abuazmi, conf...  2022-02-19  \n",
              "29728  [india, minus, bjpee, supporter, hope, yeh, sa...  2022-02-19  \n",
              "\n",
              "[264 rows x 24 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-9df1f443-fd6f-4f4b-a7c1-11e7d40d569b\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>author id</th>\n",
              "      <th>created_at</th>\n",
              "      <th>geo</th>\n",
              "      <th>country</th>\n",
              "      <th>country_code</th>\n",
              "      <th>place_full_name</th>\n",
              "      <th>place_name</th>\n",
              "      <th>place_type</th>\n",
              "      <th>id</th>\n",
              "      <th>lang</th>\n",
              "      <th>...</th>\n",
              "      <th>source</th>\n",
              "      <th>tweet</th>\n",
              "      <th>hashtags</th>\n",
              "      <th>sensitive</th>\n",
              "      <th>urls</th>\n",
              "      <th>context_text</th>\n",
              "      <th>context_probability</th>\n",
              "      <th>context_type</th>\n",
              "      <th>Preprocessed_Data</th>\n",
              "      <th>Date_Only</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>12546</th>\n",
              "      <td>123851279.0</td>\n",
              "      <td>2022-02-19 12:59:42+00:00</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>1495020266862174208.0</td>\n",
              "      <td>en</td>\n",
              "      <td>...</td>\n",
              "      <td>Twitter for iPhone</td>\n",
              "      <td>They are so desperate they have let loose #Yat...</td>\n",
              "      <td>YatiNarsinghanand,PlotToKillModi,hijab</td>\n",
              "      <td>False</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>0.0</td>\n",
              "      <td></td>\n",
              "      <td>[desperate, let, loose, yatinarsinghanand, vio...</td>\n",
              "      <td>2022-02-19</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8192</th>\n",
              "      <td>3187630801</td>\n",
              "      <td>2022-02-20 00:55:03+00:00</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>1495200290210910209</td>\n",
              "      <td>en</td>\n",
              "      <td>...</td>\n",
              "      <td>Twitter for Android</td>\n",
              "      <td>No Woman is Forced to wear and Not to wear Som...</td>\n",
              "      <td>hijab,HijabIsOurPride,HijabIsFundamentalRight,...</td>\n",
              "      <td>False</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>0.0</td>\n",
              "      <td></td>\n",
              "      <td>[woman, forced, wear, wear, something, hijab, ...</td>\n",
              "      <td>2022-02-20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8198</th>\n",
              "      <td>2443976946</td>\n",
              "      <td>2022-02-20 00:14:25+00:00</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>1495190066112651265</td>\n",
              "      <td>en</td>\n",
              "      <td>...</td>\n",
              "      <td>Twitter Web App</td>\n",
              "      <td>Protests in US cities against #Karnataka #hija...</td>\n",
              "      <td>Karnataka,hijab,Islamophobic,BJP</td>\n",
              "      <td>False</td>\n",
              "      <td>https://t.co/957kBM8AJT,https://t.co/5gY741DZP...</td>\n",
              "      <td></td>\n",
              "      <td>0.0</td>\n",
              "      <td></td>\n",
              "      <td>[protest, u, city, karnataka, hijab, ban, indi...</td>\n",
              "      <td>2022-02-20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8278</th>\n",
              "      <td>1483569403425959939</td>\n",
              "      <td>2022-02-20 01:06:11+00:00</td>\n",
              "      <td>18810aa5b43e76c7</td>\n",
              "      <td>Verenigde Staten</td>\n",
              "      <td>US</td>\n",
              "      <td>Dallas, TX</td>\n",
              "      <td>Dallas</td>\n",
              "      <td>city</td>\n",
              "      <td>1495203094224904192</td>\n",
              "      <td>en</td>\n",
              "      <td>...</td>\n",
              "      <td>Twitter for iPhone</td>\n",
              "      <td>Protest by local Dallas Muslims at JFK square ...</td>\n",
              "      <td>hijab</td>\n",
              "      <td>False</td>\n",
              "      <td>https://t.co/KCm413Yb8N</td>\n",
              "      <td></td>\n",
              "      <td>0.0</td>\n",
              "      <td></td>\n",
              "      <td>[protest, local, dallas, muslim, jfk, square, ...</td>\n",
              "      <td>2022-02-20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8280</th>\n",
              "      <td>18931596</td>\n",
              "      <td>2022-02-20 01:55:42+00:00</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>1495215554508075011</td>\n",
              "      <td>en</td>\n",
              "      <td>...</td>\n",
              "      <td>Twitter Web App</td>\n",
              "      <td>Feel sorry for #zahirawasim.  Religion must ma...</td>\n",
              "      <td>zahirawasim,hijab</td>\n",
              "      <td>False</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>0.0</td>\n",
              "      <td></td>\n",
              "      <td>[feel, sorry, zahirawasim, religion, must, mak...</td>\n",
              "      <td>2022-02-20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11300</th>\n",
              "      <td>3022076697</td>\n",
              "      <td>2022-02-20 23:02:11+00:00</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>1495534273063006214</td>\n",
              "      <td>en</td>\n",
              "      <td>...</td>\n",
              "      <td>Post Studio</td>\n",
              "      <td>Looking for an Modest dress for your upcoming ...</td>\n",
              "      <td>modestfashion,hijab,abaya,muslim</td>\n",
              "      <td>False</td>\n",
              "      <td>https://t.co/8p49EqGJ5z,https://t.co/dCiENpIuaX</td>\n",
              "      <td></td>\n",
              "      <td>0.0</td>\n",
              "      <td></td>\n",
              "      <td>[looking, modest, dress, upcoming, event, ð...</td>\n",
              "      <td>2022-02-20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11306</th>\n",
              "      <td>1407681110511964160</td>\n",
              "      <td>2022-02-20 23:36:07+00:00</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>1495542813240565761</td>\n",
              "      <td>en</td>\n",
              "      <td>...</td>\n",
              "      <td>Twitter for iPhone</td>\n",
              "      <td>Weak Hindus worship Satan, do not have manline...</td>\n",
              "      <td>India,hijab,Hindus</td>\n",
              "      <td>False</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>0.0</td>\n",
              "      <td></td>\n",
              "      <td>[weak, hindu, worship, satan, manliness, coura...</td>\n",
              "      <td>2022-02-20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18929</th>\n",
              "      <td>1463168868063399936.0</td>\n",
              "      <td>2022-02-18 06:39:52+00:00</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>1494562291458756608.0</td>\n",
              "      <td>en</td>\n",
              "      <td>...</td>\n",
              "      <td>Twitter for iPhone</td>\n",
              "      <td>ðð¿ð²ð®ð ð±ð²ð°ð¶ðð¶ð...</td>\n",
              "      <td>hijab,hijabday,PopularFrontDay,Muslim,Markhor,...</td>\n",
              "      <td>False</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>0.0</td>\n",
              "      <td></td>\n",
              "      <td>[ðð¿ð²ð®ð, ð±ð²ð°ð¶ðð...</td>\n",
              "      <td>2022-02-18</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23847</th>\n",
              "      <td>158374641.0</td>\n",
              "      <td>2022-02-19 13:46:22+00:00</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>1495032011148660736.0</td>\n",
              "      <td>en</td>\n",
              "      <td>...</td>\n",
              "      <td>Twitter Web App</td>\n",
              "      <td>@RajeevRai https://t.co/vIHQ6mIJOJ #abuazmi co...</td>\n",
              "      <td>abuazmi,Ahmedabadblast2008,AkhileshYadav,Samaj...</td>\n",
              "      <td>False</td>\n",
              "      <td>https://t.co/vIHQ6mIJOJ</td>\n",
              "      <td></td>\n",
              "      <td>0.0</td>\n",
              "      <td></td>\n",
              "      <td>[rajeevrai, http, tcovihq6mijoj, abuazmi, conf...</td>\n",
              "      <td>2022-02-19</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29728</th>\n",
              "      <td>50259839.0</td>\n",
              "      <td>2022-02-19 03:00:40+00:00</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>1494869517549604864.0</td>\n",
              "      <td>en</td>\n",
              "      <td>...</td>\n",
              "      <td>Twitter for Android</td>\n",
              "      <td>India (minus and BJPee supporters), I hope tha...</td>\n",
              "      <td>SabYaadRakkhaJayega,Election2022,PunjabElectio...</td>\n",
              "      <td>False</td>\n",
              "      <td>https://t.co/RK1w7s9kOP</td>\n",
              "      <td></td>\n",
              "      <td>0.0</td>\n",
              "      <td></td>\n",
              "      <td>[india, minus, bjpee, supporter, hope, yeh, sa...</td>\n",
              "      <td>2022-02-19</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>264 rows × 24 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9df1f443-fd6f-4f4b-a7c1-11e7d40d569b')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-9df1f443-fd6f-4f4b-a7c1-11e7d40d569b button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-9df1f443-fd6f-4f4b-a7c1-11e7d40d569b');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def keyword_extractor(dataset):\n",
        "  preprocessed_vocabulary = dict()\n",
        "\n",
        "  #Converting to lowercase\n",
        "  def to_lower_case(text):\n",
        "    text = text.lower()\n",
        "    return text\n",
        "\n",
        "  def remove_at_word(text):\n",
        "    data = text.split()\n",
        "    data = [d for d in data if d[0]!='@']\n",
        "    text = ' '.join(data)\n",
        "    return text\n",
        "\n",
        "  def remove_hashtag(text):\n",
        "    data = text.split()\n",
        "    data = [d if (d[0]!='#' or len(d) == 1) else d[1:] for d in data]\n",
        "    data = [d for d in data if d[0]!='#']\n",
        "    text = ' '.join(data)\n",
        "    return text\n",
        "\n",
        "  def remove_URL(text):\n",
        "    text = re.sub(r\"http\\S+\", \"\", text)\n",
        "    text = re.sub(r'bit.ly\\S+', '', text, flags=re.MULTILINE)\n",
        "    return text\n",
        "\n",
        "  #Removing stopwords\n",
        "  def remove_stopwords(text):\n",
        "    stopword = stopwords.words('english')\n",
        "    new_list = [x for x in text.split() if x not in stopword]\n",
        "    return ' '.join(new_list)\n",
        "\n",
        "  #Removing punctuations\n",
        "  def remove_punctuations(text):\n",
        "    punctuations = '''!()-[|]`{};:'\"\\,<>./?@#$=+%^&*_~'''\n",
        "    new_list = ['' if x in punctuations else x for x in text.split()]\n",
        "    new_list_final = []\n",
        "    for token in new_list:\n",
        "      new_token=\"\"\n",
        "      for char in token:\n",
        "        if(char not in punctuations):\n",
        "          new_token+=char\n",
        "      if(len(new_token)!=0):\n",
        "        new_list_final.append(new_token)\n",
        "    return ' '.join(new_list_final)\n",
        "\n",
        "  #Tokenization\n",
        "  def tokenization(text):\n",
        "    return word_tokenize(text)\n",
        "\n",
        "  def pre_process(text):\n",
        "    text = to_lower_case(text)\n",
        "    text = remove_at_word(text)\n",
        "    text = remove_hashtag(text)\n",
        "    text = remove_URL(text)\n",
        "    text = remove_stopwords(text)\n",
        "    text = remove_punctuations(text)\n",
        "    text = tokenization(text)\n",
        "    for token in text:\n",
        "      if token in preprocessed_vocabulary.keys():\n",
        "        preprocessed_vocabulary[token] += 1\n",
        "      else:\n",
        "        preprocessed_vocabulary[token] = 1\n",
        "    return text\n",
        "  \n",
        "  preprocessed_data = [pre_process(text) for text in dataset]\n",
        "\n",
        "  #print(preprocessed_vocabulary)\n",
        "\n",
        "  AOF_coefficient = sum(preprocessed_vocabulary.values())/len(preprocessed_vocabulary)\n",
        "  vocabulary = {token.strip():preprocessed_vocabulary[token] for token in preprocessed_vocabulary.keys() if preprocessed_vocabulary[token] > AOF_coefficient and len(token.strip())}\n",
        "\n",
        "  #print(vocabulary)\n",
        "\n",
        "  final_tokens_per_tweet = []\n",
        "  for data in preprocessed_data:\n",
        "    final_tokens_per_tweet.append([token for token in data if token in vocabulary.keys()])\n",
        "\n",
        "  #print(preprocessed_data)\n",
        "  #print(final_tokens_per_tweet)\n",
        "\n",
        "  word2id = dict()\n",
        "  id2word = dict()\n",
        "  vocabulary_size = len(vocabulary)\n",
        "  count = 0\n",
        "  for token in vocabulary.keys():\n",
        "    word2id[token] = count\n",
        "    id2word[count] = token\n",
        "    count += 1\n",
        "\n",
        "  #print(word2id)\n",
        "  #print(id2word)\n",
        "\n",
        "  directed_graph_adjacency_matrix = np.zeros((vocabulary_size, vocabulary_size))\n",
        "  edge_weight_matrix = np.zeros((vocabulary_size, vocabulary_size))\n",
        "  first_frequency = dict()\n",
        "  last_frequency = dict()\n",
        "  term_frequency = vocabulary\n",
        "  strength = dict()\n",
        "  degree = dict()\n",
        "  selective_centraility = dict()\n",
        "\n",
        "\n",
        "  for tweet in final_tokens_per_tweet:\n",
        "    if len(tweet) == 0:\n",
        "      continue\n",
        "    if tweet[0] in first_frequency.keys():\n",
        "      first_frequency[tweet[0]] += 1\n",
        "    else:\n",
        "      first_frequency[tweet[0]] = 1\n",
        "\n",
        "    if tweet[-1] in last_frequency.keys():\n",
        "      last_frequency[tweet[-1]] += 1\n",
        "    else:\n",
        "      last_frequency[tweet[-1]] = 1\n",
        "    \n",
        "\n",
        "\n",
        "    for i in range(len(tweet)-1):\n",
        "      if tweet[i] == tweet[i+1]:\n",
        "        continue\n",
        "      x = word2id[tweet[i]]\n",
        "      y = word2id[tweet[i+1]]\n",
        "      directed_graph_adjacency_matrix[x][y] += 1\n",
        "\n",
        "  for tweet in final_tokens_per_tweet:\n",
        "    for i in range(len(tweet)-1):\n",
        "\n",
        "      if tweet[i] == tweet[i+1]:\n",
        "        continue\n",
        "      x = word2id[tweet[i]]\n",
        "      y = word2id[tweet[i+1]]\n",
        "\n",
        "    # Updating degree..\n",
        "      if tweet[i] in degree.keys():\n",
        "        degree[tweet[i]] += 1\n",
        "      else:\n",
        "        degree[tweet[i]] = 1\n",
        "        \n",
        "      if tweet[i+1] in degree.keys():\n",
        "        degree[tweet[i+1]] += 1\n",
        "      else:\n",
        "        degree[tweet[i+1]] = 1\n",
        "\n",
        "      edge_weight_matrix[x][y] = directed_graph_adjacency_matrix[x][y]/(vocabulary[tweet[i]] + vocabulary[tweet[i+1]] - directed_graph_adjacency_matrix[x][y])\n",
        "\n",
        "      if tweet[i] in strength.keys():\n",
        "        strength[tweet[i]] += edge_weight_matrix[x][y]\n",
        "      else:\n",
        "        strength[tweet[i]] = edge_weight_matrix[x][y]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  first_frequency = {token:(first_frequency[token]/vocabulary[token] if token in first_frequency else 0) for token in vocabulary.keys()}\n",
        "  last_frequency = {token:(last_frequency[token]/vocabulary[token] if token in last_frequency else 0) for token in vocabulary.keys()}\n",
        "  degree = {token:(degree[token] if token in degree else 0) for token in vocabulary.keys()}\n",
        "  strength = {token:(strength[token] if token in strength else 0) for token in vocabulary.keys()}\n",
        "  selective_centraility = {token:(strength[token]/degree[token] if degree[token]!=0 else 0) for token in vocabulary.keys()}\n",
        "\n",
        "  #print(degree)\n",
        "  #print(vocabulary)\n",
        "\n",
        "  maxdegree = max(degree.items(), key=lambda x: x[1])[1]\n",
        "  max_degree_nodes_with_freq = {key:term_frequency[key] for key in degree.keys() if degree[key] == maxdegree}\n",
        "  maxfreq = max(max_degree_nodes_with_freq.items(), key=lambda x: x[1])[1]\n",
        "  central_node_name = [key for key in max_degree_nodes_with_freq.keys() if max_degree_nodes_with_freq[key] == maxfreq][0]\n",
        "  #print(\"central node: \", central_node_name)\n",
        "\n",
        "  # bfs\n",
        "  distance_from_central_node = dict()\n",
        "  central_node_id = word2id[central_node_name]\n",
        "  q = [(central_node_id, 0)]\n",
        "\n",
        "  # Set source as visited\n",
        "  distance_from_central_node[central_node_name] = 0\n",
        "\n",
        "  while q:\n",
        "      vis = q[0]\n",
        "      # Print current node\n",
        "      #print(id2word[vis[0]], vis[1])\n",
        "      q.pop(0)\n",
        "        \n",
        "      # For every adjacent vertex to\n",
        "      # the current vertex\n",
        "      for i in range(len(directed_graph_adjacency_matrix[vis[0]])):\n",
        "          if (directed_graph_adjacency_matrix[vis[0]][i] == 1 and (id2word[i] not in distance_from_central_node.keys())):\n",
        "              # Push the adjacent node\n",
        "              # in the queue\n",
        "              q.append((i, vis[1]+1))\n",
        "              distance_from_central_node[id2word[i]] = vis[1]+1\n",
        "\n",
        "  #print(distance_from_central_node)\n",
        "  inverse_distance_from_central_node = {token:(1/distance_from_central_node[token] if token in distance_from_central_node and token != central_node_name else 0) for token in vocabulary.keys()}\n",
        "  inverse_distance_from_central_node[central_node_name] = 1.0\n",
        "  #print(inverse_distance_from_central_node)\n",
        "\n",
        "  neighbour_importance = dict()\n",
        "\n",
        "  for i in range(len(directed_graph_adjacency_matrix)):\n",
        "    neighbours = set()\n",
        "\n",
        "    # traversing outgoing edges\n",
        "    for j in range(len(directed_graph_adjacency_matrix)):\n",
        "      if i == j:\n",
        "        continue\n",
        "      if directed_graph_adjacency_matrix[i][j] > 0:\n",
        "        neighbours.add(j)\n",
        "    for j in range(len(directed_graph_adjacency_matrix)):\n",
        "      if i == j:\n",
        "        continue\n",
        "      if directed_graph_adjacency_matrix[j][i] > 0:\n",
        "          neighbours.add(j)\n",
        "    if len(neighbours) != 0:\n",
        "      neighbour_importance[id2word[i]] = sum([strength[id2word[j]] for j in neighbours])/len(neighbours)\n",
        "    else:\n",
        "      neighbour_importance[id2word[i]] = 0\n",
        "      \n",
        "  #print(neighbour_importance)\n",
        "\n",
        "  unnormalized_node_weight = {node: (first_frequency[node] + last_frequency[node] + term_frequency[node] + selective_centraility[node] + inverse_distance_from_central_node[node] + neighbour_importance[node]) for node in vocabulary.keys()}\n",
        "  max_node_weight = max(unnormalized_node_weight.items(), key=lambda x: x[1])[1]\n",
        "  min_node_weight = min(unnormalized_node_weight.items(), key=lambda x: x[1])[1]\n",
        "  #print(\"max node weight: \", max_node_weight, \"min node weight: \", min_node_weight)\n",
        "  normalized_node_weight = {node: ((unnormalized_node_weight[node] - min_node_weight)/(max_node_weight - min_node_weight) if max_node_weight != min_node_weight else unnormalized_node_weight[node]) for node in unnormalized_node_weight.keys()}\n",
        "  #print(\"Unnormalized score: \", unnormalized_node_weight)\n",
        "  #print(\"Normalized score: \", normalized_node_weight)\n",
        "\n",
        "  damping_factor = 0.85\n",
        "  relevance_of_node = {node: np.random.uniform(0,1,1)[0] for node in vocabulary.keys()}\n",
        "  threshold = 0.000000001\n",
        "\n",
        "\n",
        "  #print(relevance_of_node)\n",
        "\n",
        "  count = 0\n",
        "  while True:\n",
        "    count += 1\n",
        "    current_relevance_of_node = dict()\n",
        "    for node in vocabulary.keys():\n",
        "      outer_sum = 0\n",
        "      node_idx = word2id[node]\n",
        "      for j in range(len(directed_graph_adjacency_matrix)):\n",
        "        if j == node_idx:\n",
        "          continue\n",
        "        if directed_graph_adjacency_matrix[j][node_idx] > 0:\n",
        "          den_sum = 0\n",
        "          for k in range(len(directed_graph_adjacency_matrix)):\n",
        "            if k == j:\n",
        "              continue\n",
        "            den_sum += directed_graph_adjacency_matrix[j][k]\n",
        "          outer_sum += ((directed_graph_adjacency_matrix[j][node_idx]/den_sum) * relevance_of_node[id2word[j]])\n",
        "      current_relevance_of_node[node] = (1-damping_factor)*normalized_node_weight[node] + damping_factor*normalized_node_weight[node]*outer_sum\n",
        "    \n",
        "\n",
        "    # checking convergence..\n",
        "    sq_error = sum([(current_relevance_of_node[node] - relevance_of_node[node])**2 for node in vocabulary.keys()])\n",
        "    relevance_of_node = current_relevance_of_node\n",
        "    if sq_error < threshold:\n",
        "      break\n",
        "\n",
        "  #print(relevance_of_node)\n",
        "  #print(count)\n",
        "\n",
        "  degree_centrality  = {node: 0 for node in vocabulary.keys()}\n",
        "\n",
        "  if len(directed_graph_adjacency_matrix) > 1:\n",
        "    for i in range(len(directed_graph_adjacency_matrix)):\n",
        "      count = 0\n",
        "      for j in range(len(directed_graph_adjacency_matrix)):\n",
        "        if i == j:\n",
        "          continue\n",
        "        if directed_graph_adjacency_matrix[j][i] > 0:\n",
        "          count += 1\n",
        "      degree_centrality[id2word[i]] = count / (len(directed_graph_adjacency_matrix)-1)\n",
        "\n",
        "  #print(degree_centrality)\n",
        "\n",
        "  final_keyword_rank = [{'node': node, 'NE_rank': relevance_of_node[node], 'Degree': degree_centrality[node]} for node in vocabulary.keys()]\n",
        "\n",
        "  #print(\"-----------\")\n",
        "  final_keyword_rank = sorted(final_keyword_rank, key = lambda i: (i['NE_rank'], i['Degree']), reverse = True)\n",
        "\n",
        "  final_keywords = [keyword['node'] for keyword in final_keyword_rank]\n",
        "\n",
        "  return final_keywords"
      ],
      "metadata": {
        "id": "FVkoAcGos6SQ"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for tweet in df_query['Preprocessed_Data']:\n",
        "  tweet_query.extend(tweet)"
      ],
      "metadata": {
        "id": "vgHmoH0ST3K0"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tweet_query"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HmUpvWHIT3lY",
        "outputId": "a3902799-bfb4-405c-e908-c4555513eead"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['desperate',\n",
              " 'let',\n",
              " 'loose',\n",
              " 'yatinarsinghanand',\n",
              " 'violent',\n",
              " 'rhetoric',\n",
              " 'plottokillmodi',\n",
              " 'hijab',\n",
              " 'hate',\n",
              " 'hateâ\\x80¦',\n",
              " 'yatinarsinghanand',\n",
              " 'plottokillmodi',\n",
              " 'hijab',\n",
              " 'woman',\n",
              " 'forced',\n",
              " 'wear',\n",
              " 'wear',\n",
              " 'something',\n",
              " 'hijab',\n",
              " 'hijabisourpride',\n",
              " 'hijabisfundamentalright',\n",
              " 'hijaboruniform',\n",
              " 'hijabcontroversy',\n",
              " 'hijabcircular',\n",
              " 'hijabisindividualright',\n",
              " 'ð\\x9f§\\x95',\n",
              " 'hijab',\n",
              " 'hijabisourpride',\n",
              " 'hijabisfundamentalright',\n",
              " 'hijaboruniform',\n",
              " 'hijabcontroversy',\n",
              " 'hijabcircular',\n",
              " 'hijabisindividualright',\n",
              " 'protest',\n",
              " 'u',\n",
              " 'city',\n",
              " 'karnataka',\n",
              " 'hijab',\n",
              " 'ban',\n",
              " 'indian',\n",
              " 'american',\n",
              " 'ally',\n",
              " 'demonstrated',\n",
              " 'islamophobic',\n",
              " 'unconstitutional',\n",
              " 'ban',\n",
              " 'school',\n",
              " 'india',\n",
              " 's',\n",
              " 'nationalist',\n",
              " 'bjp',\n",
              " 'government',\n",
              " 'take',\n",
              " 'action',\n",
              " 'canadaâ\\x9e¡ï¸\\x8f',\n",
              " 'http',\n",
              " 'tco957kbm8ajt',\n",
              " 'take',\n",
              " 'action',\n",
              " 'globallyâ\\x9e¡ï¸\\x8f',\n",
              " 'http',\n",
              " 'tco5gy741dzp1',\n",
              " 'http',\n",
              " 'tcobvsjhjohkc',\n",
              " 'karnataka',\n",
              " 'hijab',\n",
              " 'islamophobic',\n",
              " 'bjp',\n",
              " 'protest',\n",
              " 'local',\n",
              " 'dallas',\n",
              " 'muslim',\n",
              " 'jfk',\n",
              " 'square',\n",
              " 'downtown',\n",
              " 'draconian',\n",
              " 'hijab',\n",
              " 'law',\n",
              " 'today',\n",
              " 'entire',\n",
              " 'u',\n",
              " 'listening',\n",
              " 'voice',\n",
              " 'muslim',\n",
              " 'woman',\n",
              " 'thelivetvnews',\n",
              " 'http',\n",
              " 'tcokcm413yb8n',\n",
              " 'hijab',\n",
              " 'dallas',\n",
              " 'tx',\n",
              " 'feel',\n",
              " 'sorry',\n",
              " 'zahirawasim',\n",
              " 'religion',\n",
              " 'must',\n",
              " 'make',\n",
              " 'ppl',\n",
              " 'grow',\n",
              " 'make',\n",
              " 'ppl',\n",
              " 'regressive',\n",
              " 'time',\n",
              " 'reform',\n",
              " 'dear',\n",
              " 'zahirawasim',\n",
              " 'aisha',\n",
              " 'fought',\n",
              " 'ali',\n",
              " 'war',\n",
              " 'wearing',\n",
              " 'hijab',\n",
              " 'u',\n",
              " 'think',\n",
              " 'u',\n",
              " 'know',\n",
              " 'religion',\n",
              " 'aisha',\n",
              " 'stop',\n",
              " 'spreading',\n",
              " 'myth',\n",
              " 'ur',\n",
              " 'faith',\n",
              " 'hijab',\n",
              " 'obligation',\n",
              " 'zahirawasim',\n",
              " 'hijab',\n",
              " 'good',\n",
              " 'sunday',\n",
              " 'ð\\x9f\\x98\\x8e',\n",
              " 'tour',\n",
              " 'castle',\n",
              " 'click',\n",
              " 'link',\n",
              " 'bio',\n",
              " 'humairasquare',\n",
              " 'dropshipdiperlukan',\n",
              " 'dropshipmalaysia',\n",
              " 'hijab',\n",
              " 'love',\n",
              " 'bawalcottonvoile',\n",
              " 'tudungbawal',\n",
              " 'tudungbawalkayangan',\n",
              " 'ootd',\n",
              " 'http',\n",
              " 'tconjhzlywwvg',\n",
              " 'humairasquare',\n",
              " 'dropshipdiperlukan',\n",
              " 'dropshipmalaysia',\n",
              " 'hijab',\n",
              " 'love',\n",
              " 'bawalcottonvoile',\n",
              " 'tudungbawal',\n",
              " 'tudungbawalkayangan',\n",
              " 'ootd',\n",
              " 'illeberasim',\n",
              " 'school',\n",
              " 'uniform',\n",
              " 'fr',\n",
              " 'everyone',\n",
              " 'ban',\n",
              " 'anythingonly',\n",
              " '5',\n",
              " 'girl',\n",
              " 'suddenly',\n",
              " 'wanted',\n",
              " 'hijabcanada',\n",
              " 'using',\n",
              " 'emergency',\n",
              " '2',\n",
              " 'counter',\n",
              " 'canadafreedomconvoycan',\n",
              " 'anyone',\n",
              " 'dare',\n",
              " 'criticize',\n",
              " 'blm',\n",
              " 'amp',\n",
              " 'risk',\n",
              " 'loosing',\n",
              " 'job',\n",
              " 'kind',\n",
              " 'free',\n",
              " 'speech',\n",
              " 'liberalism',\n",
              " 'avatans',\n",
              " 'http',\n",
              " 'tcozrcptpnvip',\n",
              " 'hijab',\n",
              " 'canadafreedomconvoy',\n",
              " 'blm',\n",
              " 'hijab',\n",
              " 'woman',\n",
              " 'lead',\n",
              " 'way',\n",
              " 'raffia',\n",
              " 'arshad',\n",
              " 'appointment',\n",
              " 'judge',\n",
              " 'midland',\n",
              " 'circuit',\n",
              " 'see',\n",
              " 'personal',\n",
              " 'achievement',\n",
              " 'bigger',\n",
              " 'joint',\n",
              " 'head',\n",
              " 'st',\n",
              " 'mary',\n",
              " 's',\n",
              " 'chamber',\n",
              " 'said',\n",
              " 'led',\n",
              " 'way',\n",
              " 'muslim',\n",
              " 'woman',\n",
              " 'succeed',\n",
              " 'http',\n",
              " 'tcowgezqfoojz',\n",
              " 'hijab',\n",
              " 'ambreenzaidi',\n",
              " 'woman',\n",
              " 'hijab',\n",
              " 'lead',\n",
              " 'way',\n",
              " 'hijab',\n",
              " 'never',\n",
              " 'hijab',\n",
              " 'hijabisourpride',\n",
              " 'brahmin',\n",
              " 'dalitlivesmatter',\n",
              " 'http',\n",
              " 'tconszf9vzmjy',\n",
              " 'hijab',\n",
              " 'hijabisourpride',\n",
              " 'brahmin',\n",
              " 'dalitlivesmatter',\n",
              " 'kick',\n",
              " 'footballhelps',\n",
              " 'hijabclad',\n",
              " 'girl',\n",
              " 'mumbai',\n",
              " 'breaknewground',\n",
              " 'http',\n",
              " 'tcojzz8pg0wbq',\n",
              " 'via',\n",
              " 'timesofindia',\n",
              " 'coachzakirhussainansari',\n",
              " 'started',\n",
              " 'girl',\n",
              " 'team',\n",
              " 'group',\n",
              " '20',\n",
              " '2018',\n",
              " 'footballhelps',\n",
              " 'hijab',\n",
              " 'girl',\n",
              " 'breaknewground',\n",
              " 'coachzakirhussainansari',\n",
              " 'wish',\n",
              " 'great',\n",
              " 'shahid',\n",
              " 'palni',\n",
              " 'baba',\n",
              " 'alive',\n",
              " 'hijab',\n",
              " 'issue',\n",
              " 'ongoing',\n",
              " 'karnataka',\n",
              " 'ð\\x9f\\x98\\x93ð\\x9f\\x98\\x93',\n",
              " 'may',\n",
              " 'allah',\n",
              " 'pleased',\n",
              " 'give',\n",
              " 'higher',\n",
              " 'rank',\n",
              " 'jannah',\n",
              " 'hijab',\n",
              " 'zafarsareshwala',\n",
              " 'unfortunate',\n",
              " 'set',\n",
              " 'unpleasant',\n",
              " 'precedence',\n",
              " 'next',\n",
              " 'demand',\n",
              " 'hijab',\n",
              " 'police',\n",
              " 'military',\n",
              " 'airline',\n",
              " 'show',\n",
              " 'bunch',\n",
              " 'rogue',\n",
              " 'element',\n",
              " 'arm',\n",
              " 'twist',\n",
              " 'get',\n",
              " 'unreasonable',\n",
              " 'demand',\n",
              " 'hijab',\n",
              " 'â\\x80\\x9cfatâ\\x80\\x9d',\n",
              " 'girl',\n",
              " 'post',\n",
              " 'photo',\n",
              " 'wearing',\n",
              " 'booty',\n",
              " 'short',\n",
              " 'freefromhijab',\n",
              " 'â\\x80\\x9cskinnyâ\\x80\\x9d',\n",
              " 'girl',\n",
              " 'go',\n",
              " 'school',\n",
              " 'wearing',\n",
              " 'fitted',\n",
              " 'jean',\n",
              " 'â\\x80\\x9cpromoting',\n",
              " 'anorexiaâ\\x80\\x9d',\n",
              " 'start',\n",
              " 'wearing',\n",
              " 'loose',\n",
              " 'clothes',\n",
              " 'â\\x80\\x9cpromoting',\n",
              " 'hijabâ\\x80\\x9d',\n",
              " 'skinnyshaming',\n",
              " 'freefromhijab',\n",
              " 'hijab',\n",
              " 'skinnyshaming',\n",
              " 'mindset',\n",
              " 'congressif',\n",
              " 'muslim',\n",
              " 'allowed',\n",
              " 'wear',\n",
              " 'hijab',\n",
              " 'congress',\n",
              " 'sp',\n",
              " 'said',\n",
              " 'punjabies',\n",
              " 'allowed',\n",
              " 'wear',\n",
              " 'traditional',\n",
              " 'dress',\n",
              " 'incindia',\n",
              " 'bjp4india',\n",
              " 'http',\n",
              " 'tcoxwvwwaeqtl',\n",
              " 'hijab',\n",
              " 'à¤\\x95à¥\\x87à¤°à¤²',\n",
              " 'à¤\\xadà¤¾à¤°à¤¤',\n",
              " 'educated',\n",
              " 'woman',\n",
              " 'progressive',\n",
              " 'family',\n",
              " 'freedom',\n",
              " 'education',\n",
              " 'dressing',\n",
              " 'lifestyle',\n",
              " 'want',\n",
              " 'young',\n",
              " 'girl',\n",
              " 'wear',\n",
              " 'hijab',\n",
              " 'burka',\n",
              " 'hijab',\n",
              " 'hijab',\n",
              " 'iamcouncil',\n",
              " 'khanabadosh0',\n",
              " 'huge',\n",
              " 'rally',\n",
              " 'dallas',\n",
              " 'protesting',\n",
              " 'nazi',\n",
              " 'inspired',\n",
              " 'hijab',\n",
              " 'ban',\n",
              " 'sister',\n",
              " 'clearly',\n",
              " 'made',\n",
              " 'clear',\n",
              " 'hijabisourright',\n",
              " 'http',\n",
              " 'tcobmyhvqsep4',\n",
              " 'hijab',\n",
              " 'hijabisourright',\n",
              " 'karnataka',\n",
              " 'critical',\n",
              " 'moment',\n",
              " 'wear',\n",
              " 'hijab',\n",
              " 'asked',\n",
              " 'remove',\n",
              " 'hijabisfundamentalright',\n",
              " 'hijab',\n",
              " 'hijabisfundamentalright',\n",
              " 'reposted',\n",
              " 'isabellapodesta',\n",
              " 'ð\\x9f\\x93¸',\n",
              " 'â\\x80¢',\n",
              " 'â\\x80¢',\n",
              " 'â\\x80¢',\n",
              " 'â\\x80¢',\n",
              " 'â\\x80¢',\n",
              " 'â\\x80¢',\n",
              " 'modestfashion',\n",
              " 'hijab',\n",
              " 'modestclothing',\n",
              " 'modest',\n",
              " 'revert',\n",
              " 'reverttings',\n",
              " 'fashion',\n",
              " 'hijabifashion',\n",
              " 'abaya',\n",
              " 'muslim',\n",
              " 'muslimah',\n",
              " 'http',\n",
              " 'tcocqcnehlaty',\n",
              " 'modestfashion',\n",
              " 'hijab',\n",
              " 'modestclothing',\n",
              " 'modest',\n",
              " 'revert',\n",
              " 'reverttings',\n",
              " 'fashion',\n",
              " 'hijabifashion',\n",
              " 'abaya',\n",
              " 'muslim',\n",
              " 'muslimah',\n",
              " 'woman',\n",
              " 'iran',\n",
              " 'deal',\n",
              " 'daily',\n",
              " 'basis',\n",
              " 'iran',\n",
              " 'womensrights',\n",
              " 'hijab',\n",
              " 'veil',\n",
              " 'islam',\n",
              " 'atheist',\n",
              " 'agnostic',\n",
              " 'discrimination',\n",
              " 'http',\n",
              " 'tcoksquzftfmp',\n",
              " 'iran',\n",
              " 'womensrights',\n",
              " 'hijab',\n",
              " 'veil',\n",
              " 'islam',\n",
              " 'atheist',\n",
              " 'agnostic',\n",
              " 'discrimination',\n",
              " 'yesterday',\n",
              " 'noticed',\n",
              " 'one',\n",
              " 'mobile',\n",
              " 'store',\n",
              " 'owned',\n",
              " 'pissfuls',\n",
              " 'mu',\n",
              " 'l',\n",
              " 'men',\n",
              " 'sale',\n",
              " 'executive',\n",
              " 'mu',\n",
              " 'l',\n",
              " 'woman',\n",
              " 'r',\n",
              " 'employed',\n",
              " 'pissfuls',\n",
              " 'anywhere',\n",
              " 'known',\n",
              " 'fact',\n",
              " 'reminder',\n",
              " 'want',\n",
              " 'hijab',\n",
              " 'education',\n",
              " 'bsbommai',\n",
              " 'bcnageshbjp',\n",
              " 'hijab',\n",
              " 'zairawasim',\n",
              " 'say',\n",
              " 'absolute',\n",
              " 'justice',\n",
              " 'woman',\n",
              " 'ask',\n",
              " 'give',\n",
              " 'either',\n",
              " 'education',\n",
              " 'hijab',\n",
              " 'citing',\n",
              " 'woman',\n",
              " 'empowerment',\n",
              " 'even',\n",
              " 'worst',\n",
              " 'read',\n",
              " 'full',\n",
              " 'post',\n",
              " 'hijab',\n",
              " 'row',\n",
              " 'hijab',\n",
              " 'hijabrow',\n",
              " 'http',\n",
              " 'tcomgovcwpjbr',\n",
              " 'zairawasim',\n",
              " 'hijab',\n",
              " 'hijabrow',\n",
              " 'cleaver',\n",
              " 'strategy',\n",
              " 'whole',\n",
              " 'hijab',\n",
              " 'issue',\n",
              " 'claim',\n",
              " 'want',\n",
              " 'hijab',\n",
              " 'actually',\n",
              " 'coming',\n",
              " 'burka',\n",
              " 'afghanistan',\n",
              " 'style',\n",
              " 'shah',\n",
              " 'bano',\n",
              " '20',\n",
              " 'talibanisation',\n",
              " 'http',\n",
              " 'tcoysvcewmuig',\n",
              " 'hijab',\n",
              " 'hijabcontroversy',\n",
              " 'unveils',\n",
              " 'religious',\n",
              " 'political',\n",
              " 'faultlines',\n",
              " 'india',\n",
              " 'wearing',\n",
              " 'hijab',\n",
              " 'expression',\n",
              " 'protected',\n",
              " 'article19',\n",
              " '1',\n",
              " 'constitution',\n",
              " 'guarantee',\n",
              " 'right',\n",
              " 'freedom',\n",
              " 'speech',\n",
              " 'expression',\n",
              " 'writes',\n",
              " 'ashokbhan2',\n",
              " 'http',\n",
              " 'tconrx75zpzt2',\n",
              " 'hijabcontroversy',\n",
              " 'faultlines',\n",
              " 'india',\n",
              " 'hijab',\n",
              " 'article19',\n",
              " 'constitution',\n",
              " 'guarantee',\n",
              " 'freedom',\n",
              " 'speech',\n",
              " 'expression',\n",
              " 'slogan',\n",
              " 'bharatmatakijai',\n",
              " 'vandemataram',\n",
              " 'raised',\n",
              " 'saffronclad',\n",
              " 'hijab',\n",
              " 'set',\n",
              " 'fire',\n",
              " 'aligarh',\n",
              " 'http',\n",
              " 'tcoynrfyh4n5j',\n",
              " 'bharatmatakijai',\n",
              " 'vandemataram',\n",
              " 'saffronclad',\n",
              " 'hijab',\n",
              " 'aligarh',\n",
              " 'hijab',\n",
              " 'protest',\n",
              " 'downtown',\n",
              " 'dallasmy',\n",
              " 'sister',\n",
              " 'also',\n",
              " 'thereð\\x9f\\x99\\x82',\n",
              " 'thank',\n",
              " 'much',\n",
              " 'muslim',\n",
              " 'brother',\n",
              " 'sister',\n",
              " 'support',\n",
              " 'hijabisourright',\n",
              " 'hijabisourpride',\n",
              " 'hijabisfundamentalright',\n",
              " 'hijab',\n",
              " 'karnatakahijabrow',\n",
              " 'islamophobiainindia',\n",
              " 'http',\n",
              " 'tcoudanmcicea',\n",
              " 'hijabisourright',\n",
              " 'hijabisourpride',\n",
              " 'hijabisfundamentalright',\n",
              " 'hijab',\n",
              " 'karnatakahijabrow',\n",
              " 'islamophobiainindia',\n",
              " 'dmk',\n",
              " 'à®¤à®¿à®°à¯\\x81à®\\x9fà¯\\x8dà®\\x9fà¯\\x81à®¤à®¿à®°à®¾à®µà®¿à®\\x9fà®®à¯\\x8d',\n",
              " 'abuse',\n",
              " 'hindu',\n",
              " 'hindu',\n",
              " 'god',\n",
              " 'heritage',\n",
              " 'tradition',\n",
              " 'normal',\n",
              " 'hindu',\n",
              " 'human',\n",
              " 'amp',\n",
              " 'stone',\n",
              " 'block',\n",
              " 'harmless',\n",
              " 'factual',\n",
              " 'opinion',\n",
              " 'hijab',\n",
              " 'wearing',\n",
              " 'voter',\n",
              " 'localbodyelection',\n",
              " 'hurt',\n",
              " 'muslim',\n",
              " 'ever',\n",
              " 'cm',\n",
              " 'biased',\n",
              " 'mkstalin',\n",
              " 'http',\n",
              " 'tcozdlhlodur4',\n",
              " 'dmk',\n",
              " 'à®¤à®¿à®°à¯\\x81à®\\x9fà¯\\x8dà®\\x9fà¯\\x81à®¤à®¿à®°à®¾à®µà®¿à®\\x9fà®®à¯\\x8d',\n",
              " 'hindu',\n",
              " 'hijab',\n",
              " 'localbodyelection',\n",
              " 'mkstalin',\n",
              " 'karnatakahijabcontroversy',\n",
              " '58',\n",
              " 'student',\n",
              " 'allegedly',\n",
              " 'suspended',\n",
              " 'college',\n",
              " 'removing',\n",
              " 'hijab',\n",
              " 'however',\n",
              " 'shivamogga',\n",
              " 'deputy',\n",
              " 'commissioner',\n",
              " 'claimed',\n",
              " 'principal',\n",
              " 'institution',\n",
              " 'threatened',\n",
              " 'suspension',\n",
              " 'order',\n",
              " 'issued',\n",
              " 'http',\n",
              " 'tcoll4n3rhgne',\n",
              " 'karnatakahijabcontroversy',\n",
              " 'hijab',\n",
              " 'radrama',\n",
              " 'rohinisgh',\n",
              " 'never',\n",
              " 'hijab',\n",
              " 'wanted',\n",
              " 'create',\n",
              " 'optic',\n",
              " 'uttarakhandelections2022',\n",
              " 'hijab',\n",
              " 'uttarakhandelections2022',\n",
              " 'bommai',\n",
              " 'priyaakulkarni2',\n",
              " 'currently',\n",
              " 'hijab',\n",
              " 'valentineday',\n",
              " 'point',\n",
              " 'achieve',\n",
              " 'attack',\n",
              " 'hindu',\n",
              " 'religious',\n",
              " 'festival',\n",
              " 'like',\n",
              " 'diwali',\n",
              " 'holi',\n",
              " 'bhai',\n",
              " 'duaj',\n",
              " 'durga',\n",
              " 'pooja',\n",
              " 'ganesh',\n",
              " 'chaturthi',\n",
              " 'many',\n",
              " 'banhijab',\n",
              " 'muslim',\n",
              " 'liveorleave',\n",
              " 'hijab',\n",
              " 'valentineday',\n",
              " 'banhijab',\n",
              " 'liveorleave',\n",
              " 'india',\n",
              " 'would',\n",
              " 'hijab',\n",
              " 'perfect',\n",
              " 'tool',\n",
              " 'transport',\n",
              " 'someone',\n",
              " 'hidden',\n",
              " 'plain',\n",
              " 'sight',\n",
              " 'hijab',\n",
              " '3',\n",
              " 'phase',\n",
              " 'voting',\n",
              " 'suddenly',\n",
              " 'hijabrow',\n",
              " 'news',\n",
              " 'medium',\n",
              " 'reporting',\n",
              " 'drama',\n",
              " 'bring',\n",
              " 'bjp',\n",
              " 'election',\n",
              " 'amp',\n",
              " 'saturated',\n",
              " 'election',\n",
              " 'student',\n",
              " 'amp',\n",
              " 'relegion',\n",
              " 'used',\n",
              " 'puppet',\n",
              " 'political',\n",
              " 'people',\n",
              " 'yet',\n",
              " 'hijabcontroversy',\n",
              " 'hijab',\n",
              " 'hijabrow',\n",
              " 'hijabcontroversy',\n",
              " 'hijab',\n",
              " 'ktaka',\n",
              " 'minister',\n",
              " 'k',\n",
              " 'ishwarappa',\n",
              " 'said',\n",
              " 'saffron',\n",
              " 'flag',\n",
              " 'replace',\n",
              " 'tricolour',\n",
              " 'red',\n",
              " 'fort',\n",
              " 'day',\n",
              " 'bjp',\n",
              " 'never',\n",
              " 'fails',\n",
              " 'exhibit',\n",
              " 'disrespect',\n",
              " 'nation',\n",
              " 'pride',\n",
              " 'seditious',\n",
              " 'crime',\n",
              " 'tricolor',\n",
              " 'ishwarappa',\n",
              " 'redfort',\n",
              " 'saffronflag',\n",
              " 'saffron',\n",
              " 'hijab',\n",
              " 'tricolor',\n",
              " 'ishwarappa',\n",
              " 'redfort',\n",
              " 'saffronflag',\n",
              " 'saffron',\n",
              " 'hijab',\n",
              " 'frontlineindia',\n",
              " 'bharat4justice',\n",
              " 'yes',\n",
              " 'itâ\\x80\\x99s',\n",
              " 'hindutva',\n",
              " 'agenda',\n",
              " 'hater',\n",
              " 'hijab',\n",
              " 'excuse',\n",
              " 'hindutva',\n",
              " 'hater',\n",
              " 'hijab',\n",
              " 'shakibhaq',\n",
              " 'ð\\x9f¤£ð\\x9f¤£ð\\x9f¤£',\n",
              " 'converted',\n",
              " 'duplicate',\n",
              " 'demand',\n",
              " 'anything',\n",
              " 'original',\n",
              " 'say',\n",
              " 'hijab',\n",
              " 'http',\n",
              " 'tcoqzxnsh8mkq',\n",
              " 'hijab',\n",
              " 'dhaka',\n",
              " 'ð\\x9f\\x87§ð\\x9f\\x87©',\n",
              " 'choose',\n",
              " 'wear',\n",
              " 'hijab',\n",
              " 'protect',\n",
              " 'choice',\n",
              " 'making',\n",
              " 'equally',\n",
              " 'sure',\n",
              " 'stand',\n",
              " 'choice',\n",
              " 'someone',\n",
              " 'turn',\n",
              " 'http',\n",
              " 'tcozmafaxxboe',\n",
              " 'hijab',\n",
              " 'bommai',\n",
              " 'busy',\n",
              " 'spreading',\n",
              " 'hatred',\n",
              " 'schoolcollege',\n",
              " 'going',\n",
              " 'hijab',\n",
              " 'wearing',\n",
              " 'muslim',\n",
              " 'girl',\n",
              " 'http',\n",
              " 'tcou4plmdbvqz',\n",
              " 'bommai',\n",
              " 'hijab',\n",
              " 'advaidism',\n",
              " '1',\n",
              " 'support',\n",
              " 'hijab',\n",
              " 'burka',\n",
              " 'muslim',\n",
              " 'fellow',\n",
              " 'citizen',\n",
              " 'hijabburka',\n",
              " 'existence',\n",
              " 'prof',\n",
              " 'ultimate',\n",
              " 'expression',\n",
              " 'freedom',\n",
              " 'doesnâ\\x80\\x99t',\n",
              " 'need',\n",
              " 'logic',\n",
              " 'logic',\n",
              " 'patriarchal',\n",
              " 'tool',\n",
              " 'fool',\n",
              " 'rationally',\n",
              " 'marginalized',\n",
              " 'people',\n",
              " 'hijab',\n",
              " 'beyond',\n",
              " 'logic',\n",
              " 'hijab',\n",
              " 'burka',\n",
              " 'mysuru',\n",
              " 'anniezaidi',\n",
              " 'people',\n",
              " 'like',\n",
              " 'keep',\n",
              " 'muslim',\n",
              " 'backward',\n",
              " 'propoganda',\n",
              " 'changed',\n",
              " '2022',\n",
              " 'suddenly',\n",
              " 'muslim',\n",
              " 'girl',\n",
              " 'want',\n",
              " 'hijab',\n",
              " 'hijab',\n",
              " 'false',\n",
              " 'propegenda',\n",
              " 'hijab',\n",
              " 'burqa',\n",
              " 'spreading',\n",
              " 'called',\n",
              " 'indian',\n",
              " 'muslim',\n",
              " 'woman',\n",
              " 's',\n",
              " 'actually',\n",
              " 'r',\n",
              " 'propengendist',\n",
              " 'practice',\n",
              " 'since',\n",
              " '2030',\n",
              " 'year',\n",
              " 'india',\n",
              " 'true',\n",
              " 'hijab',\n",
              " 'practicing',\n",
              " '1400',\n",
              " 'year',\n",
              " 'among',\n",
              " 'muslim',\n",
              " 'woman',\n",
              " 'either',\n",
              " 'form',\n",
              " 'burqa',\n",
              " 'hijab',\n",
              " 'burqa',\n",
              " 'karnataka',\n",
              " 'ð\\x9f\\x91\\x89',\n",
              " 'http',\n",
              " 'tcoqnb5661ycy',\n",
              " 'à¤®à¤°à¤¾à¤',\n",
              " 'à¥\\x80',\n",
              " 'à¤®à¤°à¤¾à¤',\n",
              " 'à¥\\x80à¤¬à¤¾à¤¤à¤®à¥\\x8dà¤¯à¤¾',\n",
              " 'marathi',\n",
              " 'marathinews',\n",
              " 'navarashtra',\n",
              " 'hijab',\n",
              " 'controversy',\n",
              " 'karnataka',\n",
              " 'à¤®à¤°à¤¾à¤',\n",
              " 'à¥\\x80',\n",
              " 'à¤®à¤°à¤¾à¤',\n",
              " 'à¥\\x80à¤¬à¤¾à¤¤à¤®à¥\\x8dà¤¯à¤¾',\n",
              " 'marathi',\n",
              " 'marathinews',\n",
              " 'navarashtra',\n",
              " 'hijab',\n",
              " 'donâ\\x80\\x99t',\n",
              " 'know',\n",
              " 'headscarf',\n",
              " 'girl',\n",
              " 'think',\n",
              " 'men',\n",
              " 'taken',\n",
              " 'copyright',\n",
              " 'thought',\n",
              " 'thatâ\\x80\\x99s',\n",
              " 'religionâ\\x80\\x99s',\n",
              " 'worst',\n",
              " 'crime',\n",
              " 'writes',\n",
              " 'prasannara',\n",
              " 'hijab',\n",
              " 'orhanpamuk',\n",
              " 'snow',\n",
              " 'http',\n",
              " 'tcogojmyre0ba',\n",
              " 'http',\n",
              " 'tcoihxp4yn4jq',\n",
              " 'hijab',\n",
              " 'orhanpamuk',\n",
              " 'snow',\n",
              " 'tamilnadu',\n",
              " 'row',\n",
              " 'bjp',\n",
              " 'booth',\n",
              " 'agent',\n",
              " 'asks',\n",
              " 'woman',\n",
              " 'remove',\n",
              " 'hijab',\n",
              " 'http',\n",
              " 'tcosa6jmnmtzz',\n",
              " 'booth',\n",
              " 'agent',\n",
              " 'girirajan',\n",
              " 'created',\n",
              " 'ruckus',\n",
              " 'alameen',\n",
              " 'school',\n",
              " 'polling',\n",
              " 'booth',\n",
              " '8th',\n",
              " 'ward',\n",
              " 'melur',\n",
              " 'municipality',\n",
              " 'handsoffmyhijab',\n",
              " 'tamilnadu',\n",
              " 'hijab',\n",
              " 'girirajan',\n",
              " 'handsoffmyhijab',\n",
              " 'wearing',\n",
              " 'hijab',\n",
              " 'personal',\n",
              " 'choice',\n",
              " 'governmentâ\\x80\\x99s',\n",
              " 'hijab',\n",
              " 'restriction',\n",
              " 'school',\n",
              " 'college',\n",
              " 'violate',\n",
              " 'indiaâ\\x80\\x99s',\n",
              " 'obligation',\n",
              " 'international',\n",
              " 'human',\n",
              " 'right',\n",
              " 'law',\n",
              " 'human',\n",
              " 'right',\n",
              " 'watch',\n",
              " 'hijabrow',\n",
              " 'hijab',\n",
              " 'hijabisindividualright',\n",
              " 'hijabrow',\n",
              " 'hijab',\n",
              " 'hijabisindividualright',\n",
              " 'view',\n",
              " 'follow',\n",
              " 'upscinsta',\n",
              " ...]"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "keyword_dataset = df_query['tweet'].tolist()\n",
        "tweet_query_keyword_extractor = keyword_extractor(keyword_dataset)"
      ],
      "metadata": {
        "id": "IhXLi1MqtCFt"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(keyword_dataset[0])\n",
        "print(len(tweet_query))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ofVXCem0S_NV",
        "outputId": "219e793c-d453-4c59-ba51-8a428a5268dc"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "They are so desperate they have let loose #YatiNarsinghanand again with his violent rhetoric. #PlotToKillModi to #hijab to hate and more hate.â¦\n",
            "6398\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tweet_query_keyword_extractor"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HplmCxZVtH1U",
        "outputId": "4ee80907-09b3-4900-dd32-0b5e9000afbc"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['hijab',\n",
              " 'muslim',\n",
              " 'women',\n",
              " 'india',\n",
              " 'wear',\n",
              " 'muslims',\n",
              " 'hijabrow',\n",
              " 'islam',\n",
              " 'allah',\n",
              " 'like',\n",
              " 'girls',\n",
              " 'hijabisourright',\n",
              " 'karnataka',\n",
              " 'wearing',\n",
              " 'amp',\n",
              " 'school',\n",
              " 'hijabisourpride',\n",
              " 'ban',\n",
              " 'you',\n",
              " 'hijabisfundamentalright',\n",
              " 'girl',\n",
              " 'hijabcontroversy',\n",
              " 'one',\n",
              " 'education',\n",
              " 'choice',\n",
              " 'need',\n",
              " 'world',\n",
              " 'schools',\n",
              " 'people',\n",
              " 'islamic',\n",
              " 'woman',\n",
              " 'also',\n",
              " 'want',\n",
              " 'religion',\n",
              " 'first',\n",
              " 'bjp',\n",
              " 'right',\n",
              " 'even',\n",
              " 'sitting',\n",
              " 'hijabisindividualright',\n",
              " 'leaders',\n",
              " 'judge',\n",
              " 'hijabban',\n",
              " 'protest',\n",
              " 'quietly',\n",
              " 'students',\n",
              " 'uniform',\n",
              " 'tell',\n",
              " 'allowed',\n",
              " 'fighting',\n",
              " 'colleges',\n",
              " 'burqa',\n",
              " 'issue',\n",
              " 'started',\n",
              " 'surely',\n",
              " 'public',\n",
              " 'decision',\n",
              " 'religious',\n",
              " 'said',\n",
              " 'support',\n",
              " 'indian',\n",
              " 'rely',\n",
              " 'alive',\n",
              " 'abaya',\n",
              " 'islamophobia',\n",
              " 'freedom',\n",
              " 'muslimah',\n",
              " 'persecution',\n",
              " 'allahuakbar',\n",
              " 'sunnah',\n",
              " 'see',\n",
              " 'ramadan',\n",
              " 'makkah',\n",
              " 'alhamdulillah',\n",
              " 'get',\n",
              " 'islamicquotes',\n",
              " 'dua',\n",
              " 'quran',\n",
              " 'rights',\n",
              " 'hijabplot',\n",
              " 'country',\n",
              " 'hindu',\n",
              " 'practice',\n",
              " 'remove',\n",
              " 'men',\n",
              " 'make',\n",
              " 'karnatakahijabcontroversy',\n",
              " 'cocktribute',\n",
              " 'u',\n",
              " 'jannah',\n",
              " 'saudi',\n",
              " 'prophetmuhammad',\n",
              " 'modest',\n",
              " 'asked',\n",
              " 'dress',\n",
              " 'give',\n",
              " 'college',\n",
              " 'us',\n",
              " 'whole',\n",
              " 'khan',\n",
              " 'subuhi',\n",
              " 'court',\n",
              " 'hindus',\n",
              " 'state',\n",
              " 'saffron',\n",
              " 'many',\n",
              " 'hijaboruniform',\n",
              " 'face',\n",
              " 'stateâ\\x80\\x99',\n",
              " 'hijabaurkitab',\n",
              " 'modestfashion',\n",
              " 'hijabi',\n",
              " 'lashes',\n",
              " 'muhammad',\n",
              " 'way',\n",
              " 'deen',\n",
              " 'stand',\n",
              " 'islamists',\n",
              " 'must',\n",
              " 'important',\n",
              " 'lawyer',\n",
              " 'itâ\\x80\\x99s',\n",
              " 'latest',\n",
              " 'educated',\n",
              " 'â\\x80\\x93',\n",
              " 'much',\n",
              " 'made',\n",
              " 'it',\n",
              " 'not',\n",
              " 'please',\n",
              " 'anything',\n",
              " 'supreme',\n",
              " 'ongoing',\n",
              " 'never',\n",
              " 'class',\n",
              " 'hindutva',\n",
              " 'existence',\n",
              " 'money',\n",
              " 'towards',\n",
              " 'thanks',\n",
              " 'demanding',\n",
              " 'say',\n",
              " 'teen',\n",
              " 'karnatakahijabrow',\n",
              " 'today',\n",
              " 'day',\n",
              " 'home',\n",
              " 'choose',\n",
              " 'understand',\n",
              " 'â\\x80\\x98hijab',\n",
              " 'good',\n",
              " 'entire',\n",
              " 'bank',\n",
              " 'suddenly',\n",
              " 'called',\n",
              " 'order',\n",
              " 'protesting',\n",
              " 'time',\n",
              " 'police',\n",
              " 'muslimgirls',\n",
              " 'go',\n",
              " 'hate',\n",
              " 'cover',\n",
              " 'faith',\n",
              " 'nsfw',\n",
              " 'respect',\n",
              " 'around',\n",
              " 'sometimes',\n",
              " 'across',\n",
              " 'oribelle',\n",
              " 'name',\n",
              " 'islamophobiainindia',\n",
              " 'logic',\n",
              " 'this',\n",
              " 'movie',\n",
              " 'still',\n",
              " 'think',\n",
              " 'since',\n",
              " '»',\n",
              " 'ask',\n",
              " 'attack',\n",
              " 'donâ\\x80\\x99t',\n",
              " 'read',\n",
              " 'hai',\n",
              " 'love',\n",
              " 'upcoming',\n",
              " 'cant',\n",
              " 'permission',\n",
              " 'years',\n",
              " 'protests',\n",
              " 'modesty',\n",
              " 'ð\\x9f\\x91\\x89ð\\x9f\\x91\\x89',\n",
              " 'â\\x80¢',\n",
              " 'stop',\n",
              " 'fact',\n",
              " 'asking',\n",
              " 'writes',\n",
              " 'agenda',\n",
              " 'shop',\n",
              " 'â\\x81',\n",
              " 'come',\n",
              " 'know',\n",
              " 'actor',\n",
              " 'im',\n",
              " 'follow',\n",
              " 'controversy',\n",
              " 'can',\n",
              " '2022',\n",
              " 'demand',\n",
              " 'her',\n",
              " 'wears',\n",
              " 'fight',\n",
              " 'daily',\n",
              " 'put',\n",
              " 'secular',\n",
              " 'veil',\n",
              " 'old',\n",
              " 'step',\n",
              " 'plain',\n",
              " 'case',\n",
              " 'looking',\n",
              " 'congress',\n",
              " 'femboy',\n",
              " 'wrong',\n",
              " 'milf',\n",
              " 'propaganda',\n",
              " 'square',\n",
              " 'obligation',\n",
              " 'without',\n",
              " 'male',\n",
              " 'might',\n",
              " 'supporting',\n",
              " 'thats',\n",
              " 'booth',\n",
              " 'came',\n",
              " 'ð\\x9f\\x91\\x89',\n",
              " 'post',\n",
              " 'speech',\n",
              " 'paid',\n",
              " 'wish',\n",
              " 'males',\n",
              " 'hijabisouridentity',\n",
              " 'row',\n",
              " 'concept',\n",
              " 'elections',\n",
              " 'banned',\n",
              " 'media',\n",
              " 'place',\n",
              " 'issues',\n",
              " 'pakistan',\n",
              " 'full',\n",
              " 'niqab',\n",
              " 'election',\n",
              " 'yasser',\n",
              " 'biased',\n",
              " 'let',\n",
              " 'create',\n",
              " 'purchased',\n",
              " 'skull',\n",
              " 'rule',\n",
              " 'per',\n",
              " 'used',\n",
              " 'too',\n",
              " 'law',\n",
              " 'issued',\n",
              " 'islamicpost',\n",
              " 'well',\n",
              " 'video',\n",
              " 'political',\n",
              " 'look',\n",
              " 'pk',\n",
              " 'phone',\n",
              " 'hijabcircular',\n",
              " 'dmme',\n",
              " 'says',\n",
              " 'places',\n",
              " 'life',\n",
              " 'beautiful',\n",
              " 'beard',\n",
              " 'zairawasim',\n",
              " 'spread',\n",
              " 'part',\n",
              " 'crown',\n",
              " 'banning',\n",
              " 'via',\n",
              " 'nations',\n",
              " 'karnatakahijab',\n",
              " 'clearly',\n",
              " 'arafath',\n",
              " 'every',\n",
              " 'job',\n",
              " 'bwc',\n",
              " 'hijabisourfaith',\n",
              " 'property',\n",
              " 'soon',\n",
              " 'shivamogga',\n",
              " 'france',\n",
              " 'view',\n",
              " 'true',\n",
              " 'really',\n",
              " 'roza',\n",
              " 'caste',\n",
              " 'wife',\n",
              " 'namaz',\n",
              " 'expression',\n",
              " 'community',\n",
              " 'i',\n",
              " 'burkah',\n",
              " 'friend',\n",
              " 'already',\n",
              " 'international',\n",
              " 'hypocrisy',\n",
              " 'clothes',\n",
              " 'prince',\n",
              " 'everyone',\n",
              " 'footfetish',\n",
              " 'keep',\n",
              " 'sisters',\n",
              " '«',\n",
              " 'solidarity',\n",
              " 'spreading',\n",
              " 'news',\n",
              " 'r',\n",
              " 'meher',\n",
              " 'father',\n",
              " 'justice',\n",
              " 'month',\n",
              " 'someone',\n",
              " 'rss',\n",
              " 'things',\n",
              " 'loose',\n",
              " 'talak',\n",
              " '1',\n",
              " 'history',\n",
              " 'front',\n",
              " '2',\n",
              " 'real',\n",
              " 'else',\n",
              " 'minorities',\n",
              " 'muslimmen',\n",
              " 'using',\n",
              " 'link',\n",
              " 'taken',\n",
              " 'watch',\n",
              " 'govt',\n",
              " 'city',\n",
              " 'reason',\n",
              " 'either',\n",
              " 'shirt',\n",
              " 'to',\n",
              " 'eventð\\x9f\\x98\\x8d',\n",
              " 'bella',\n",
              " 'regressive',\n",
              " 'â\\x9d¤ï¸\\x8f',\n",
              " 'ago',\n",
              " 'that',\n",
              " 'worship',\n",
              " 'fear',\n",
              " 'black',\n",
              " 'back',\n",
              " 'empowerment',\n",
              " 'kuwait',\n",
              " 'long',\n",
              " 'wasim',\n",
              " 'boy']"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tweet_keywords_yake = []\n",
        "kw_extractor = yake.KeywordExtractor(top=20, stopwords=None)\n",
        "keywords = kw_extractor.extract_keywords(' '.join(tweet_query))\n",
        "#keywords = kw_extractor.extract_keywords(' '.join(df_query['tweet'].tolist()))\n",
        "for kw, v in keywords:\n",
        "  print(\"Keyphrase: \",kw, \": score\", v)\n",
        "  for key in kw.split():\n",
        "    if(key.lower() not in tweet_keywords_yake):\n",
        "      tweet_keywords_yake.append(key.lower())\n",
        "\n",
        "print(tweet_keywords_yake)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UZDm6Cznts-9",
        "outputId": "6fb35460-969d-4ceb-89af-d966c01c82f3"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keyphrase:  hijab hijab hijab : score 3.538515818965514e-07\n",
            "Keyphrase:  hijab karnataka india : score 4.4295050053984284e-07\n",
            "Keyphrase:  muslim islam muslim : score 7.831784725969966e-07\n",
            "Keyphrase:  hijab hijabisfundamentalright india : score 8.733034882458195e-07\n",
            "Keyphrase:  hijabisourright hijabcontroversy hijab : score 1.2084471666090418e-06\n",
            "Keyphrase:  karnataka india hijab : score 1.2402614015115601e-06\n",
            "Keyphrase:  india hijab karnataka : score 1.2402614015115601e-06\n",
            "Keyphrase:  muslim hijab started : score 1.3280528877404291e-06\n",
            "Keyphrase:  hijab abaya muslim : score 1.3444833831980227e-06\n",
            "Keyphrase:  hijab hijabrow http : score 1.425915298203952e-06\n",
            "Keyphrase:  persecution muslim hijab : score 1.4602327688888964e-06\n",
            "Keyphrase:  hijabcontroversy hijab hijabisfundamentalright : score 1.5153685192729542e-06\n",
            "Keyphrase:  hijabisourright hijab http : score 1.6067046974268703e-06\n",
            "Keyphrase:  wear hijab hijab : score 1.6311894877235963e-06\n",
            "Keyphrase:  hijab muslim india : score 1.6435816858664886e-06\n",
            "Keyphrase:  islam muslim allah : score 1.6736686738241312e-06\n",
            "Keyphrase:  hijab started india : score 1.6956730006765869e-06\n",
            "Keyphrase:  hijab muslim woman : score 1.7331287757545118e-06\n",
            "Keyphrase:  hijab karnatakahijabcontroversy hijabrow : score 2.070500854170206e-06\n",
            "Keyphrase:  wearing hijab hijab : score 2.179807258036074e-06\n",
            "['hijab', 'karnataka', 'india', 'muslim', 'islam', 'hijabisfundamentalright', 'hijabisourright', 'hijabcontroversy', 'started', 'abaya', 'hijabrow', 'http', 'persecution', 'wear', 'allah', 'woman', 'karnatakahijabcontroversy', 'wearing']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "docs_preprocessed = []"
      ],
      "metadata": {
        "id": "0fZNQhToT43g"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Storing file name and data\n",
        "total_documents = 0\n",
        "path = '/content/drive/MyDrive/Tweelink_Dataset/Tweelink_Articles_Processed'\n",
        "for filename in glob(os.path.join(path, '*')):\n",
        "   with open(os.path.join(os.getcwd(), filename), 'r', encoding = 'utf-8',errors = 'ignore') as f:\n",
        "     filename = os.path.basename(f.name)\n",
        "     data = json.load(f)\n",
        "     d_date = data[\"Date\"]\n",
        "     if(d_date==\"\" or d_date==\"Date\"):\n",
        "       continue\n",
        "     format = '%Y-%m-%d'\n",
        " \n",
        "     d_present_date = datetime.datetime.strptime(d_date, format)\n",
        " \n",
        "     if(str(d_present_date.date()) not in [str(u_present_date.date()), str(u_prev_date.date()), str(u_next_date.date())]):\n",
        "       continue\n",
        "   \n",
        "     docs_preprocessed.append({'Name':filename, 'Data':data})\n",
        "     total_documents+=1\n",
        "print(total_documents)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I6fzO-H6T83w",
        "outputId": "12e3bd00-3b90-4e13-ef67-c7e37f0e23b1"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "295\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(docs_preprocessed[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "31_owAnySlGn",
        "outputId": "c896ab68-0dbf-4ea2-d32f-e80a12601692"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'Name': 'SuperBowl_10.json', 'Data': {'S. No.': '10', 'Base Hashtag': 'SuperBowl', 'Link': 'https://techcrunch.com/2022/02/19/this-week-in-apps-androids-privacy-sandbox-super-bowl-app-ads-app-annie-rebrands/', 'Date': '2022-02-20', 'Location': '', 'Title': 'This Week in Apps: Android’s Privacy Sandbox, Super Bowl app ads, App Annie rebrands', 'Body': 'Welcome back to This Week in Apps, the weekly TechCrunch series that recaps the latest in mobile OS news, mobile applications and the overall app economy. The app industry continues to grow, with a record number of downloads and consumer spending across both the iOS and Google Play stores combined in 2021, according to the latest year-end reports. App Annie says global spending across iOS, Google Play and third-party Android app stores in China grew 19% in 2021 to reach $170 billion. Downloads of apps also grew by 5%, reaching 230 billion in 2021, and mobile ad spend grew 23% year-over-year to reach $295 billion. In addition, consumers are spending more time in apps than ever before — even topping the time they spend watching TV, in some cases. The average American watches 3.1 hours of TV per day, for example, but in 2021, they spent 4.1 hours on their mobile device. And they’re not even the world’s heaviest mobile users. In markets like Brazil, Indonesia and South Korea, users surpassed five hours per day in mobile apps in 2021. Apps aren’t just a way to pass idle hours, either. They can grow to become huge businesses. In 2021, 233 apps and games generated over $100 million in consumer spend, and 13 topped $1 billion in revenue, App Annie noted. This was up 20% from 2020 when 193 apps and games topped $100 million in annual consumer spend, and just eight apps topped $1 billion. This Week in Apps offers a way to keep up with this fast-moving industry in one place with the latest from the world of apps, including news, updates, startup fundings, mergers and acquisitions, and suggestions about new apps and games to try, too.', 'Body_processed': ['week', 'apps', 'android', '’', 'privacy', 'sandbox', 'super', 'bowl', 'app', 'ad', 'app', 'annie', 'rebrands', 'welcome', 'back', 'week', 'apps', 'weekly', 'techcrunch', 'series', 'recap', 'latest', 'mobile', 'o', 'news', 'mobile', 'application', 'overall', 'app', 'economy', 'app', 'industry', 'continues', 'grow', 'record', 'number', 'downloads', 'consumer', 'spending', 'across', 'io', 'google', 'play', 'store', 'combined', '2021', 'according', 'latest', 'yearend', 'report', 'app', 'annie', 'say', 'global', 'spending', 'across', 'io', 'google', 'play', 'thirdparty', 'android', 'app', 'store', 'china', 'grew', '19', '2021', 'reach', '170', 'billion', 'downloads', 'apps', 'also', 'grew', '5', 'reaching', '230', 'billion', '2021', 'mobile', 'ad', 'spend', 'grew', '23', 'yearoveryear', 'reach', '295', 'billion', 'addition', 'consumer', 'spending', 'time', 'apps', 'ever', '—', 'even', 'topping', 'time', 'spend', 'watching', 'tv', 'case', 'average', 'american', 'watch', '31', 'hour', 'tv', 'per', 'day', 'example', '2021', 'spent', '41', 'hour', 'mobile', 'device', '’', 'even', 'world', '’', 'heaviest', 'mobile', 'user', 'market', 'like', 'brazil', 'indonesia', 'south', 'korea', 'user', 'surpassed', 'five', 'hour', 'per', 'day', 'mobile', 'apps', '2021', 'apps', '’', 'way', 'pas', 'idle', 'hour', 'either', 'grow', 'become', 'huge', 'business', '2021', '233', 'apps', 'game', 'generated', '100', 'million', 'consumer', 'spend', '13', 'topped', '1', 'billion', 'revenue', 'app', 'annie', 'noted', '20', '2020', '193', 'apps', 'game', 'topped', '100', 'million', 'annual', 'consumer', 'spend', 'eight', 'apps', 'topped', '1', 'billion', 'week', 'apps', 'offer', 'way', 'keep', 'fastmoving', 'industry', 'one', 'place', 'latest', 'world', 'apps', 'including', 'news', 'update', 'startup', 'funding', 'merger', 'acquisition', 'suggestion', 'new', 'apps', 'game', 'try']}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_relevant_docs_list_for_base_hashtag(base_hashtag, base_date, docs_preprocessed):\n",
        "  relevant_docs_list = []\n",
        "  for doc in docs_preprocessed:\n",
        "    if doc['Data']['Base Hashtag']==base_hashtag:\n",
        "      current_date = datetime.datetime.strptime(base_date, format)\n",
        "      prev_date = current_date - datetime.timedelta(days=1)\n",
        "      next_date = current_date + datetime.timedelta(days=1)\n",
        "      if(doc['Data']['Date'] in [str(prev_date.date()), str(current_date.date()), str(next_date.date())]):\n",
        "        relevant_docs_list.append(doc['Name'])\n",
        "  return relevant_docs_list"
      ],
      "metadata": {
        "id": "ASFbpl047xKd"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def precision_at_k(k, base_hashtag, base_date, prediction_list, docs_preprocessed):\n",
        "  relevant_docs_list = get_relevant_docs_list_for_base_hashtag(base_hashtag, base_date, docs_preprocessed)\n",
        "  num_of_relevant_results=0\n",
        "  for itr in range(k):\n",
        "    if (prediction_list[itr][0] in relevant_docs_list):\n",
        "      num_of_relevant_results+=1\n",
        "  return num_of_relevant_results/k"
      ],
      "metadata": {
        "id": "JdWZkYhc7yh7"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def mean_average_precision(max_k, base_hashtag, base_date, relevant_docs, docs_preprocessed):\n",
        "  average_precision=0\n",
        "  ctr=0\n",
        "  for k_val in range(1,max_k+1):\n",
        "    ctr+=1\n",
        "    precision_at_k_val = precision_at_k(k_val, base_hashtag, base_date, relevant_docs, docs_preprocessed)\n",
        "    #print('Hashtag: {}   Precision@{}: {}'.format(base_hashtag, k_val, precision_at_k_val))\n",
        "    average_precision += precision_at_k_val\n",
        "  return average_precision/ctr"
      ],
      "metadata": {
        "id": "k1XBPmJv73xD"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def recall_at_k(k, base_hashtag, base_date, prediction_list, docs_preprocessed):\n",
        "  relevant_docs_list = get_relevant_docs_list_for_base_hashtag(base_hashtag, base_date, docs_preprocessed)\n",
        "  current_num_of_relevant_results=0\n",
        "  for itr in range(k):\n",
        "    if (prediction_list[itr][0] in relevant_docs_list):\n",
        "      current_num_of_relevant_results+=1\n",
        "  if(len(relevant_docs_list)==0):\n",
        "    return 0\n",
        "  return current_num_of_relevant_results/len(relevant_docs_list)"
      ],
      "metadata": {
        "id": "YJuX6kb277hg"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def mean_average_recall(max_k, base_hashtag, base_date, relevant_docs, docs_preprocessed):\n",
        "  average_recall=0\n",
        "  ctr=0\n",
        "  for k_val in range(1,max_k+1):\n",
        "    ctr+=1\n",
        "    recall_at_k_val = recall_at_k(k_val, base_hashtag, base_date, relevant_docs, docs_preprocessed)\n",
        "    #print('Hashtag: {}   Recall@{}: {}'.format(base_hashtag, k_val, recall_at_k_val))\n",
        "    average_recall += recall_at_k_val\n",
        "  return average_recall/ctr"
      ],
      "metadata": {
        "id": "DiHZ2YSi783Y"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def find_relevant_documents_cosine_similarity_count_vectorizer(docs_preprocessed, processed_query):\n",
        "#   cosine_similarities_cv = {}\n",
        "#   for document in docs_preprocessed:\n",
        "#     query_sent = ' '.join(map(str, processed_query))\n",
        "#     doc_text_sent = ' '.join(map(str, document['Data']['Body_processed']))\n",
        "#     data = [query_sent, doc_text_sent]\n",
        "#     count_vectorizer = CountVectorizer(encoding='latin-1', decode_error='ignore', ngram_range=(1,2))\n",
        "#     vector_matrix = count_vectorizer.fit_transform(data)\n",
        "#     cosine_similarity_matrix = cosine_similarity(vector_matrix)\n",
        "#     cosine_similarities_cv[document['Name']] = cosine_similarity_matrix[0][1]\n",
        "#   relevant_docs = list( sorted(cosine_similarities_cv.items(), key=operator.itemgetter(1),reverse=True))[:20]\n",
        "#   for i in range(len(relevant_docs)):\n",
        "#     for j in range(len(docs_preprocessed)):\n",
        "#       if(relevant_docs[i][0] == docs_preprocessed[j]['Name']):\n",
        "#         relevant_docs[i] = (relevant_docs[i][0], relevant_docs[i][1], docs_preprocessed[j]['Data']['Date'] )\n",
        "\n",
        "#   return relevant_docs"
      ],
      "metadata": {
        "id": "kq1KVcjncYkG"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def find_relevant_documents_cosine_similarity_tfidf_vectorizer(docs_preprocessed, processed_query):\n",
        "#   cosine_similarities_tfidf = {}\n",
        "#   for document in docs_preprocessed:\n",
        "#     query_sent = ' '.join(map(str, processed_query))\n",
        "#     doc_text_sent = ' '.join(map(str, document['Data']['Body_processed']))\n",
        "#     data = [query_sent, doc_text_sent]\n",
        "#     Tfidf_vect = TfidfVectorizer(encoding='latin-1', decode_error='ignore', ngram_range=(1,2))\n",
        "#     vector_matrix = Tfidf_vect.fit_transform(data)\n",
        "#     cosine_similarity_matrix = cosine_similarity(vector_matrix)\n",
        "#     cosine_similarities_tfidf[document['Name']] = cosine_similarity_matrix[0][1]\n",
        "#   relevant_docs = list( sorted(cosine_similarities_tfidf.items(), key=operator.itemgetter(1),reverse=True))[:20]\n",
        "#   for i in range(len(relevant_docs)):\n",
        "#     for j in range(len(docs_preprocessed)):\n",
        "#       if(relevant_docs[i][0] == docs_preprocessed[j]['Name']):\n",
        "#         relevant_docs[i] = (relevant_docs[i][0], relevant_docs[i][1], docs_preprocessed[j]['Data']['Date'] )\n",
        "\n",
        "#   return relevant_docs"
      ],
      "metadata": {
        "id": "OZemps7wh0VO"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Making own Corpus and applying Soft Cosine"
      ],
      "metadata": {
        "id": "PEkr0E3WU3_3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.similarities import SoftCosineSimilarity\n",
        "from gensim.models import Word2Vec\n",
        "from gensim import utils\n",
        "\n",
        "def find_relevant_documents_soft_cosine_similarity(docs_preprocessed, processed_query):\n",
        "  whole_corpus_token_form = [article_data['Data']['Body_processed'] for article_data in docs_preprocessed]\n",
        "  whole_corpus_token_form.append(processed_query)\n",
        "\n",
        "  model = Word2Vec(whole_corpus_token_form, vector_size=20, min_count=1)\n",
        "  termsim_index = WordEmbeddingSimilarityIndex(model.wv)\n",
        "  sc_dictionary = dictionary = Dictionary(whole_corpus_token_form)\n",
        "  bow_corpus = [dictionary.doc2bow(document) for document in whole_corpus_token_form]\n",
        "  similarity_matrix = SparseTermSimilarityMatrix(termsim_index, sc_dictionary)\n",
        "  docsim_index = SoftCosineSimilarity(bow_corpus, similarity_matrix, num_best=30)\n",
        "  relevant_docs = docsim_index[dictionary.doc2bow(processed_query)]\n",
        "  iter = 0\n",
        "  diff = 0\n",
        "  for i in range(len(relevant_docs)):\n",
        "    if relevant_docs[i][0] >= len(docs_preprocessed):\n",
        "      continue\n",
        "    relevant_docs[iter] = (docs_preprocessed[relevant_docs[i][0]]['Name'], relevant_docs[i][1], docs_preprocessed[relevant_docs[i][0]]['Data']['Date'])\n",
        "    diff = i - iter\n",
        "    iter += 1\n",
        "  return relevant_docs[:len(relevant_docs)-diff]"
      ],
      "metadata": {
        "id": "TtQfIr8mk-G2"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import timeit\n",
        "\n",
        "# def preprocess_for_soft_cosine(sentence):\n",
        "#     return [w for w in sentence.lower().split() if w not in stop_words]\n",
        "# def find_relevant_documents_soft_cosine_similarity(docs_preprocessed, final_processed_query):\n",
        "#   soft_cosine_similarities = {}\n",
        "#   termsim_index = WordEmbeddingSimilarityIndex(model)\n",
        "#   for document_info in docs_preprocessed:\n",
        "\n",
        "#     # setting up strings for soft-cosine similarity\n",
        "#     document = preprocess_for_soft_cosine(document_info['Data']['Body'])\n",
        "#     processed_query = final_processed_query\n",
        "\n",
        "#     # soft cosine processing started\n",
        "#     data = [processed_query, document]\n",
        "\n",
        "#     start = timeit.timeit()\n",
        "#     sc_dictionary = Dictionary(data)\n",
        "#     processed_query = sc_dictionary.doc2bow(processed_query)\n",
        "#     document = sc_dictionary.doc2bow(document)\n",
        "\n",
        "#     end = timeit.timeit()\n",
        "#     print(\"text to bog: \", end - start)\n",
        "\n",
        "#     start = timeit.timeit()\n",
        "#     data = [processed_query, document]\n",
        "#     tfidf = TfidfModel(data)\n",
        "#     processed_query = tfidf[processed_query]\n",
        "#     document = tfidf[document]\n",
        "\n",
        "#     end = timeit.timeit()\n",
        "#     print(\"tf-idf: \", end - start)\n",
        "\n",
        "#     start = timeit.timeit()\n",
        "#     termsim_matrix = SparseTermSimilarityMatrix(termsim_index, sc_dictionary, tfidf)\n",
        "#     sc_similarity = termsim_matrix.inner_product(processed_query, document, normalized=(True, True))\n",
        "    \n",
        "\n",
        "#     soft_cosine_similarities[document_info['Name']] = sc_similarity\n",
        "\n",
        "#     end = timeit.timeit()\n",
        "#     print(\"into model: \", end - start)\n",
        "\n",
        "\n",
        "#   relevant_docs = list( sorted(soft_cosine_similarities.items(), key=operator.itemgetter(1),reverse=True))[:20]\n",
        "#   for i in range(len(relevant_docs)):\n",
        "#    for j in range(len(docs_preprocessed)):\n",
        "#      if(relevant_docs[i][0] == docs_preprocessed[j]['Name']):\n",
        "#        relevant_docs[i] = (relevant_docs[i][0], relevant_docs[i][1], docs_preprocessed[j]['Data']['Date'] )\n",
        "#   return relevant_docs"
      ],
      "metadata": {
        "id": "g4hzr0SO8vgS"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plain Model (Soft Cosine Similarity)"
      ],
      "metadata": {
        "id": "BGQNcNz2__ri"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plain Model without YAKE / Keyword Extraction (Cosine Similarity Count Vectorizer)\n",
        "relevant_docs_sc_plain = find_relevant_documents_soft_cosine_similarity(docs_preprocessed, tweet_query)\n",
        "\n",
        "for rank, doc in enumerate(relevant_docs_sc_plain):\n",
        "  print('Rank: {} Relevant Document: {}'.format(rank+1,doc))\n",
        "\n",
        "print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ElHD4x3c3yC0",
        "outputId": "0fc628c5-ca46-4603-e5e8-4e870bdd6252"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 13651/13651 [00:34<00:00, 393.09it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rank: 1 Relevant Document: ('hijab_282.json', 0.8820684552192688, '2022-02-19')\n",
            "Rank: 2 Relevant Document: ('hijab_288.json', 0.7525495290756226, '2022-02-19')\n",
            "Rank: 3 Relevant Document: ('hijab_285.json', 0.7524893283843994, '2022-02-19')\n",
            "Rank: 4 Relevant Document: ('hijab_286.json', 0.7430623769760132, '2022-02-19')\n",
            "Rank: 5 Relevant Document: ('hijab_287.json', 0.7378961443901062, '2022-02-19')\n",
            "Rank: 6 Relevant Document: ('hijab_290.json', 0.7282575368881226, '2022-02-19')\n",
            "Rank: 7 Relevant Document: ('MultiverseOfMadness_16.json', 0.7059887647628784, '2022-02-18')\n",
            "Rank: 8 Relevant Document: ('hijab_281.json', 0.7059272527694702, '2022-02-19')\n",
            "Rank: 9 Relevant Document: ('NationalDrinkWineDay_221.json', 0.6961204409599304, '2022-02-18')\n",
            "Rank: 10 Relevant Document: ('hijab_283.json', 0.694062352180481, '2022-02-19')\n",
            "Rank: 11 Relevant Document: ('SuperBowl_10.json', 0.6907355785369873, '2022-02-20')\n",
            "Rank: 12 Relevant Document: ('hijab_289.json', 0.6847090721130371, '2022-02-19')\n",
            "Rank: 13 Relevant Document: ('hijab_284.json', 0.6789780259132385, '2022-02-19')\n",
            "Rank: 14 Relevant Document: ('QueenElizabeth_154.json', 0.6769605278968811, '2022-02-20')\n",
            "Rank: 15 Relevant Document: ('DeepSidhu_77.json', 0.6729302406311035, '2022-02-18')\n",
            "Rank: 16 Relevant Document: ('Cyberpunk2077_57.json', 0.6551707983016968, '2022-02-18')\n",
            "Rank: 17 Relevant Document: ('TaylorSwift_237.json', 0.6478411555290222, '2022-02-18')\n",
            "Rank: 18 Relevant Document: ('vaccine_291.json', 0.6440983414649963, '2022-02-19')\n",
            "Rank: 19 Relevant Document: ('BoycottWalgreens_117.json', 0.6372767686843872, '2022-02-18')\n",
            "Rank: 20 Relevant Document: ('BoycottWalgreens_247.json', 0.6372767686843872, '2022-02-18')\n",
            "Rank: 21 Relevant Document: ('OperationDudula_267.json', 0.6346715688705444, '2022-02-20')\n",
            "Rank: 22 Relevant Document: ('vaccine_300.json', 0.629259467124939, '2022-02-19')\n",
            "Rank: 23 Relevant Document: ('OperationDudula_268.json', 0.6244097352027893, '2022-02-20')\n",
            "Rank: 24 Relevant Document: ('Eminem_23.json', 0.6194092035293579, '2022-02-18')\n",
            "Rank: 25 Relevant Document: ('BlandDoritos_84.json', 0.6143376231193542, '2022-02-18')\n",
            "Rank: 26 Relevant Document: ('BJPwinningUP_63.json', 0.6132742762565613, '2022-02-18')\n",
            "Rank: 27 Relevant Document: ('WriddhimanSaha_138.json', 0.613023042678833, '2022-02-20')\n",
            "Rank: 28 Relevant Document: ('stormfranklin_148.json', 0.6087393760681152, '2022-02-20')\n",
            "Rank: 29 Relevant Document: ('BoycottWalgreens_249.json', 0.6057480573654175, '2022-02-18')\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gensim/similarities/termsim.py:382: RuntimeWarning: divide by zero encountered in true_divide\n",
            "  normalized_corpus = np.multiply(corpus, 1.0 / corpus_norm)\n",
            "/usr/local/lib/python3.7/dist-packages/gensim/similarities/termsim.py:382: RuntimeWarning: invalid value encountered in multiply\n",
            "  normalized_corpus = np.multiply(corpus, 1.0 / corpus_norm)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mean_average_precision_hashtag_sc_plain = mean_average_precision(20, u_base_hashtag, u_time, relevant_docs_sc_plain, docs_preprocessed)\n",
        "print('Mean Average Precision Plain Model (Cosine Similarity Count Vectorizer) : {}'.format(mean_average_precision_hashtag_sc_plain))\n",
        "\n",
        "mean_average_recall_hashtag_sc_plain = mean_average_recall(20, u_base_hashtag, u_time, relevant_docs_sc_plain, docs_preprocessed)\n",
        "print('Mean Average Recall Plain Model (Cosine Similarity Count Vectorizer) : {}'.format(mean_average_recall_hashtag_sc_plain))"
      ],
      "metadata": {
        "id": "JizF_u4v__VR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d1d9ef15-ffc6-48b8-a097-9eed0486bbfb"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Average Precision Plain Model (Cosine Similarity Count Vectorizer) : 0.7866241575761699\n",
            "Mean Average Recall Plain Model (Cosine Similarity Count Vectorizer) : 0.36500000000000005\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# print(type(relevant_docs_sc_plain))\n",
        "# print(type(relevant_docs_sc_plain[0]))\n",
        "# print(type(relevant_docs_sc_plain[0][0]))"
      ],
      "metadata": {
        "id": "MZo_TYtN1_TB"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model with Keyword Extractor (Soft Cosine Similarity)"
      ],
      "metadata": {
        "id": "KRtKEUnGAkn7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Model with Keyword Extractor (Soft Cosine Similarity)\n",
        "relevant_docs_sc_keyword_extractor = find_relevant_documents_soft_cosine_similarity(docs_preprocessed, tweet_query_keyword_extractor)\n",
        "\n",
        "for rank, doc in enumerate(relevant_docs_sc_keyword_extractor):\n",
        "  print('Rank: {} Relevant Document: {}'.format(rank+1,doc))\n",
        "\n",
        "print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Bn0AhB-4CVI",
        "outputId": "e4ae4955-e9d1-4458-9a82-03ff2db6b592"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 12563/12563 [00:30<00:00, 408.19it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rank: 1 Relevant Document: ('SuperBowl_10.json', 1.0, '2022-02-20')\n",
            "Rank: 2 Relevant Document: ('MultiverseOfMadness_16.json', 0.9915482997894287, '2022-02-18')\n",
            "Rank: 3 Relevant Document: ('stormfranklin_141.json', 0.9676375985145569, '2022-02-20')\n",
            "Rank: 4 Relevant Document: ('stormfranklin_143.json', 0.961224377155304, '2022-02-20')\n",
            "Rank: 5 Relevant Document: ('Cyberpunk2077_58.json', 0.9401090145111084, '2022-02-18')\n",
            "Rank: 6 Relevant Document: ('stormfranklin_144.json', 0.9386342167854309, '2022-02-20')\n",
            "Rank: 7 Relevant Document: ('QueenElizabeth_160.json', 0.933012843132019, '2022-02-20')\n",
            "Rank: 8 Relevant Document: ('TaylorSwift_235.json', 0.9318116903305054, '2022-02-18')\n",
            "Rank: 9 Relevant Document: ('stormfranklin_150.json', 0.9291172027587891, '2022-02-20')\n",
            "Rank: 10 Relevant Document: ('WriddhimanSaha_133.json', 0.9269289970397949, '2022-02-20')\n",
            "Rank: 11 Relevant Document: ('OperationDudula_268.json', 0.925754964351654, '2022-02-20')\n",
            "Rank: 12 Relevant Document: ('Eminem_23.json', 0.9160871505737305, '2022-02-18')\n",
            "Rank: 13 Relevant Document: ('TaylorSwift_233.json', 0.9150249361991882, '2022-02-18')\n",
            "Rank: 14 Relevant Document: ('OperationDudula_269.json', 0.9134373068809509, '2022-02-20')\n",
            "Rank: 15 Relevant Document: ('QueenElizabeth_155.json', 0.9093219637870789, '2022-02-20')\n",
            "Rank: 16 Relevant Document: ('QueenElizabeth_151.json', 0.9077713489532471, '2022-02-20')\n",
            "Rank: 17 Relevant Document: ('BoycottWalgreens_246.json', 0.9076019525527954, '2022-02-18')\n",
            "Rank: 18 Relevant Document: ('BoycottWalgreens_116.json', 0.9076019525527954, '2022-02-18')\n",
            "Rank: 19 Relevant Document: ('Cyberpunk2077_57.json', 0.9066200256347656, '2022-02-18')\n",
            "Rank: 20 Relevant Document: ('BJPwinningUP_63.json', 0.9041388630867004, '2022-02-18')\n",
            "Rank: 21 Relevant Document: ('WriddhimanSaha_138.json', 0.9040865898132324, '2022-02-20')\n",
            "Rank: 22 Relevant Document: ('IPL_230.json', 0.9030429124832153, '2022-02-18')\n",
            "Rank: 23 Relevant Document: ('stormfranklin_147.json', 0.9030259251594543, '2022-02-20')\n",
            "Rank: 24 Relevant Document: ('IPL_278.json', 0.9021975994110107, '2022-02-19')\n",
            "Rank: 25 Relevant Document: ('hijab_284.json', 0.901602566242218, '2022-02-19')\n",
            "Rank: 26 Relevant Document: ('stormfranklin_146.json', 0.9012008309364319, '2022-02-20')\n",
            "Rank: 27 Relevant Document: ('IPL_274.json', 0.899117648601532, '2022-02-19')\n",
            "Rank: 28 Relevant Document: ('IPL_225.json', 0.8990942239761353, '2022-02-18')\n",
            "Rank: 29 Relevant Document: ('IPL_228.json', 0.8978896141052246, '2022-02-18')\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gensim/similarities/termsim.py:382: RuntimeWarning: divide by zero encountered in true_divide\n",
            "  normalized_corpus = np.multiply(corpus, 1.0 / corpus_norm)\n",
            "/usr/local/lib/python3.7/dist-packages/gensim/similarities/termsim.py:382: RuntimeWarning: invalid value encountered in multiply\n",
            "  normalized_corpus = np.multiply(corpus, 1.0 / corpus_norm)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mean_average_precision_hashtag_sc_keyword_extractor = mean_average_precision(20, u_base_hashtag, u_time, relevant_docs_sc_keyword_extractor, docs_preprocessed)\n",
        "print('Mean Average Precision Plain Model (Cosine Similarity Count Vectorizer) : {}'.format(mean_average_precision_hashtag_sc_keyword_extractor))\n",
        "\n",
        "mean_average_recall_hashtag_sc_keyword_extractor = mean_average_recall(20, u_base_hashtag, u_time, relevant_docs_sc_keyword_extractor, docs_preprocessed)\n",
        "print('Mean Average Recall Plain Model (Cosine Similarity Count Vectorizer) : {}'.format(mean_average_recall_hashtag_sc_keyword_extractor))"
      ],
      "metadata": {
        "id": "rfFw39WpAkOh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4bff5766-54bc-403d-a441-0a242608fb90"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Average Precision Plain Model (Cosine Similarity Count Vectorizer) : 0.0\n",
            "Mean Average Recall Plain Model (Cosine Similarity Count Vectorizer) : 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model with YAKE (Soft Cosine Similarity)"
      ],
      "metadata": {
        "id": "nx8vZj3KA5nZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Model with YAKE (Soft Cosine Similarity)\n",
        "relevant_docs_sc_yake = find_relevant_documents_soft_cosine_similarity(docs_preprocessed, tweet_keywords_yake)\n",
        "\n",
        "for rank, doc in enumerate(relevant_docs_sc_yake):\n",
        "  print('Rank: {} Relevant Document: {}'.format(rank+1,doc))\n",
        "\n",
        "print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XwLMwTWk4Ead",
        "outputId": "92e4d233-d16d-4985-cda1-efe6606b9416"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 12455/12455 [00:29<00:00, 416.13it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rank: 1 Relevant Document: ('stormfranklin_141.json', 1.0, '2022-02-20')\n",
            "Rank: 2 Relevant Document: ('ShivajiJayanti_204.json', 1.0, '2022-02-18')\n",
            "Rank: 3 Relevant Document: ('ScottyFromWelding_164.json', 1.0, '2022-02-20')\n",
            "Rank: 4 Relevant Document: ('CallTheMidwife_251.json', 1.0, '2022-02-20')\n",
            "Rank: 5 Relevant Document: ('ScottyFromWelding_170.json', 1.0, '2022-02-20')\n",
            "Rank: 6 Relevant Document: ('stormfranklin_150.json', 1.0, '2022-02-20')\n",
            "Rank: 7 Relevant Document: ('ScottyFromWelding_165.json', 1.0, '2022-02-20')\n",
            "Rank: 8 Relevant Document: ('stormfranklin_144.json', 1.0, '2022-02-20')\n",
            "Rank: 9 Relevant Document: ('CallTheMidwife_258.json', 1.0, '2022-02-20')\n",
            "Rank: 10 Relevant Document: ('JohnsonOut21_47.json', 1.0, '2022-02-20')\n",
            "Rank: 11 Relevant Document: ('PunjabElections2022_124.json', 1.0, '2022-02-20')\n",
            "Rank: 12 Relevant Document: ('BoycottWalgreens_241.json', 1.0, '2022-02-18')\n",
            "Rank: 13 Relevant Document: ('UkraineRussiaCrisis_264.json', 1.0, '2022-02-19')\n",
            "Rank: 14 Relevant Document: ('PunjabElections2022_127.json', 1.0, '2022-02-20')\n",
            "Rank: 15 Relevant Document: ('NationalDrinkWineDay_226.json', 1.0, '2022-02-18')\n",
            "Rank: 16 Relevant Document: ('OperationDudula_266.json', 1.0, '2022-02-20')\n",
            "Rank: 17 Relevant Document: ('BoycottWalgreens_111.json', 1.0, '2022-02-18')\n",
            "Rank: 18 Relevant Document: ('NationalDrinkWineDay_224.json', 1.0, '2022-02-18')\n",
            "Rank: 19 Relevant Document: ('Cyberpunk2077_57.json', 1.0, '2022-02-18')\n",
            "Rank: 20 Relevant Document: ('UkraineRussiaCrisis_217.json', 1.0, '2022-02-18')\n",
            "Rank: 21 Relevant Document: ('CallTheMidwife_255.json', 1.0, '2022-02-20')\n",
            "Rank: 22 Relevant Document: ('IndiaFightsCorona_246.json', 1.0, '2022-02-18')\n",
            "Rank: 23 Relevant Document: ('MultiverseOfMadness_16.json', 1.0, '2022-02-18')\n",
            "Rank: 24 Relevant Document: ('UkraineRussiaCrisis_215.json', 1.0, '2022-02-18')\n",
            "Rank: 25 Relevant Document: ('ScottyFromWelding_163.json', 0.9998844861984253, '2022-02-20')\n",
            "Rank: 26 Relevant Document: ('QueenElizabeth_155.json', 0.9995628595352173, '2022-02-20')\n",
            "Rank: 27 Relevant Document: ('CallTheMidwife_252.json', 0.9957855939865112, '2022-02-20')\n",
            "Rank: 28 Relevant Document: ('OperationDudula_269.json', 0.9946131706237793, '2022-02-20')\n",
            "Rank: 29 Relevant Document: ('JohnsonOut21_44.json', 0.9931861162185669, '2022-02-19')\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gensim/similarities/termsim.py:382: RuntimeWarning: divide by zero encountered in true_divide\n",
            "  normalized_corpus = np.multiply(corpus, 1.0 / corpus_norm)\n",
            "/usr/local/lib/python3.7/dist-packages/gensim/similarities/termsim.py:382: RuntimeWarning: invalid value encountered in multiply\n",
            "  normalized_corpus = np.multiply(corpus, 1.0 / corpus_norm)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mean_average_precision_hashtag_sc_yake = mean_average_precision(20, u_base_hashtag, u_time, relevant_docs_sc_yake, docs_preprocessed)\n",
        "print('Mean Average Precision YAKE Model (Soft Cosine Similarity) : {}'.format(mean_average_precision_hashtag_sc_yake))\n",
        "\n",
        "mean_average_recall_hashtag_sc_yake = mean_average_recall(20, u_base_hashtag, u_time, relevant_docs_sc_yake, docs_preprocessed)\n",
        "print('Mean Average Recall YAKE Model (Soft Cosine Similarity) : {}'.format(mean_average_recall_hashtag_sc_yake))"
      ],
      "metadata": {
        "id": "XmDA7wo7A_ih",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7cd6d24f-a1ff-41eb-e9f8-88305ac9e21c"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Average Precision YAKE Model (Soft Cosine Similarity) : 0.0\n",
            "Mean Average Recall YAKE Model (Soft Cosine Similarity) : 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Across all hashtags**"
      ],
      "metadata": {
        "id": "zxyJ3ooRzXwW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# global_list = [['narendramodi', '2022-02-14', 'india'],['UkraineRussiaCrisis', '2022-02-14', 'ukraine'],['IPL', '2022-02-14', 'india'],['TaylorSwift', '2022-02-14', 'USA'],['IndiaFightsCorona', '2022-02-14', 'india'],['narendramodi', '2022-02-15', 'india'],['UkraineRussiaCrisis', '2022-02-15', 'ukraine'],['IPL', '2022-02-15', 'india'],['TaylorSwift', '2022-02-15', 'USA'],['IndiaFightsCorona', '2022-02-15', 'india'],['narendramodi', '2022-02-16', 'india'],['UkraineRussiaCrisis', '2022-02-16', 'ukraine'],['IPL', '2022-02-16', 'india'],['TaylorSwift', '2022-02-16', 'USA'],['IndiaFightsCorona', '2022-02-16', 'india'],['narendramodi', '2022-02-17', 'india'],['UkraineRussiaCrisis', '2022-02-17', 'ukraine'],['IPL', '2022-02-17', 'india'],['TaylorSwift', '2022-02-17', 'USA'],['IndiaFightsCorona', '2022-02-17', 'india'],['narendramodi', '2022-02-18', 'india'],['UkraineRussiaCrisis', '2022-02-18', 'ukraine'],['IPL', '2022-02-18', 'india'],['TaylorSwift', '2022-02-18', 'USA'],['IndiaFightsCorona', '2022-02-18', 'india'],['narendramodi', '2022-02-19', 'india'],['UkraineRussiaCrisis', '2022-02-19', 'ukraine'],['IPL', '2022-02-19', 'india'],['hijab', '2022-02-19', 'india'],['vaccine', '2022-02-19', 'india'],['MillionAtIndiaPavilion', '2022-02-14', 'UAE'],['PunjabPanjeNaal', '2022-02-14', 'India'],['Euphoria', '2022-02-14', 'World'],['OscarsFanFavorite', '2022-02-14', 'World'],['ShameOnBirenSingh', '2022-02-14', 'india'],['BappiLahiri', '2022-02-16', 'india'],['BlandDoritos', '2022-02-16', 'USA'],['VERZUZ', '2022-02-16', 'USA'],['DragRaceUK', '2022-02-16', 'United Kingdom'],['BoycottWalgreens', '2022-02-18', 'USA'],['PunjabElections2022', '2022-02-20', 'india'],['WriddhimanSaha', '2022-02-20', 'india'],['stormfranklin', '2022-02-20', 'USA'],['QueenElizabeth', '2022-02-20', 'United Kingdom'],['ScottyFromWelding', '2022-02-20', 'Australia'],['CarabaoCupFinal', '2022-02-27', 'London'],['NZvSA', '2022-02-28', 'New Zealand'],['IPCC', '2022-02-28', 'Worldwide'],['SuperBowl', '2022-02-14', 'USA'],['MultiverseOfMadness', '2022-02-14', 'USA'],['Eminem', '2022-02-14', 'USA'],['IPLAuction', '2022-02-14', 'india'],['JohnsonOut21', '2022-02-14', 'United Kingdom'],['Cyberpunk2077', '2022-02-15', 'Worldwide'],['Wordle242', '2022-02-15', 'Worldwide'],['DeepSidhu', '2022-02-15', 'india'],['CanadaHasFallen', '2022-02-15', 'canada'],['IStandWithTrudeau', '2022-02-15', 'canada'],['CNNPHVPDebate', '2022-02-26', 'philippines'],['qldfloods', '2022-02-26', 'australia'],['Eurovision', '2022-02-26', 'worldwide'],['IndiansInUkraine', '2022-02-26', 'india'],['PritiPatel', '2022-02-26', 'united kingdom'],['TaylorCatterall', '2022-02-27', 'united kingdom'],['PSLFinal', '2022-02-27', 'pakistan'],['AustraliaDecides', '2022-02-27', 'australia'],['WorldNGODay', '2022-02-27', 'worldwide'],['TheBatman', '2022-02-28', 'USA'],['NationalScienceDay', '2022-02-28', 'india'],['msdtrong', '2022-02-14', 'india'],['Boycott_ChennaiSuperKings', '2022-02-14', 'india'],['GlanceJio', '2022-02-14', 'india'],['ArabicKuthu', '2022-02-14', 'india'],['Djokovic', '2022-02-15', 'australia'],['Real Madrid', '2022-02-15', 'santiago'],['bighit', '2022-02-15', 'korea'],['Maxwell', '2022-02-15', 'australia'],['mafsau', '2022-02-16', 'australia'],['channi', '2022-02-16', 'punjab'],['ayalaan', '2022-02-16', 'india'],['jkbose', '2022-02-16', 'india'],['HappyBirthdayPrinceSK', '2022-02-16', 'india'],['RandomActsOfKindnessDay', '2022-02-17', 'worldwide'],['happybirthdayjhope', '2022-02-17', 'korea'],['mohsinbaig', '2022-02-17', 'pakistan'],['aewdynamite', '2022-02-17', 'worldwide'],['aaraattu', '2022-02-17', 'india'],['ShivajiJayanti', '2022-02-18', 'india'],['PlotToKillModi', '2022-02-18', 'india'],['NationalDrinkWineDay', '2022-02-18', 'usa'],['HorizonForbiddenWest', '2022-02-18', 'worldwide'],['BoycottWalgreens', '2022-02-18', 'usa'],['CallTheMidwife', '2022-02-20', 'worldwide'],['OperationDudula', '2022-02-20', 'south africa'],['truthsocial', '2022-02-21', 'usa'],['nbaallstar', '2022-02-21', 'usa'],['shivamogga', '2022-02-21', 'india'],['HalftimeShow', '2022-02-14', 'usa'],['OttawaStrong', '2022-02-14', 'canada'],['DrDre', '2022-02-14', 'usa'],['BattleOfBillingsBridge', '2022-02-14', 'usa'],['FullyFaltooNFTdrop', '2022-02-14', 'worldwide'],['AK61', '2022-02-15', 'india'],['sandhyamukherjee', '2022-02-15', 'india'],['MUNBHA', '2022-02-15', 'worldwide'],['nursesstrike', '2022-02-15', 'australia'],['Realme9ProPlus', '2022-02-16', 'worldwide'],['KarnatakaHijabControversy', '2022-02-16', 'india'],['BJPwinningUP', '2022-02-16', 'india'],['Punjab_With_Modi', '2022-02-16', 'india'],['PushpaTheRule', '2022-02-16', 'india'],['RehmanMalik', '2022-02-22', 'india'],['harisrauf', '2022-02-22', 'pakistan'],['Rosettenville', '2022-02-22', 'south africa'],['NFU22', '2022-02-22', 'worldwide'],['justiceforharsha', '2022-02-22', 'india'],['wordle251', '2022-02-24', 'worldwide'],['ARSWOL', '2022-02-24', 'worldwide'],['stopwar', '2022-02-24', 'worldwide'],['PrayForPeace', '2022-02-24', 'worldwide'],['StopPutinNOW', '2022-02-24', 'worldwide'],['TeamGirlsCup', '2022-02-25', 'worldwide'],['Canucks', '2022-02-25', 'worldwide'],['PinkShirtDay', '2022-02-25', 'canada'],['superrugbypacific', '2022-02-25', 'australia']]"
      ],
      "metadata": {
        "id": "Ft7ith1b86zx"
      },
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# global_average_mean_average_precision_cs_cv = []\n",
        "# global_mean_average_recall_cs_cv = []\n",
        "\n",
        "# global_average_mean_average_precision_cs_tfidf = []\n",
        "# global_mean_average_recall_cs_tfidf = []\n",
        "\n",
        "# for iter in tqdm(range(len(global_list))):\n",
        "#   u_base_hashtag = global_list[iter][0]\n",
        "#   u_time = global_list[iter][1]\n",
        "#   u_location = global_list[iter][2]\n",
        "#   tweet_query = []\n",
        "#   format = '%Y-%m-%d'\n",
        "#   u_present_date = datetime.datetime.strptime(u_time, format)\n",
        "#   u_prev_date = u_present_date - datetime.timedelta(days=1)\n",
        "#   u_next_date = u_present_date + datetime.timedelta(days=1)\n",
        "#   df_query = df.loc[df['hashtags'].str.contains(u_base_hashtag) & df['Date_Only'].isin([str(u_present_date.date()), str(u_prev_date.date()), str(u_next_date.date())])]\n",
        "\n",
        "#   for tweet in df_query['Preprocessed_Data']:\n",
        "#     tweet_query.extend(tweet)\n",
        "  \n",
        "#   # tweet_keywords = []\n",
        "#   # kw_extractor = yake.KeywordExtractor(top=20, stopwords=None)\n",
        "#   # keywords = kw_extractor.extract_keywords(' '.join(tweet_query))\n",
        "#   # for kw, v in keywords:\n",
        "#   #   #print(\"Keyphrase: \",kw, \": score\", v)\n",
        "#   #   for key in kw.split():\n",
        "#   #     if(key not in tweet_keywords):\n",
        "#   #       tweet_keywords.append(key)\n",
        "  \n",
        "#   docs_preprocessed = []\n",
        "\n",
        "#   total_documents = 0\n",
        "#   path = '/content/drive/MyDrive/Tweelink_Dataset/Tweelink_Articles_Processed'\n",
        "#   for filename in glob(os.path.join(path, '*')):\n",
        "#     with open(os.path.join(os.getcwd(), filename), 'r', encoding = 'utf-8',errors = 'ignore') as f:\n",
        "#       filename = os.path.basename(f.name)\n",
        "#       data = json.load(f)\n",
        "#       d_date = data[\"Date\"]\n",
        "#       if(d_date==\"\" or d_date==\"Date\"):\n",
        "#         continue\n",
        "#       format = '%Y-%m-%d'\n",
        "  \n",
        "#       d_present_date = datetime.datetime.strptime(d_date, format)\n",
        "  \n",
        "#       if(str(d_present_date.date()) not in [str(u_present_date.date()), str(u_prev_date.date()), str(u_next_date.date())]):\n",
        "#         continue\n",
        "    \n",
        "#       docs_preprocessed.append({'Name':filename, 'Data':data})\n",
        "#       total_documents+=1\n",
        "  \n",
        "#   relevant_docs_cs_cv = find_relevant_documents_cosine_similarity_count_vectorizer(docs_preprocessed, tweet_query)\n",
        "#   mean_average_precision_hashtag_cs_cv = mean_average_precision(20, u_base_hashtag, u_time, relevant_docs_cs_cv, docs_preprocessed)\n",
        "#   global_average_mean_average_precision_cs_cv.append(mean_average_precision_hashtag_cs_cv)\n",
        "#   mean_average_recall_hashtag_cs_cv = mean_average_recall(20, u_base_hashtag, u_time, relevant_docs_cs_cv, docs_preprocessed)\n",
        "#   global_mean_average_recall_cs_cv.append(mean_average_recall_hashtag_cs_cv)\n",
        "\n",
        "#   relevant_docs_cs_tfidf = find_relevant_documents_cosine_similarity_tfidf_vectorizer(docs_preprocessed, tweet_query)\n",
        "#   mean_average_precision_hashtag_cs_tfidf = mean_average_precision(20, u_base_hashtag, u_time, relevant_docs_cs_tfidf, docs_preprocessed)\n",
        "#   global_average_mean_average_precision_cs_tfidf.append(mean_average_precision_hashtag_cs_tfidf)\n",
        "#   mean_average_recall_hashtag_cs_tfidf = mean_average_recall(20, u_base_hashtag, u_time, relevant_docs_cs_tfidf, docs_preprocessed)\n",
        "#   global_mean_average_recall_cs_tfidf.append(mean_average_recall_hashtag_cs_tfidf)"
      ],
      "metadata": {
        "id": "Ik-Hd_WC892W"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # cs cv\n",
        "# overall_average_mean_average_precision_cs_cv = sum(global_average_mean_average_precision_cs_cv)/len(global_average_mean_average_precision_cs_cv)\n",
        "# print(overall_average_mean_average_precision_cs_cv)\n",
        "\n",
        "# overall_mean_average_recall_cs_cv = sum(global_mean_average_recall_cs_cv)/len(global_mean_average_recall_cs_cv)\n",
        "# print(overall_mean_average_recall_cs_cv)"
      ],
      "metadata": {
        "id": "LP499ZPM9AGB"
      },
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # cs tfidf\n",
        "# overall_average_mean_average_precision_cs_tfidf = sum(global_average_mean_average_precision_cs_tfidf)/len(global_average_mean_average_precision_cs_tfidf)\n",
        "# print(overall_average_mean_average_precision_cs_tfidf)\n",
        "\n",
        "# overall_mean_average_recall_cs_tfidf = sum(global_mean_average_recall_cs_tfidf)/len(global_mean_average_recall_cs_tfidf)\n",
        "# print(overall_mean_average_recall_cs_tfidf)"
      ],
      "metadata": {
        "id": "QM0hy0wi9C2r"
      },
      "execution_count": 89,
      "outputs": []
    }
  ]
}