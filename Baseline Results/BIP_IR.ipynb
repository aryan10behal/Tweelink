{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A-6HD8Pe-_Sp",
        "outputId": "d016f876-c26a-4d38-d87d-9d22e7c1114c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EjXXowTxCgx7",
        "outputId": "9f514ff2-422c-4e95-af48-a457976168fa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/LIAAD/yake\n",
            "  Cloning https://github.com/LIAAD/yake to /tmp/pip-req-build-6ajlsfi_\n",
            "  Running command git clone -q https://github.com/LIAAD/yake /tmp/pip-req-build-6ajlsfi_\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.7/dist-packages (from yake==0.4.8) (0.8.9)\n",
            "Requirement already satisfied: click>=6.0 in /usr/local/lib/python3.7/dist-packages (from yake==0.4.8) (7.1.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from yake==0.4.8) (1.21.5)\n",
            "Requirement already satisfied: segtok in /usr/local/lib/python3.7/dist-packages (from yake==0.4.8) (1.5.11)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.7/dist-packages (from yake==0.4.8) (2.6.3)\n",
            "Requirement already satisfied: jellyfish in /usr/local/lib/python3.7/dist-packages (from yake==0.4.8) (0.9.0)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from segtok->yake==0.4.8) (2019.12.20)\n"
          ]
        }
      ],
      "source": [
        "! pip install git+https://github.com/LIAAD/yake\n",
        "import yake"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gShHsbJb_jDW"
      },
      "outputs": [],
      "source": [
        "file1 = open(\"/content/drive/MyDrive/IR_Tweets_Data/twitter_base_preprocessed.pkl\", \"rb\")\n",
        "df = pickle.load(file1)\n",
        "file1.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "npSDU9no-G68"
      },
      "outputs": [],
      "source": [
        "from collections import defaultdict\n",
        "from math import log, sqrt\n",
        "import re\n",
        "import numpy as np\n",
        "import sys\n",
        "from copy import deepcopy\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import csv\n",
        "import json\n",
        "from itertools import islice\n",
        "from collections import OrderedDict\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import os\n",
        "import nltk\n",
        "from glob import glob\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "from tqdm import tqdm\n",
        "import pickle\n",
        "import math\n",
        "from sklearn.model_selection import train_test_split\n",
        "import operator\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "import datetime\n",
        "\n",
        "\n",
        "# ## Import the data ad create the inverted index\n",
        "\n",
        "\n",
        "\n",
        "def import_dataset():\n",
        "    \"\"\"\n",
        "    This function import all the articles in the TIME corpus,\n",
        "    returning list of lists where each sub-list contains all the\n",
        "    terms present in the document as a string.\n",
        "    \"\"\"\n",
        "    # articles = []\n",
        "    # with open('TIME.ALL', 'r') as f:\n",
        "    #     tmp = []\n",
        "    #     for row in f:\n",
        "    #         if row.startswith(\"*TEXT\"):\n",
        "    #             if tmp != []:\n",
        "    #                 articles.append(tmp)\n",
        "    #             tmp = []\n",
        "    #         else:\n",
        "    #             row = re.sub(r'[^a-zA-Z\\s]+', '', row)\n",
        "    #             tmp += row.split()\n",
        "    # return articles\n",
        "\n",
        "    docs_preprocessed = []\n",
        "    path = '/content/drive/MyDrive/Tweelink_Articles_Processed'\n",
        "    for filename in glob(os.path.join(path, '*')):\n",
        "      with open(os.path.join(os.getcwd(), filename), 'r', encoding = 'utf-8',errors = 'ignore') as f:\n",
        "        filename = os.path.basename(f.name)\n",
        "        data = json.load(f)\n",
        "        d_date = data[\"Date\"]\n",
        "        if(d_date==\"\" or d_date==\"Date\"):\n",
        "          continue\n",
        "        format = '%Y-%m-%d'\n",
        "    \n",
        "        d_present_date = datetime.datetime.strptime(d_date, format)\n",
        "    \n",
        "        if(str(d_present_date.date()) not in [str(u_present_date.date()), str(u_prev_date.date()), str(u_next_date.date())]):\n",
        "          continue\n",
        "      \n",
        "        # docs_preprocessed.append({'Name':filename, 'Data':data})\n",
        "        docs_preprocessed.append(data['Body_processed'])\n",
        "    return docs_preprocessed\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# def remove_stop_words(corpus):\n",
        "#    '''\n",
        "#    This function removes from the corpus all the stop words present in the file TIME.STP\n",
        "#    '''\n",
        "#    stop_w = [line.rstrip('\\n') for line in open('TIME.STP')]\n",
        "#    stop_w=list(filter(None, stop_w))\n",
        "#    for i in range(0,len(corpus)):\n",
        "#        corpus[i] = [x for x in corpus[i] if x not in stop_w]\n",
        "#    return corpus \n",
        "\n",
        "\n",
        "def make_inverted_index(corpus):\n",
        "    \"\"\"\n",
        "    This function builds an inverted index as an hash table (dictionary)\n",
        "    where the keys are the terms and the values are ordered lists of\n",
        "    docIDs containing the term.\n",
        "    \"\"\"\n",
        "    # corpus = remove_stop_words(corpus)\n",
        "    index = defaultdict(set)\n",
        "    for docid, article in enumerate(corpus):\n",
        "        for term in article:\n",
        "            index[term].add(docid)\n",
        "    return index\n",
        "\n",
        "\n",
        "# ### Union of two posting lists\n",
        "\n",
        "\n",
        "def posting_lists_union(pl1, pl2):\n",
        "        \"\"\"\n",
        "        Returns a new posting list resulting from the union of the\n",
        "        two lists passed as arguments.\n",
        "        \"\"\"\n",
        "        pl1 = sorted(list(pl1))\n",
        "        pl2 = sorted(list(pl2))\n",
        "        union = []\n",
        "        i = 0\n",
        "        j = 0\n",
        "        while (i < len(pl1) and j < len(pl2)):\n",
        "            if (pl1[i] == pl2[j]):\n",
        "                union.append(pl1[i])\n",
        "                i += 1\n",
        "                j += 1\n",
        "            elif (pl1[i] < pl2[j]):\n",
        "                union.append(pl1[i])\n",
        "                i += 1\n",
        "            else:\n",
        "                union.append(pl2[j])\n",
        "                j += 1\n",
        "        for k in range(i, len(pl1)):\n",
        "            union.append(pl1[k])\n",
        "        for k in range(j, len(pl2)):\n",
        "            union.append(pl2[k])\n",
        "        return union\n",
        "\n",
        "\n",
        "# ## Precomputing weights\n",
        "\n",
        "\n",
        "def DF(term, index):\n",
        "    '''\n",
        "    Function computing Document Frequency for a term.\n",
        "    '''\n",
        "    return len(index[term])\n",
        "\n",
        "\n",
        "def IDF(term, index, corpus):\n",
        "    '''\n",
        "    Function computing Inverse Document Frequency for a term.\n",
        "    '''\n",
        "    return log(len(corpus)/DF(term, index))\n",
        "\n",
        "\n",
        "def RSV_weights(corpus,index):\n",
        "    '''\n",
        "    This function precomputes the Retrieval Status Value weights\n",
        "    for each term in the index\n",
        "    '''\n",
        "    N = len(corpus)\n",
        "    w = {}\n",
        "    for term in index.keys():\n",
        "        p = DF(term, index)/(N+0.5)  \n",
        "        w[term] = IDF(term, index, corpus) + log(p/(1-p))\n",
        "    return w\n",
        "    \n",
        "\n",
        "\n",
        "# ## BIM Class\n",
        "\n",
        "\n",
        "class BIM():\n",
        "    '''\n",
        "    Binary Independence Model class\n",
        "    '''\n",
        "    \n",
        "    def __init__(self, corpus):\n",
        "        self.original_corpus = deepcopy(corpus)\n",
        "        self.articles = corpus\n",
        "        self.index = make_inverted_index(self.articles)\n",
        "        self.weights = RSV_weights(self.articles, self.index)\n",
        "        self.ranked = []\n",
        "        self.query_text = ''\n",
        "        self.N_retrieved = 0\n",
        "    \n",
        "    \n",
        "        \n",
        "    def RSV_doc_query(self, doc_id, query):\n",
        "        '''\n",
        "        This function computes the Retrieval Status Value for a given couple document - query\n",
        "        using the precomputed weights\n",
        "        '''\n",
        "        score = 0\n",
        "        doc = self.articles[doc_id]\n",
        "        for term in doc:\n",
        "            if term in query:\n",
        "                score += self.weights[term]     \n",
        "        return score\n",
        "\n",
        "    \n",
        "        \n",
        "    def ranking(self, query):\n",
        "        '''\n",
        "        Auxiliary function for the function answer_query. Computes the score only for documents\n",
        "        that are in the posting list of al least one term in the query\n",
        "        '''\n",
        "\n",
        "        docs = []\n",
        "        for term in self.index: \n",
        "            if term in query:\n",
        "                docs = posting_lists_union(docs, self.index[term])\n",
        "                \n",
        "        scores = []\n",
        "        for doc in docs:\n",
        "            scores.append((doc, self.RSV_doc_query(doc, query)))\n",
        "        \n",
        "        self.ranked = sorted(scores, key=lambda x: x[1], reverse = True)\n",
        "        return self.ranked\n",
        "    \n",
        "    \n",
        "    \n",
        "    def recompute_weights(self, relevant_idx, query):\n",
        "        '''\n",
        "        Auxiliary function for relevance_feedback function and\n",
        "        for the pseudo relevance feedback in answer_query function.\n",
        "        Recomputes the weights, only for the terms in the query\n",
        "        based on a set of relevant documents.\n",
        "        '''\n",
        "        \n",
        "        relevant_docs = []\n",
        "        for idx in relevant_idx:\n",
        "            doc_id = self.ranked[idx-1][0]\n",
        "            relevant_docs.append(self.articles[doc_id])\n",
        "        \n",
        "        N = len(self.articles)\n",
        "        N_rel = len(relevant_idx)\n",
        "        \n",
        "        for term in query:\n",
        "            if term in self.weights.keys():\n",
        "                vri = 0\n",
        "                for doc in relevant_docs:\n",
        "                    if term in doc:\n",
        "                        vri += 1\n",
        "                p = (vri + 0.5) /( N_rel + 1)\n",
        "                u = (DF(term, self.index) - vri + 0.5) / (N - N_rel +1)\n",
        "                self.weights[term] = log((1-u)/u) + log(p/(1-p))\n",
        "\n",
        "            \n",
        "    \n",
        "    def answer_query(self, query_text):\n",
        "        '''\n",
        "        Function to answer a free text query. Shows the first 30 words of the\n",
        "        15 most relevant documents. \n",
        "        Also implements the pseudo relevance feedback with k = 5\n",
        "        '''\n",
        "        \n",
        "        self.query_text = query_text\n",
        "        query =  query_text.upper().split()\n",
        "        ranking = self.ranking(query)\n",
        "        \n",
        "        ## pseudo relevance feedback \n",
        "        i = 0\n",
        "        new_ranking=[]\n",
        "        while i<10 and ranking != new_ranking:\n",
        "            self.recompute_weights([1,2,3,4,5], query)\n",
        "            new_ranking = self.ranking(query)\n",
        "            i+=1\n",
        "        \n",
        "        ranking = new_ranking\n",
        "        \n",
        "        self.N_retrieved = 15\n",
        "        \n",
        "        ## print retrieved documents\n",
        "        print(len(ranking))\n",
        "        for i in range(0, self.N_retrieved):\n",
        "            \n",
        "            article = self.original_corpus[ranking[i][0]]\n",
        "            if (len(article) > 30):\n",
        "                article = article[0:30]\n",
        "            text = \" \".join(article)\n",
        "            print(f\"Article {i + 1}, score: {ranking[i][1]}\")\n",
        "            print(text, '\\n')\n",
        "\n",
        "        self.weights = RSV_weights(self.articles, self.index)\n",
        "\n",
        "\n",
        "            \n",
        "    def relevance_feedback(self, *args):\n",
        "        '''\n",
        "        Function that implements relevance feedback for the last query formulated.\n",
        "        The set of relevant documents is given by the user through a set of indexes\n",
        "        '''\n",
        "        if(self.query_text == ''):\n",
        "            print('Cannot get feedback before a query is formulated.')\n",
        "            return\n",
        "        \n",
        "        relevant_idx = list(args)\n",
        "        \n",
        "        if(isinstance(relevant_idx[0], list)):\n",
        "            relevant_idx = relevant_idx[0]\n",
        "        \n",
        "        query = self.query_text.upper().split()\n",
        "        self.recompute_weights(relevant_idx,query)\n",
        "        \n",
        "        self.answer_query(self.query_text)\n",
        "    \n",
        "    \n",
        "    \n",
        "    def read_document(self,rank_num):\n",
        "        '''\n",
        "        Function that allows the user to select a document among the ones returned \n",
        "        by answer_query and read the whole text\n",
        "        '''\n",
        "        if(self.query_text == ''):\n",
        "            print('Cannot select a document before a query is formulated.')\n",
        "            return\n",
        "            \n",
        "        article = self.original_corpus[self.ranked[rank_num - 1][0]]\n",
        "        text = \" \".join(article)\n",
        "        print(f\"Article {rank_num}, score: {self.ranked[rank_num][1]}\")\n",
        "        print(text, '\\n')\n",
        "        \n",
        "        \n",
        "    def show_more(self):\n",
        "        '''\n",
        "        Function that allows the user to see more 10 retrieved documents\n",
        "        '''\n",
        "        \n",
        "        if(self.N_retrieved + 10 > len(self.ranked)):\n",
        "            print('No more documents available')\n",
        "            return \n",
        "        \n",
        "        for i in range(self.N_retrieved, self.N_retrieved+10):\n",
        "            article = self.original_corpus[self.ranked[i][0]]\n",
        "            if (len(article) > 30):\n",
        "                article = article[0:30]\n",
        "            text = \" \".join(article)\n",
        "            print(f\"Article {i + 1}, score: {self.ranked[i][1]}\")\n",
        "            print(text, '\\n')\n",
        "        \n",
        "        self.N_retrieved += 10 \n",
        "        \n",
        "\n",
        "\n",
        "\n",
        "# Example of usage\n",
        "\n",
        "# articles = import_dataset()\n",
        "# bim  = BIM(articles)\n",
        "# bim.answer_query('Italy and Great Britain fight the enemy')\n",
        "# bim.relevance_feedback(5,6,8)\n",
        "# bim.show_more()\n",
        "# bim.read_document(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FqZ4Z7JT_oIz",
        "outputId": "29f8b0d5-17eb-46ee-b874-cab73d0f04a4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter base hashtag: hijab\n",
            "Enter time: 2022-02-19\n",
            "Enter Location: India\n"
          ]
        }
      ],
      "source": [
        "u_base_hashtag = input(\"Enter base hashtag: \")\n",
        "u_time = input(\"Enter time: \")\n",
        "u_location = input(\"Enter Location: \")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yGrY-qLq_qSc"
      },
      "outputs": [],
      "source": [
        "tweet_query = []\n",
        "format = '%Y-%m-%d'\n",
        "u_present_date = datetime.datetime.strptime(u_time, format)\n",
        "u_prev_date = u_present_date - datetime.timedelta(days=1)\n",
        "u_next_date = u_present_date + datetime.timedelta(days=1)\n",
        "df_query = df.loc[df['hashtags'].str.contains(u_base_hashtag) & df['Date_Only'].isin([str(u_present_date.date()), str(u_prev_date.date()), str(u_next_date.date())])]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pz867JVX_rxA"
      },
      "outputs": [],
      "source": [
        "for tweet in df_query['Preprocessed_Data']:\n",
        "  tweet_query.extend(tweet)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wdCaO1wPClna",
        "outputId": "cf0fe483-bae2-43f9-93ed-b121fb253437"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['hijab', 'karnataka', 'india', 'muslim', 'islam', 'hijabisfundamentalright', 'hijabisourright', 'hijabcontroversy', 'started', 'abaya', 'hijabrow', 'http', 'persecution', 'wear', 'allah', 'woman', 'karnatakahijabcontroversy', 'wearing', 'school', 'world', 'modestfashion', 'quran', 'hijabisindividualright', 'judge', 'modest', 'deen', 'prophetmuhammad', 'dua', 'makkah', 'college', 'hijabban', 'religion', 'china', 'remove', 'islamophobia', 'girl', 'hindu', 'education', 'islamist', 'hijabisourpride', 'hijaboruniform', 'student', 'permission', 'beard', 'freedom', 'asian', 'allowed', 'public', 'saudi', 'hijabplot', 'hijabaurkitab', 'class', 'niqab', 'muslimmen', 'leader']\n"
          ]
        }
      ],
      "source": [
        "tweet_keywords = []\n",
        "kw_extractor = yake.KeywordExtractor(top=100, stopwords=None)\n",
        "keywords = kw_extractor.extract_keywords(' '.join(tweet_query))\n",
        "for kw, v in keywords:\n",
        "  # print(\"Keyphrase: \",kw, \": score\", v)\n",
        "  for key in kw.split():\n",
        "    if(key not in tweet_keywords):\n",
        "      tweet_keywords.append(key)\n",
        "\n",
        "print(tweet_keywords)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UZa8mrhMOji-"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "id": "PblKJeWT_8YP",
        "outputId": "aee34d18-1b4b-41a2-d5e6-58cd9c79e2b2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0\n"
          ]
        },
        {
          "ename": "IndexError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-69-a11e9ffe5fd1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0marticles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimport_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mbim\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mBIM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marticles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mbim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0manswer_query\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtweet_keywords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;31m# bim.relevance_feedback(5,6,8)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# bim.show_more()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-64-a3478d7ccd47>\u001b[0m in \u001b[0;36manswer_query\u001b[0;34m(self, query_text)\u001b[0m\n\u001b[1;32m    273\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mN_retrieved\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 275\u001b[0;31m             \u001b[0marticle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moriginal_corpus\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mranking\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    276\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marticle\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m                 \u001b[0marticle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marticle\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: list index out of range"
          ]
        }
      ],
      "source": [
        "# Example of usage\n",
        "\n",
        "articles = import_dataset()\n",
        "bim  = BIM(articles)\n",
        "bim.answer_query(\" \".join(tweet_keywords))\n",
        "# bim.relevance_feedback(5,6,8)\n",
        "# bim.show_more()\n",
        "# bim.read_document(2)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "BIP_IR.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
