{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A-6HD8Pe-_Sp",
        "outputId": "ffef36f3-0dd0-4b34-c4a7-3412715fe7bd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EjXXowTxCgx7",
        "outputId": "b5b7037a-a94e-4009-beca-cda17546d3ab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/LIAAD/yake\n",
            "  Cloning https://github.com/LIAAD/yake to /tmp/pip-req-build-kvhhxsk5\n",
            "  Running command git clone -q https://github.com/LIAAD/yake /tmp/pip-req-build-kvhhxsk5\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.7/dist-packages (from yake==0.4.8) (0.8.9)\n",
            "Requirement already satisfied: click>=6.0 in /usr/local/lib/python3.7/dist-packages (from yake==0.4.8) (7.1.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from yake==0.4.8) (1.21.5)\n",
            "Requirement already satisfied: segtok in /usr/local/lib/python3.7/dist-packages (from yake==0.4.8) (1.5.11)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.7/dist-packages (from yake==0.4.8) (2.6.3)\n",
            "Requirement already satisfied: jellyfish in /usr/local/lib/python3.7/dist-packages (from yake==0.4.8) (0.9.0)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from segtok->yake==0.4.8) (2019.12.20)\n"
          ]
        }
      ],
      "source": [
        "! pip install git+https://github.com/LIAAD/yake\n",
        "import yake"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "from math import log, sqrt\n",
        "import re\n",
        "import numpy as np\n",
        "import sys\n",
        "from copy import deepcopy\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import csv\n",
        "import json\n",
        "from itertools import islice\n",
        "from collections import OrderedDict\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import os\n",
        "import nltk\n",
        "from glob import glob\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "from tqdm import tqdm\n",
        "import pickle\n",
        "import math\n",
        "from sklearn.model_selection import train_test_split\n",
        "import operator\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "import datetime"
      ],
      "metadata": {
        "id": "G49FIo8uLD8j"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "gShHsbJb_jDW"
      },
      "outputs": [],
      "source": [
        "file1 = open(\"/content/drive/MyDrive/Tweelink_Dataset/twitter_base_preprocessed.pkl\", \"rb\")\n",
        "df = pickle.load(file1)\n",
        "file1.close()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_relevant_docs_list_for_base_hashtag(base_hashtag, base_date, docs_preprocessed):\n",
        "  relevant_docs_list = []\n",
        "  for doc in docs_preprocessed:\n",
        "    if doc['Data']['Base Hashtag']==base_hashtag:\n",
        "      current_date = datetime.datetime.strptime(base_date, format)\n",
        "      prev_date = current_date - datetime.timedelta(days=1)\n",
        "      next_date = current_date + datetime.timedelta(days=1)\n",
        "      if(doc['Data']['Date'] in [str(prev_date.date()), str(current_date.date()), str(next_date.date())]):\n",
        "        relevant_docs_list.append(doc['Name'])\n",
        "  return relevant_docs_list"
      ],
      "metadata": {
        "id": "FRVSYuVjT3cm"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def precision_at_k(k, base_hashtag, base_date, prediction_list, docs_preprocessed):\n",
        "  relevant_docs_list = get_relevant_docs_list_for_base_hashtag(base_hashtag, base_date, docs_preprocessed)\n",
        "  num_of_relevant_results=0\n",
        "  for itr in range(k):\n",
        "    if (prediction_list[itr] in relevant_docs_list):\n",
        "      num_of_relevant_results+=1\n",
        "  return num_of_relevant_results/k"
      ],
      "metadata": {
        "id": "uiu_FtqqT4GS"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def mean_average_precision(max_k, base_hashtag, base_date, relevant_docs, docs_preprocessed):\n",
        "  average_precision=0\n",
        "  ctr=0\n",
        "  for k_val in range(1,max_k+1):\n",
        "    ctr+=1\n",
        "    precision_at_k_val = precision_at_k(k_val, base_hashtag, base_date, relevant_docs, docs_preprocessed)\n",
        "    print('Hashtag: {}   Precision@{}: {}'.format(base_hashtag, k_val, precision_at_k_val))\n",
        "    average_precision += precision_at_k_val\n",
        "  return average_precision/ctr"
      ],
      "metadata": {
        "id": "4Q6W2l8-T6G3"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def recall_at_k(k, base_hashtag, base_date, prediction_list, docs_preprocessed):\n",
        "  relevant_docs_list = get_relevant_docs_list_for_base_hashtag(base_hashtag, base_date, docs_preprocessed)\n",
        "  current_num_of_relevant_results=0\n",
        "  for itr in range(k):\n",
        "    if (prediction_list[itr] in relevant_docs_list):\n",
        "      current_num_of_relevant_results+=1\n",
        "  if(len(relevant_docs_list)==0):\n",
        "    return 0\n",
        "  return current_num_of_relevant_results/len(relevant_docs_list)"
      ],
      "metadata": {
        "id": "cihke9X6T7oh"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def mean_average_recall(max_k, base_hashtag, base_date, relevant_docs, docs_preprocessed):\n",
        "  average_recall=0\n",
        "  ctr=0\n",
        "  for k_val in range(1,max_k+1):\n",
        "    ctr+=1\n",
        "    recall_at_k_val = recall_at_k(k_val, base_hashtag, base_date, relevant_docs, docs_preprocessed)\n",
        "    print('Hashtag: {}   Recall@{}: {}'.format(base_hashtag, k_val, recall_at_k_val))\n",
        "    average_recall += recall_at_k_val\n",
        "  return average_recall/ctr"
      ],
      "metadata": {
        "id": "B52GnqVwT9Sf"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "npSDU9no-G68"
      },
      "outputs": [],
      "source": [
        "# ## Import the data ad create the inverted index\n",
        "\n",
        "def import_dataset():\n",
        "    \"\"\"\n",
        "    This function import all the articles in the TIME corpus,\n",
        "    returning list of lists where each sub-list contains all the\n",
        "    terms present in the document as a string.\n",
        "    \"\"\"\n",
        "    # articles = []\n",
        "    # with open('TIME.ALL', 'r') as f:\n",
        "    #     tmp = []\n",
        "    #     for row in f:\n",
        "    #         if row.startswith(\"*TEXT\"):\n",
        "    #             if tmp != []:\n",
        "    #                 articles.append(tmp)\n",
        "    #             tmp = []\n",
        "    #         else:\n",
        "    #             row = re.sub(r'[^a-zA-Z\\s]+', '', row)\n",
        "    #             tmp += row.split()\n",
        "    # return articles\n",
        "\n",
        "    docs_preprocessed_metrics = []\n",
        "    docs_preprocessed = []\n",
        "    docs_preprocessed_with_names = []\n",
        "    path = '/content/drive/MyDrive/Tweelink_Dataset/Tweelink_Articles_Processed'\n",
        "    for filename in glob(os.path.join(path, '*')):\n",
        "      with open(os.path.join(os.getcwd(), filename), 'r', encoding = 'utf-8',errors = 'ignore') as f:\n",
        "        filename = os.path.basename(f.name)\n",
        "        data = json.load(f)\n",
        "        d_date = data[\"Date\"]\n",
        "        if(d_date==\"\" or d_date==\"Date\"):\n",
        "          continue\n",
        "        format = '%Y-%m-%d'\n",
        "    \n",
        "        d_present_date = datetime.datetime.strptime(d_date, format)\n",
        "    \n",
        "        if(str(d_present_date.date()) not in [str(u_present_date.date()), str(u_prev_date.date()), str(u_next_date.date())]):\n",
        "          continue\n",
        "      \n",
        "        docs_preprocessed_metrics.append({'Name':filename, 'Data':data})\n",
        "        docs_preprocessed.append(data['Body_processed'])\n",
        "        docs_preprocessed_with_names.append(filename)\n",
        "\n",
        "    return docs_preprocessed, docs_preprocessed_with_names, docs_preprocessed_metrics\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# def remove_stop_words(corpus):\n",
        "#    '''\n",
        "#    This function removes from the corpus all the stop words present in the file TIME.STP\n",
        "#    '''\n",
        "#    stop_w = [line.rstrip('\\n') for line in open('TIME.STP')]\n",
        "#    stop_w=list(filter(None, stop_w))\n",
        "#    for i in range(0,len(corpus)):\n",
        "#        corpus[i] = [x for x in corpus[i] if x not in stop_w]\n",
        "#    return corpus \n",
        "\n",
        "\n",
        "def make_inverted_index(corpus):\n",
        "    \"\"\"\n",
        "    This function builds an inverted index as an hash table (dictionary)\n",
        "    where the keys are the terms and the values are ordered lists of\n",
        "    docIDs containing the term.\n",
        "    \"\"\"\n",
        "    # corpus = remove_stop_words(corpus)\n",
        "    index = defaultdict(set)\n",
        "    for docid, article in enumerate(corpus):\n",
        "        for term in article:\n",
        "            index[term].add(docid)\n",
        "    return index\n",
        "\n",
        "\n",
        "# ### Union of two posting lists\n",
        "\n",
        "\n",
        "def posting_lists_union(pl1, pl2):\n",
        "        \"\"\"\n",
        "        Returns a new posting list resulting from the union of the\n",
        "        two lists passed as arguments.\n",
        "        \"\"\"\n",
        "        pl1 = sorted(list(pl1))\n",
        "        pl2 = sorted(list(pl2))\n",
        "        union = []\n",
        "        i = 0\n",
        "        j = 0\n",
        "        while (i < len(pl1) and j < len(pl2)):\n",
        "            if (pl1[i] == pl2[j]):\n",
        "                union.append(pl1[i])\n",
        "                i += 1\n",
        "                j += 1\n",
        "            elif (pl1[i] < pl2[j]):\n",
        "                union.append(pl1[i])\n",
        "                i += 1\n",
        "            else:\n",
        "                union.append(pl2[j])\n",
        "                j += 1\n",
        "        for k in range(i, len(pl1)):\n",
        "            union.append(pl1[k])\n",
        "        for k in range(j, len(pl2)):\n",
        "            union.append(pl2[k])\n",
        "        return union\n",
        "\n",
        "\n",
        "# ## Precomputing weights\n",
        "\n",
        "\n",
        "def DF(term, index):\n",
        "    '''\n",
        "    Function computing Document Frequency for a term.\n",
        "    '''\n",
        "    return len(index[term])\n",
        "\n",
        "\n",
        "def IDF(term, index, corpus):\n",
        "    '''\n",
        "    Function computing Inverse Document Frequency for a term.\n",
        "    '''\n",
        "    return log(len(corpus)/DF(term, index))\n",
        "\n",
        "\n",
        "def RSV_weights(corpus,index):\n",
        "    '''\n",
        "    This function precomputes the Retrieval Status Value weights\n",
        "    for each term in the index\n",
        "    '''\n",
        "    N = len(corpus)\n",
        "    w = {}\n",
        "    for term in index.keys():\n",
        "        p = DF(term, index)/(N+0.5)  \n",
        "        w[term] = IDF(term, index, corpus) + log(p/(1-p))\n",
        "    return w\n",
        "    \n",
        "\n",
        "\n",
        "# ## BIM Class\n",
        "\n",
        "\n",
        "class BIM():\n",
        "    '''\n",
        "    Binary Independence Model class\n",
        "    '''\n",
        "    \n",
        "    def __init__(self, corpus, articles_with_name):\n",
        "        self.original_corpus = deepcopy(corpus)\n",
        "        self.articles_with_name = articles_with_name\n",
        "        self.articles = corpus\n",
        "        self.index = make_inverted_index(self.articles)\n",
        "        self.weights = RSV_weights(self.articles, self.index)\n",
        "        self.ranked = []\n",
        "        self.query_text = ''\n",
        "        self.N_retrieved = 0\n",
        "    \n",
        "        \n",
        "    def RSV_doc_query(self, doc_id, query):\n",
        "        '''\n",
        "        This function computes the Retrieval Status Value for a given couple document - query\n",
        "        using the precomputed weights\n",
        "        '''\n",
        "        score = 0\n",
        "        doc = self.articles[doc_id]\n",
        "        for term in doc:\n",
        "            if term in query:\n",
        "                score += self.weights[term]     \n",
        "        return score\n",
        "\n",
        "    \n",
        "        \n",
        "    def ranking(self, query):\n",
        "        '''\n",
        "        Auxiliary function for the function answer_query. Computes the score only for documents\n",
        "        that are in the posting list of al least one term in the query\n",
        "        '''\n",
        "\n",
        "        docs = []\n",
        "        for term in self.index: \n",
        "            # print(term in query, query, term)\n",
        "            if term in query:\n",
        "                docs = posting_lists_union(docs, self.index[term])\n",
        "\n",
        "        scores = []\n",
        "        for doc in docs:\n",
        "            scores.append((doc, self.RSV_doc_query(doc, query)))\n",
        "        \n",
        "        self.ranked = sorted(scores, key=lambda x: x[1], reverse = True)\n",
        "        return self.ranked\n",
        "    \n",
        "    \n",
        "    \n",
        "    def recompute_weights(self, relevant_idx, query):\n",
        "        '''\n",
        "        Auxiliary function for relevance_feedback function and\n",
        "        for the pseudo relevance feedback in answer_query function.\n",
        "        Recomputes the weights, only for the terms in the query\n",
        "        based on a set of relevant documents.\n",
        "        '''\n",
        "        \n",
        "        relevant_docs = []\n",
        "        for idx in relevant_idx:\n",
        "            doc_id = self.ranked[idx-1][0]\n",
        "            relevant_docs.append(self.articles[doc_id])\n",
        "        \n",
        "        N = len(self.articles)\n",
        "        N_rel = len(relevant_idx)\n",
        "        \n",
        "        for term in query:\n",
        "            if term in self.weights.keys():\n",
        "                vri = 0\n",
        "                for doc in relevant_docs:\n",
        "                    if term in doc:\n",
        "                        vri += 1\n",
        "                p = (vri + 0.5) /( N_rel + 1)\n",
        "                u = (DF(term, self.index) - vri + 0.5) / (N - N_rel +1)\n",
        "                self.weights[term] = log((1-u)/u) + log(p/(1-p))\n",
        "\n",
        "            \n",
        "    \n",
        "    def answer_query(self, query_text):\n",
        "        '''\n",
        "        Function to answer a free text query. Shows the first 30 words of the\n",
        "        15 most relevant documents. \n",
        "        Also implements the pseudo relevance feedback with k = 5\n",
        "        '''\n",
        "        \n",
        "        self.query_text = query_text\n",
        "        query =  query_text.lower().split()\n",
        "        ranking = self.ranking(query)\n",
        "\n",
        "        ## pseudo relevance feedback \n",
        "        i = 0\n",
        "        new_ranking=[]\n",
        "        while i<10 and ranking != new_ranking:\n",
        "            self.recompute_weights([1,2,3,4,5], query)\n",
        "            new_ranking = self.ranking(query)\n",
        "            i+=1\n",
        "\n",
        "\n",
        "        ranking = new_ranking\n",
        "        \n",
        "        self.N_retrieved = 20\n",
        "        \n",
        "        ## print retrieved documents\n",
        "        ret_list = []\n",
        "        for i in range(0, self.N_retrieved):\n",
        "            article = self.original_corpus[ranking[i][0]]\n",
        "            if (len(article) > 30):\n",
        "                article = article[0:30]\n",
        "            text = \" \".join(article)\n",
        "            print(f\"Article {self.articles_with_name[self.ranked[i][0]]}, score: {ranking[i][1]}\")\n",
        "            ret_list.append(self.articles_with_name[self.ranked[i][0]])\n",
        "            # print(text, '\\n')\n",
        "\n",
        "        self.weights = RSV_weights(self.articles, self.index)\n",
        "        return ret_list\n",
        "\n",
        "\n",
        "            \n",
        "    def relevance_feedback(self, *args):\n",
        "        '''\n",
        "        Function that implements relevance feedback for the last query formulated.\n",
        "        The set of relevant documents is given by the user through a set of indexes\n",
        "        '''\n",
        "        if(self.query_text == ''):\n",
        "            print('Cannot get feedback before a query is formulated.')\n",
        "            return\n",
        "        \n",
        "        relevant_idx = list(args)\n",
        "        \n",
        "        if(isinstance(relevant_idx[0], list)):\n",
        "            relevant_idx = relevant_idx[0]\n",
        "        \n",
        "        query = self.query_text.upper().split()\n",
        "        self.recompute_weights(relevant_idx,query)\n",
        "        \n",
        "        self.answer_query(self.query_text)\n",
        "    \n",
        "    \n",
        "    def read_document(self,rank_num):\n",
        "        '''\n",
        "        Function that allows the user to select a document among the ones returned \n",
        "        by answer_query and read the whole text\n",
        "        '''\n",
        "        if(self.query_text == ''):\n",
        "            print('Cannot select a document before a query is formulated.')\n",
        "            return\n",
        "            \n",
        "        article = self.original_corpus[self.ranked[rank_num - 1][0]]\n",
        "        text = \" \".join(article)\n",
        "        print(f\"Article {rank_num}, score: {self.ranked[rank_num][1]}\")\n",
        "        print(text, '\\n')\n",
        "        \n",
        "        \n",
        "    def show_more(self):\n",
        "        '''\n",
        "        Function that allows the user to see more 10 retrieved documents\n",
        "        '''\n",
        "        \n",
        "        if(self.N_retrieved + 10 > len(self.ranked)):\n",
        "            print('No more documents available')\n",
        "            return \n",
        "        \n",
        "        for i in range(self.N_retrieved, self.N_retrieved+10):\n",
        "            article = self.original_corpus[self.ranked[i][0]]\n",
        "            if (len(article) > 30):\n",
        "                article = article[0:30]\n",
        "            text = \" \".join(article)\n",
        "            print(f\"Article {self.articles_with_name[self.ranked[i][0]]}, score: {self.ranked[i][1]}\")\n",
        "            print(text, '\\n')\n",
        "        \n",
        "        self.N_retrieved += 10 \n",
        "        \n",
        "\n",
        "\n",
        "\n",
        "# Example of usage\n",
        "\n",
        "# articles, articles_with_name = import_dataset()\n",
        "# bim  = BIM(articles, articles_with_name)\n",
        "# bim.answer_query('Italy and Great Britain fight the enemy')\n",
        "# bim.relevance_feedback(5,6,8)\n",
        "# bim.show_more()\n",
        "# bim.read_document(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FqZ4Z7JT_oIz",
        "outputId": "dcd56418-7118-48ca-f792-da9c0e8c9b01"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter base hashtag: hijab\n",
            "Enter time: 2022-02-19\n",
            "Enter Location: india\n"
          ]
        }
      ],
      "source": [
        "u_base_hashtag = input(\"Enter base hashtag: \")\n",
        "u_time = input(\"Enter time: \")\n",
        "u_location = input(\"Enter Location: \")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "yGrY-qLq_qSc"
      },
      "outputs": [],
      "source": [
        "tweet_query = []\n",
        "format = '%Y-%m-%d'\n",
        "u_present_date = datetime.datetime.strptime(u_time, format)\n",
        "u_prev_date = u_present_date - datetime.timedelta(days=1)\n",
        "u_next_date = u_present_date + datetime.timedelta(days=1)\n",
        "df_query = df.loc[df['hashtags'].str.contains(u_base_hashtag) & df['Date_Only'].isin([str(u_present_date.date()), str(u_prev_date.date()), str(u_next_date.date())])]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "pz867JVX_rxA"
      },
      "outputs": [],
      "source": [
        "for tweet in df_query['Preprocessed_Data']:\n",
        "  tweet_query.extend(tweet)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wdCaO1wPClna",
        "outputId": "3510a9eb-0c25-4c7b-d7c4-3f09cb78a742"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['hijab', 'karnataka', 'india', 'muslim', 'islam', 'hijabisfundamentalright', 'hijabisourright', 'hijabcontroversy', 'started', 'abaya', 'hijabrow', 'http', 'persecution', 'wear', 'allah', 'woman', 'karnatakahijabcontroversy', 'wearing', 'school', 'world', 'modestfashion', 'quran', 'hijabisindividualright', 'judge', 'modest', 'deen', 'prophetmuhammad', 'dua', 'makkah', 'college', 'hijabban', 'religion', 'china', 'remove', 'islamophobia', 'girl', 'hindu', 'education', 'islamist', 'hijabisourpride', 'hijaboruniform', 'student', 'permission', 'beard', 'freedom', 'asian', 'allowed', 'public', 'saudi', 'hijabplot', 'hijabaurkitab', 'class', 'niqab', 'muslimmen', 'leader']\n"
          ]
        }
      ],
      "source": [
        "tweet_keywords = []\n",
        "kw_extractor = yake.KeywordExtractor(top=100, stopwords=None)\n",
        "keywords = kw_extractor.extract_keywords(' '.join(tweet_query))\n",
        "for kw, v in keywords:\n",
        "  # print(\"Keyphrase: \",kw, \": score\", v)\n",
        "  for key in kw.split():\n",
        "    if(key not in tweet_keywords):\n",
        "      tweet_keywords.append(key)\n",
        "\n",
        "print(tweet_keywords)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example of usage\n",
        "articles, articles_with_name, docs_preprocessed_metrics = import_dataset()"
      ],
      "metadata": {
        "id": "u_GxOwk2G9-5"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#results with normal query\n",
        "bim  = BIM(articles, articles_with_name)\n",
        "relevant_articles_list_1 = bim.answer_query(\" \".join(tweet_query))\n",
        "\n",
        "# bim.relevance_feedback(5,6,8)\n",
        "# bim.show_more()\n",
        "# bim.read_document(2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oK89bse1GWVn",
        "outputId": "ec63c556-c55d-40bd-fefc-312f45843af3"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Article UkraineRussiaCrisis_212.json, score: 932.1615793235385\n",
            "Article UkraineRussiaCrisis_264.json, score: 790.8933846296359\n",
            "Article vaccine_299.json, score: 679.4266212521104\n",
            "Article UkraineRussiaCrisis_213.json, score: 636.0073299018009\n",
            "Article narendramodi_256.json, score: 588.133861214538\n",
            "Article UkraineRussiaCrisis_261.json, score: 570.5954727445408\n",
            "Article UkraineRussiaCrisis_220.json, score: 512.6806648481777\n",
            "Article UkraineRussiaCrisis_215.json, score: 418.2201738243974\n",
            "Article UkraineRussiaCrisis_270.json, score: 387.71954960765765\n",
            "Article hijab_286.json, score: 374.4658480987304\n",
            "Article UkraineRussiaCrisis_216.json, score: 324.95690417228866\n",
            "Article hijab_285.json, score: 311.56960827872285\n",
            "Article TaylorSwift_235.json, score: 310.28095151470364\n",
            "Article vaccine_291.json, score: 308.59067375183935\n",
            "Article UkraineRussiaCrisis_211.json, score: 308.56011700514296\n",
            "Article hijab_284.json, score: 295.26180956009523\n",
            "Article UkraineRussiaCrisis_266.json, score: 283.52551098503864\n",
            "Article IPL_221.json, score: 273.44893883244976\n",
            "Article UkraineRussiaCrisis_269.json, score: 272.3111373260762\n",
            "Article narendramodi_260.json, score: 266.6263391472428\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mean_average_precision_hashtag = mean_average_precision(20, u_base_hashtag, u_time, relevant_articles_list_1, docs_preprocessed_metrics)\n",
        "print(mean_average_precision_hashtag)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4kXaqZ1pUKfF",
        "outputId": "4bd88438-f405-4c42-cba2-3b6a2e6ca34b"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hashtag: hijab   Precision@1: 0.0\n",
            "Hashtag: hijab   Precision@2: 0.0\n",
            "Hashtag: hijab   Precision@3: 0.0\n",
            "Hashtag: hijab   Precision@4: 0.0\n",
            "Hashtag: hijab   Precision@5: 0.0\n",
            "Hashtag: hijab   Precision@6: 0.0\n",
            "Hashtag: hijab   Precision@7: 0.0\n",
            "Hashtag: hijab   Precision@8: 0.0\n",
            "Hashtag: hijab   Precision@9: 0.0\n",
            "Hashtag: hijab   Precision@10: 0.1\n",
            "Hashtag: hijab   Precision@11: 0.09090909090909091\n",
            "Hashtag: hijab   Precision@12: 0.16666666666666666\n",
            "Hashtag: hijab   Precision@13: 0.15384615384615385\n",
            "Hashtag: hijab   Precision@14: 0.14285714285714285\n",
            "Hashtag: hijab   Precision@15: 0.13333333333333333\n",
            "Hashtag: hijab   Precision@16: 0.1875\n",
            "Hashtag: hijab   Precision@17: 0.17647058823529413\n",
            "Hashtag: hijab   Precision@18: 0.16666666666666666\n",
            "Hashtag: hijab   Precision@19: 0.15789473684210525\n",
            "Hashtag: hijab   Precision@20: 0.15\n",
            "0.08130721896782268\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PblKJeWT_8YP",
        "outputId": "9bdddbc2-357d-4c1b-89d5-b702475b6c2f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Article hijab_287.json, score: 354.2969396047313\n",
            "Article hijab_285.json, score: 303.2142699556743\n",
            "Article hijab_286.json, score: 302.2618356805923\n",
            "Article hijab_288.json, score: 293.202659097206\n",
            "Article hijab_281.json, score: 252.95173934816063\n",
            "Article hijab_284.json, score: 218.78264950179994\n",
            "Article hijab_289.json, score: 212.4746252952575\n",
            "Article hijab_282.json, score: 202.36480224012365\n",
            "Article hijab_290.json, score: 161.09023758058015\n",
            "Article hijab_283.json, score: 160.60603394208113\n",
            "Article narendramodi_253.json, score: 18.670173258663034\n",
            "Article UkraineRussiaCrisis_264.json, score: 18.61105938233506\n",
            "Article UkraineRussiaCrisis_218.json, score: 16.915064008001238\n",
            "Article IndiaFightsCorona_246.json, score: 14.857431629982603\n",
            "Article UkraineRussiaCrisis_214.json, score: 14.322889165260447\n",
            "Article narendramodi_255.json, score: 12.69521616601773\n",
            "Article narendramodi_259.json, score: 9.55567938396042\n",
            "Article IndiaFightsCorona_247.json, score: 9.5485927768403\n",
            "Article IndiaFightsCorona_249.json, score: 9.478737290911438\n",
            "Article vaccine_292.json, score: 8.734951717100754\n"
          ]
        }
      ],
      "source": [
        "# results with top k keywords\n",
        "bim  = BIM(articles, articles_with_name)\n",
        "relevant_articles_list_2 = bim.answer_query(\" \".join(tweet_keywords))\n",
        "\n",
        "# bim.relevance_feedback(5,6,8)\n",
        "# bim.show_more()\n",
        "# bim.read_document(2)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mean_average_recall_hashtag = mean_average_recall(20, u_base_hashtag, u_time, relevant_articles_list_2, docs_preprocessed_metrics)\n",
        "print(mean_average_recall_hashtag)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oLqIiT2oXnKF",
        "outputId": "1ad63956-8281-40cf-f71f-aeb949381332"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hashtag: hijab   Recall@1: 0.05\n",
            "Hashtag: hijab   Recall@2: 0.1\n",
            "Hashtag: hijab   Recall@3: 0.15\n",
            "Hashtag: hijab   Recall@4: 0.2\n",
            "Hashtag: hijab   Recall@5: 0.25\n",
            "Hashtag: hijab   Recall@6: 0.3\n",
            "Hashtag: hijab   Recall@7: 0.35\n",
            "Hashtag: hijab   Recall@8: 0.4\n",
            "Hashtag: hijab   Recall@9: 0.45\n",
            "Hashtag: hijab   Recall@10: 0.5\n",
            "Hashtag: hijab   Recall@11: 0.5\n",
            "Hashtag: hijab   Recall@12: 0.5\n",
            "Hashtag: hijab   Recall@13: 0.5\n",
            "Hashtag: hijab   Recall@14: 0.5\n",
            "Hashtag: hijab   Recall@15: 0.5\n",
            "Hashtag: hijab   Recall@16: 0.5\n",
            "Hashtag: hijab   Recall@17: 0.5\n",
            "Hashtag: hijab   Recall@18: 0.5\n",
            "Hashtag: hijab   Recall@19: 0.5\n",
            "Hashtag: hijab   Recall@20: 0.5\n",
            "0.3875\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "BIP_IR.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}