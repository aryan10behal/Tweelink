{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "keyword_extractor.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "36fGtBv0WSWc"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import os\n",
        "import nltk\n",
        "from glob import glob\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "from tqdm import tqdm\n",
        "import pickle\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('corpus')\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SxFqV53CYUKq",
        "outputId": "b44a3423-7885-461b-f496-47aeaf1aff14"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Error loading corpus: Package 'corpus' not found in index\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = ['Very excited for todays IPL contest RPS vs KKR, @msdhoni vs @GautamGambhir fight! #IPL','#poll who score 50 + score today #smithy #dhoni #stokes #Rahane #KKRvRPS #rpsvskkr #cricketlovers #ipl #IPL2017', 'RPS should be happy team today, because KKR have decided to rest NCN. He has been in prime form. #KKRvRPS #IPL @RPSupergiants @KKRiders', 'KKR seek to extend unbeaten run against Pune https://t.co/NdEuZIdxL5 via @cricbuzz @RPSupergiants @KKRiders #IPL', '#RPSvKKR Predict What will be the outcome? #ipl #KKRvRPS #ipl #Smithy #Gambhir 21']"
      ],
      "metadata": {
        "id": "WU8bCsz1XOz1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preprocessed_vocabulary = dict()"
      ],
      "metadata": {
        "id": "AuQx1xrJfbS0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Converting to lowercase\n",
        "def to_lower_case(text):\n",
        "  text = text.lower()\n",
        "  return text\n",
        "\n",
        "def remove_at_word(text):\n",
        "  data = text.split()\n",
        "  data = [d for d in data if d[0]!='@']\n",
        "  text = ' '.join(data)\n",
        "  return text\n",
        "\n",
        "def remove_hashtag(text):\n",
        "  data = text.split()\n",
        "  data = [d if (d[0]!='#' or len(d) == 1) else d[1:] for d in data]\n",
        "  data = [d for d in data if d[0]!='#']\n",
        "  text = ' '.join(data)\n",
        "  return text\n",
        "\n",
        "def remove_URL(text):\n",
        "  text = re.sub(r\"http\\S+\", \"\", text)\n",
        "  text = re.sub(r'bit.ly\\S+', '', text, flags=re.MULTILINE)\n",
        "  return text\n",
        "\n",
        "#Removing stopwords\n",
        "def remove_stopwords(text):\n",
        "  stopword = stopwords.words('english')\n",
        "  new_list = [x for x in text.split() if x not in stopword]\n",
        "  return ' '.join(new_list)\n",
        "\n",
        "#Removing punctuations\n",
        "def remove_punctuations(text):\n",
        "  punctuations = '''!()-[|]`{};:'\"\\,<>./?@#$=+%^&*_~'''\n",
        "  new_list = ['' if x in punctuations else x for x in text.split()]\n",
        "  new_list_final = []\n",
        "  for token in new_list:\n",
        "    new_token=\"\"\n",
        "    for char in token:\n",
        "      if(char not in punctuations):\n",
        "        new_token+=char\n",
        "    if(len(new_token)!=0):\n",
        "      new_list_final.append(new_token)\n",
        "  return ' '.join(new_list_final)\n",
        "\n",
        "#Tokenization\n",
        "def tokenization(text):\n",
        "  return word_tokenize(text)\n",
        "\n",
        "def pre_process(text):\n",
        "  global preprocessed_vocabulary\n",
        "  text = to_lower_case(text)\n",
        "  text = remove_at_word(text)\n",
        "  text = remove_hashtag(text)\n",
        "  text = remove_URL(text)\n",
        "  text = remove_stopwords(text)\n",
        "  text = remove_punctuations(text)\n",
        "  text = tokenization(text)\n",
        "  for token in text:\n",
        "    if token in preprocessed_vocabulary.keys():\n",
        "      preprocessed_vocabulary[token] += 1\n",
        "    else:\n",
        "      preprocessed_vocabulary[token] = 1\n",
        "  return text"
      ],
      "metadata": {
        "id": "cHQbm7TvX6K4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preprocessed_data = [pre_process(text) for text in dataset]"
      ],
      "metadata": {
        "id": "OU-oaiBbfV-c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(preprocessed_vocabulary)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bzTgNNKrlmxN",
        "outputId": "339c9518-7bea-4ffe-ef89-5a20c05203f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'excited': 1, 'todays': 1, 'ipl': 7, 'contest': 1, 'rps': 2, 'vs': 2, 'kkr': 3, 'fight': 1, 'poll': 1, 'score': 2, '50': 1, 'today': 2, 'smithy': 2, 'dhoni': 1, 'stokes': 1, 'rahane': 1, 'kkrvrps': 3, 'rpsvskkr': 1, 'cricketlovers': 1, 'ipl2017': 1, 'happy': 1, 'team': 1, 'decided': 1, 'rest': 1, 'ncn': 1, 'prime': 1, 'form': 1, 'seek': 1, 'extend': 1, 'unbeaten': 1, 'run': 1, 'pune': 1, 'via': 1, 'rpsvkkr': 1, 'predict': 1, 'outcome': 1, 'gambhir': 1, '21': 1}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "AOF_coefficient = sum(preprocessed_vocabulary.values())/len(preprocessed_vocabulary)\n",
        "vocabulary = {token.strip():preprocessed_vocabulary[token] for token in preprocessed_vocabulary.keys() if preprocessed_vocabulary[token] > AOF_coefficient and len(token.strip())}"
      ],
      "metadata": {
        "id": "_I9BuKpGiIIm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(vocabulary)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y2gp2lDlfPo5",
        "outputId": "591c583f-f355-4aab-e54b-306731e59fda"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'ipl': 7, 'rps': 2, 'vs': 2, 'kkr': 3, 'score': 2, 'today': 2, 'smithy': 2, 'kkrvrps': 3}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "final_tokens_per_tweet = []\n",
        "for data in preprocessed_data:\n",
        "  final_tokens_per_tweet.append([token for token in data if token in vocabulary.keys()])\n",
        "\n",
        "print(preprocessed_data)\n",
        "print(final_tokens_per_tweet)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lz5V8UeeDOZP",
        "outputId": "2f0ceddd-90ff-4f1f-9a51-ba1a1ce80c1a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['excited', 'todays', 'ipl', 'contest', 'rps', 'vs', 'kkr', 'vs', 'fight', 'ipl'], ['poll', 'score', '50', 'score', 'today', 'smithy', 'dhoni', 'stokes', 'rahane', 'kkrvrps', 'rpsvskkr', 'cricketlovers', 'ipl', 'ipl2017'], ['rps', 'happy', 'team', 'today', 'kkr', 'decided', 'rest', 'ncn', 'prime', 'form', 'kkrvrps', 'ipl'], ['kkr', 'seek', 'extend', 'unbeaten', 'run', 'pune', 'via', 'ipl'], ['rpsvkkr', 'predict', 'outcome', 'ipl', 'kkrvrps', 'ipl', 'smithy', 'gambhir', '21']]\n",
            "[['ipl', 'rps', 'vs', 'kkr', 'vs', 'ipl'], ['score', 'score', 'today', 'smithy', 'kkrvrps', 'ipl'], ['rps', 'today', 'kkr', 'kkrvrps', 'ipl'], ['kkr', 'ipl'], ['ipl', 'kkrvrps', 'ipl', 'smithy']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word2id = dict()\n",
        "id2word = dict()\n",
        "vocabulary_size = len(vocabulary)\n",
        "count = 0\n",
        "for token in vocabulary.keys():\n",
        "  word2id[token] = count\n",
        "  id2word[count] = token\n",
        "  count += 1\n",
        "\n",
        "print(word2id)\n",
        "print(id2word)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jdsX2kJep6G-",
        "outputId": "258f086e-d135-41de-a063-05e1c08a3c62"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'ipl': 0, 'rps': 1, 'vs': 2, 'kkr': 3, 'score': 4, 'today': 5, 'smithy': 6, 'kkrvrps': 7}\n",
            "{0: 'ipl', 1: 'rps', 2: 'vs', 3: 'kkr', 4: 'score', 5: 'today', 6: 'smithy', 7: 'kkrvrps'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "directed_graph_adjacency_matrix = np.zeros((vocabulary_size, vocabulary_size))\n",
        "edge_weight_matrix = np.zeros((vocabulary_size, vocabulary_size))\n",
        "first_frequency = dict()\n",
        "last_frequency = dict()\n",
        "term_frequency = vocabulary\n",
        "strength = dict()\n",
        "degree = dict()\n",
        "selective_centraility = dict()\n",
        "\n",
        "\n",
        "for tweet in tqdm(final_tokens_per_tweet):\n",
        "\n",
        "  if tweet[0] in first_frequency.keys():\n",
        "    first_frequency[tweet[0]] += 1\n",
        "  else:\n",
        "    first_frequency[tweet[0]] = 1\n",
        "\n",
        "  if tweet[-1] in last_frequency.keys():\n",
        "    last_frequency[tweet[-1]] += 1\n",
        "  else:\n",
        "    last_frequency[tweet[-1]] = 1\n",
        "  \n",
        "\n",
        "\n",
        "  for i in range(len(tweet)-1):\n",
        "    if tweet[i] == tweet[i+1]:\n",
        "      continue\n",
        "    x = word2id[tweet[i]]\n",
        "    y = word2id[tweet[i+1]]\n",
        "    directed_graph_adjacency_matrix[x][y] += 1\n",
        "\n",
        "for tweet in tqdm(final_tokens_per_tweet):\n",
        "  for i in range(len(tweet)-1):\n",
        "\n",
        "\n",
        "    if tweet[i] == tweet[i+1]:\n",
        "      continue\n",
        "    x = word2id[tweet[i]]\n",
        "    y = word2id[tweet[i+1]]\n",
        "\n",
        "  # Updating degree..\n",
        "    if tweet[i] in degree.keys():\n",
        "      degree[tweet[i]] += 1\n",
        "    else:\n",
        "      degree[tweet[i]] = 1\n",
        "      \n",
        "    if tweet[i+1] in degree.keys():\n",
        "      degree[tweet[i+1]] += 1\n",
        "    else:\n",
        "      degree[tweet[i+1]] = 1\n",
        "\n",
        "    edge_weight_matrix[x][y] = directed_graph_adjacency_matrix[x][y]/(vocabulary[tweet[i]] + vocabulary[tweet[i+1]] - directed_graph_adjacency_matrix[x][y])\n",
        "\n",
        "    if tweet[i] in strength.keys():\n",
        "      strength[tweet[i]] += edge_weight_matrix[x][y]\n",
        "    else:\n",
        "      strength[tweet[i]] = edge_weight_matrix[x][y]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "first_frequency = {token:(first_frequency[token]/vocabulary[token] if token in first_frequency else 0) for token in vocabulary.keys()}\n",
        "last_frequency = {token:(last_frequency[token]/vocabulary[token] if token in last_frequency else 0) for token in vocabulary.keys()}\n",
        "degree = {token:(degree[token] if token in degree else 0) for token in vocabulary.keys()}\n",
        "strength = {token:(strength[token] if token in strength else 0) for token in vocabulary.keys()}\n",
        "selective_centraility = {token:(strength[token]/degree[token] if degree[token]!=0 else 0) for token in vocabulary.keys()}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I3uSKzpBEyZf",
        "outputId": "1295761c-3fcb-464e-de96-425f76907e17"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5/5 [00:00<00:00, 29873.96it/s]\n",
            "100%|██████████| 5/5 [00:00<00:00, 18608.27it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(degree)\n",
        "print(vocabulary)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VTJGacqFVxDA",
        "outputId": "ece940f9-9990-4263-9d1d-4446401a2e05"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'ipl': 8, 'rps': 3, 'vs': 4, 'kkr': 5, 'score': 1, 'today': 4, 'smithy': 3, 'kkrvrps': 6}\n",
            "{'ipl': 7, 'rps': 2, 'vs': 2, 'kkr': 3, 'score': 2, 'today': 2, 'smithy': 2, 'kkrvrps': 3}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "maxdegree = max(degree.items(), key=lambda x: x[1])[1]\n",
        "max_degree_nodes_with_freq = {key:term_frequency[key] for key in degree.keys() if degree[key] == maxdegree}\n",
        "maxfreq = max(max_degree_nodes_with_freq.items(), key=lambda x: x[1])[1]\n",
        "central_node_name = [key for key in max_degree_nodes_with_freq.keys() if max_degree_nodes_with_freq[key] == maxfreq][0]\n",
        "print(\"central node: \", central_node_name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bLbhHBBnTZpR",
        "outputId": "206675d1-bb91-459c-a6ba-715c7d1dcec2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "central node:  ipl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# bfs\n",
        "distance_from_central_node = dict()\n",
        "central_node_id = word2id[central_node_name]\n",
        "q = [(central_node_id, 0)]\n",
        "\n",
        "# Set source as visited\n",
        "distance_from_central_node[central_node_name] = 0\n",
        "\n",
        "while q:\n",
        "    vis = q[0]\n",
        "    # Print current node\n",
        "    print(id2word[vis[0]], vis[1])\n",
        "    q.pop(0)\n",
        "      \n",
        "    # For every adjacent vertex to\n",
        "    # the current vertex\n",
        "    for i in range(len(directed_graph_adjacency_matrix[vis[0]])):\n",
        "        if (directed_graph_adjacency_matrix[vis[0]][i] == 1 and (id2word[i] not in distance_from_central_node.keys())):\n",
        "            # Push the adjacent node\n",
        "            # in the queue\n",
        "            q.append((i, vis[1]+1))\n",
        "            distance_from_central_node[id2word[i]] = vis[1]+1\n",
        "\n",
        "print(distance_from_central_node)\n",
        "inverse_distance_from_central_node = {token:(1/distance_from_central_node[token] if token in distance_from_central_node and token != central_node_name else 0) for token in vocabulary.keys()}\n",
        "inverse_distance_from_central_node[central_node_name] = 1.0\n",
        "print(inverse_distance_from_central_node)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rcBNSaW7W94x",
        "outputId": "9c9896e4-0e1a-459e-cab9-694fb94dacdf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ipl 0\n",
            "rps 1\n",
            "smithy 1\n",
            "kkrvrps 1\n",
            "vs 2\n",
            "today 2\n",
            "kkr 3\n",
            "{'ipl': 0, 'rps': 1, 'smithy': 1, 'kkrvrps': 1, 'vs': 2, 'today': 2, 'kkr': 3}\n",
            "{'ipl': 1.0, 'rps': 1.0, 'vs': 0.5, 'kkr': 0.3333333333333333, 'score': 0, 'today': 0.5, 'smithy': 1.0, 'kkrvrps': 1.0}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "neighbour_importance = dict()\n",
        "\n",
        "for i in range(len(directed_graph_adjacency_matrix)):\n",
        "  neighbours = set()\n",
        "\n",
        "  # traversing outgoing edges\n",
        "  for j in range(len(directed_graph_adjacency_matrix)):\n",
        "    if i == j:\n",
        "      continue\n",
        "    if directed_graph_adjacency_matrix[i][j] > 0:\n",
        "      neighbours.add(j)\n",
        "  for j in range(len(directed_graph_adjacency_matrix)):\n",
        "     if i == j:\n",
        "      continue\n",
        "     if directed_graph_adjacency_matrix[j][i] > 0:\n",
        "        neighbours.add(j)\n",
        "  if len(neighbours) != 0:\n",
        "    neighbour_importance[id2word[i]] = sum([strength[id2word[j]] for j in neighbours])/len(neighbours)\n",
        "  else:\n",
        "    neighbour_importance[id2word[i]] = 0\n",
        "    \n",
        "print(neighbour_importance)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HzfIYq3IbOHk",
        "outputId": "0db6b77c-253d-445b-a8d7-6a75d467d38f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'ipl': 0.6276984126984126, 'rps': 0.4398148148148148, 'vs': 0.5296296296296296, 'kkr': 0.6512896825396826, 'score': 0.5833333333333333, 'today': 0.4527777777777778, 'smithy': 0.7433862433862434, 'kkrvrps': 0.3907407407407408}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "unnormalized_node_weight = {node: (first_frequency[node] + last_frequency[node] + term_frequency[node] + selective_centraility[node] + inverse_distance_from_central_node[node] + neighbour_importance[node]) for node in vocabulary.keys()}\n",
        "max_node_weight = max(unnormalized_node_weight.items(), key=lambda x: x[1])[1]\n",
        "min_node_weight = min(unnormalized_node_weight.items(), key=lambda x: x[1])[1]\n",
        "print(\"max node weight: \", max_node_weight, \"min node weight: \", min_node_weight)\n",
        "normalized_node_weight = {node: ((unnormalized_node_weight[node] - min_node_weight)/(max_node_weight - min_node_weight) if max_node_weight != min_node_weight else unnormalized_node_weight[node]) for node in unnormalized_node_weight.keys()}\n",
        "print(\"Unnormalized score: \", unnormalized_node_weight)\n",
        "print(\"Normalized score: \", normalized_node_weight)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5oYYt8q5hAuZ",
        "outputId": "19eb401f-53f3-41c9-ada8-3acc42e3e41b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "max node weight:  9.52998015873016 min node weight:  3.0986111111111114\n",
            "Unnormalized score:  {'ipl': 9.52998015873016, 'rps': 4.162037037037037, 'vs': 3.1233796296296297, 'kkr': 4.430178571428572, 'score': 3.416666666666667, 'today': 3.0986111111111114, 'smithy': 4.326719576719577, 'kkrvrps': 4.605026455026455}\n",
            "Normalized score:  {'ipl': 1.0, 'rps': 0.16534985289323675, 'vs': 0.003851204671218141, 'kkr': 0.20704261417099346, 'score': 0.04945378708648396, 'today': 0.0, 'smithy': 0.19095599343084227, 'kkrvrps': 0.23422934258033795}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "damping_factor = 0.85\n",
        "relevance_of_node = {node: np.random.uniform(0,1,1)[0] for node in vocabulary.keys()}\n",
        "threshold = 0.000000001\n",
        "\n",
        "\n",
        "print(relevance_of_node)\n",
        "\n",
        "count = 0\n",
        "while True:\n",
        "  count += 1\n",
        "  current_relevance_of_node = dict()\n",
        "  for node in vocabulary.keys():\n",
        "    outer_sum = 0\n",
        "    node_idx = word2id[node]\n",
        "    for j in range(len(directed_graph_adjacency_matrix)):\n",
        "      if j == node_idx:\n",
        "        continue\n",
        "      if directed_graph_adjacency_matrix[j][node_idx] > 0:\n",
        "        den_sum = 0\n",
        "        for k in range(len(directed_graph_adjacency_matrix)):\n",
        "          if k == j:\n",
        "            continue\n",
        "          den_sum += directed_graph_adjacency_matrix[j][k]\n",
        "        outer_sum += ((directed_graph_adjacency_matrix[j][node_idx]/den_sum) * relevance_of_node[id2word[j]])\n",
        "    current_relevance_of_node[node] = (1-damping_factor)*normalized_node_weight[node] + damping_factor*normalized_node_weight[node]*outer_sum\n",
        "  \n",
        "\n",
        "  # checking convergence..\n",
        "  sq_error = sum([(current_relevance_of_node[node] - relevance_of_node[node])**2 for node in vocabulary.keys()])\n",
        "  relevance_of_node = current_relevance_of_node\n",
        "  if sq_error < threshold:\n",
        "    break\n",
        "\n",
        "print(relevance_of_node)\n",
        "print(count)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LeBhkP4Iy5t0",
        "outputId": "481575b3-a943-4904-ad20-6201d715ea31"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'ipl': 0.7103288528750432, 'rps': 0.05578237780724982, 'vs': 0.29588928573735207, 'kkr': 0.20964259931506024, 'score': 0.23801881520407897, 'today': 0.9920450562636448, 'smithy': 0.9209308688858173, 'kkrvrps': 0.18787532903134696}\n",
            "{'ipl': 0.209298750775006, 'rps': 0.03460902545948005, 'vs': 0.0006682853809892396, 'kkr': 0.031115198480412807, 'score': 0.007418068062972595, 'today': 0.0, 'smithy': 0.03996859218589993, 'kkrvrps': 0.059049366830692354}\n",
            "10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "degree_centrality  = {node: 0 for node in vocabulary.keys()}\n",
        "\n",
        "if len(directed_graph_adjacency_matrix) > 1:\n",
        "  for i in range(len(directed_graph_adjacency_matrix)):\n",
        "    count = 0\n",
        "    for j in range(len(directed_graph_adjacency_matrix)):\n",
        "      if i == j:\n",
        "        continue\n",
        "      if directed_graph_adjacency_matrix[j][i] > 0:\n",
        "        count += 1\n",
        "    degree_centrality[id2word[i]] = count / (len(directed_graph_adjacency_matrix)-1)\n",
        "\n",
        "print(degree_centrality)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e0xhb9q06YZ5",
        "outputId": "2a165254-8c25-4eec-de7a-5a92bcf5c330"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'ipl': 0.42857142857142855, 'rps': 0.14285714285714285, 'vs': 0.2857142857142857, 'kkr': 0.2857142857142857, 'score': 0.0, 'today': 0.2857142857142857, 'smithy': 0.2857142857142857, 'kkrvrps': 0.42857142857142855}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "final_keyword_rank = [{'node': node, 'NE_rank': relevance_of_node[node], 'Degree': degree_centrality[node]} for node in vocabulary.keys()]\n",
        "for data in final_keyword_rank:\n",
        "  print(data)\n",
        "\n",
        "print(\"-----------\")\n",
        "final_keyword_rank = sorted(final_keyword_rank, key = lambda i: (i['NE_rank'], i['Degree']), reverse = True)\n",
        "for data in final_keyword_rank:\n",
        "  print(data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LbbkUy7n9n4A",
        "outputId": "820a9f44-4620-4a08-a808-9e559053557f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'node': 'ipl', 'NE_rank': 0.209298750775006, 'Degree': 0.42857142857142855}\n",
            "{'node': 'rps', 'NE_rank': 0.03460902545948005, 'Degree': 0.14285714285714285}\n",
            "{'node': 'vs', 'NE_rank': 0.0006682853809892396, 'Degree': 0.2857142857142857}\n",
            "{'node': 'kkr', 'NE_rank': 0.031115198480412807, 'Degree': 0.2857142857142857}\n",
            "{'node': 'score', 'NE_rank': 0.007418068062972595, 'Degree': 0.0}\n",
            "{'node': 'today', 'NE_rank': 0.0, 'Degree': 0.2857142857142857}\n",
            "{'node': 'smithy', 'NE_rank': 0.03996859218589993, 'Degree': 0.2857142857142857}\n",
            "{'node': 'kkrvrps', 'NE_rank': 0.059049366830692354, 'Degree': 0.42857142857142855}\n",
            "-----------\n",
            "{'node': 'ipl', 'NE_rank': 0.209298750775006, 'Degree': 0.42857142857142855}\n",
            "{'node': 'kkrvrps', 'NE_rank': 0.059049366830692354, 'Degree': 0.42857142857142855}\n",
            "{'node': 'smithy', 'NE_rank': 0.03996859218589993, 'Degree': 0.2857142857142857}\n",
            "{'node': 'rps', 'NE_rank': 0.03460902545948005, 'Degree': 0.14285714285714285}\n",
            "{'node': 'kkr', 'NE_rank': 0.031115198480412807, 'Degree': 0.2857142857142857}\n",
            "{'node': 'score', 'NE_rank': 0.007418068062972595, 'Degree': 0.0}\n",
            "{'node': 'vs', 'NE_rank': 0.0006682853809892396, 'Degree': 0.2857142857142857}\n",
            "{'node': 'today', 'NE_rank': 0.0, 'Degree': 0.2857142857142857}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.uniform(-1,0,1)[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3iNVU_-d2G5r",
        "outputId": "f88e003e-807e-4412-86a3-e06f4a3e3497"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-0.8363891837073725"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(edge_weight_matrix)):\n",
        "  for j in range(len(edge_weight_matrix)):\n",
        "    print(id2word[i], id2word[j], edge_weight_matrix[i][j], end= \" | \")\n",
        "  print()"
      ],
      "metadata": {
        "id": "9GCBr5fFGjOy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}