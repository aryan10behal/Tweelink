{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a18e146",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code credits: https://towardsdatascience.com/an-extensive-guide-to-collecting-tweets-from-twitter-api-v2-for-academic-research-using-python-3-518fcb71df2a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "463bd74d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "import json\n",
    "import csv\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import dateutil.parser\n",
    "import unicodedata\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4421254",
   "metadata": {},
   "source": [
    "# Enter your bearer_token here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a34c5ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['TOKEN'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0a1f8c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def auth():\n",
    "    return os.getenv('TOKEN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f225bee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_headers(bearer_token):\n",
    "    headers = {\"Authorization\": \"Bearer {}\".format(bearer_token)}\n",
    "    return headers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "147c3543",
   "metadata": {},
   "source": [
    "# Change date here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a866abd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_url(keyword, start_date, end_date, max_results = 10):\n",
    "    \n",
    "    search_url = \"https://api.twitter.com/2/tweets/search/recent\"\n",
    "\n",
    "    query_params = {\n",
    "        'query': '#' + keyword + ' lang:en -is:retweet',\n",
    "        'start_time': start_date,\n",
    "        'end_time': end_date,\n",
    "        'max_results': max_results,\n",
    "        'sort_order': 'relevancy',\n",
    "        'tweet.fields': 'attachments,author_id,context_annotations,conversation_id,created_at,entities,geo,id,in_reply_to_user_id,lang,public_metrics,possibly_sensitive,referenced_tweets,reply_settings,source,text,withheld',\n",
    "        'expansions': 'attachments.poll_ids,attachments.media_keys,author_id,entities.mentions.username,geo.place_id,in_reply_to_user_id,referenced_tweets.id,referenced_tweets.id.author_id',\n",
    "        'user.fields': 'created_at,description,entities,id,location,name,pinned_tweet_id,profile_image_url,protected,public_metrics,url,username,verified,withheld',\n",
    "        'media.fields': 'duration_ms,height,media_key,preview_image_url,type,url,width,public_metrics,alt_text',\n",
    "        'place.fields': 'contained_within,country,country_code,full_name,geo,id,name,place_type',\n",
    "        'next_token': {}\n",
    "    }\n",
    "    return (search_url, query_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "43232a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def custom_create_url(tag):\n",
    "#   url = \"https://api.twitter.com/2/tweets/search/recent\"\n",
    "#   params = {\n",
    "#       'query': '#'+tag+' lang:en -is:retweet',\n",
    "#       'start_time': '2022-02-11T00:00:00.000Z',\n",
    "#       'end_time': '2022-02-17T00:00:00.000Z',\n",
    "#       'max_results':100,\n",
    "#       'sort_order': 'relevancy',\n",
    "#       'tweet.fields': 'attachments,author_id,context_annotations,conversation_id,created_at,entities,geo,id,in_reply_to_user_id,lang,public_metrics,possibly_sensitive,referenced_tweets,reply_settings,source,text,withheld',\n",
    "#       'expansions': 'attachments.poll_ids,attachments.media_keys,author_id,entities.mentions.username,geo.place_id,in_reply_to_user_id,referenced_tweets.id,referenced_tweets.id.author_id',\n",
    "#       'user.fields': 'created_at,description,entities,id,location,name,pinned_tweet_id,profile_image_url,protected,public_metrics,url,username,verified,withheld',\n",
    "#       'media.fields': 'duration_ms,height,media_key,preview_image_url,type,url,width,public_metrics,alt_text',\n",
    "#       'place.fields': 'contained_within,country,country_code,full_name,geo,id,name,place_type',\n",
    "#       'poll.fields': 'duration_minutes,end_datetime,id,options,voting_status',\n",
    "#       'next_token': {}\n",
    "#   }\n",
    "#   return url, params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f1ab8fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def bearer_oauth(r):\n",
    "#     \"\"\"\n",
    "#     Method required by bearer token authentication.\n",
    "#     \"\"\"\n",
    "\n",
    "#     r.headers[\"Authorization\"] = f\"Bearer {bearer_token}\"\n",
    "#     r.headers[\"User-Agent\"] = \"v2TweetLookupPython\"\n",
    "#     return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3b85b5e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def connect_to_endpoint(url, headers, params, next_token = None):\n",
    "    params['next_token'] = next_token   #params object received from create_url function\n",
    "    response = requests.request(\"GET\", url, headers = headers, params = params)\n",
    "    print(\"Endpoint Response Code: \" + str(response.status_code))\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(response.status_code, response.text)\n",
    "    return response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e345277b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def connect_to_endpoint(url):\n",
    "#     response = requests.request(\"GET\", url, auth=bearer_oauth)\n",
    "#     print(response.status_code)\n",
    "#     if response.status_code != 200:\n",
    "#         raise Exception(\n",
    "#             \"Request returned an error: {} {}\".format(\n",
    "#                 response.status_code, response.text\n",
    "#             )\n",
    "#         )\n",
    "#     return response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ce9fb054",
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_to_csv(json_response, fileName):\n",
    "\n",
    "    #A counter variable\n",
    "    counter = 0\n",
    "\n",
    "    #Open OR create the target CSV file\n",
    "    csvFile = open(fileName, \"a\", newline=\"\", encoding='utf-8')\n",
    "    csvWriter = csv.writer(csvFile)\n",
    "\n",
    "    #Loop through each tweet\n",
    "    for tweet in json_response['data']:\n",
    "        \n",
    "        # We will create a variable for each since some of the keys might not exist for some tweets\n",
    "        # So we will account for that\n",
    "\n",
    "        # 1. Author ID\n",
    "        author_id = tweet['author_id']\n",
    "\n",
    "        # 2. Time created\n",
    "        created_at = dateutil.parser.parse(tweet['created_at'])\n",
    "\n",
    "        # 3. Geolocation\n",
    "        if ('geo' in tweet):   \n",
    "            geo = tweet['geo']['place_id']\n",
    "        else:\n",
    "            geo = \" \"\n",
    "\n",
    "        # 4. Tweet ID\n",
    "        tweet_id = tweet['id']\n",
    "\n",
    "        # 5. Language\n",
    "        lang = tweet['lang']\n",
    "\n",
    "        # 6. Tweet metrics\n",
    "        retweet_count = tweet['public_metrics']['retweet_count']\n",
    "        reply_count = tweet['public_metrics']['reply_count']\n",
    "        like_count = tweet['public_metrics']['like_count']\n",
    "        quote_count = tweet['public_metrics']['quote_count']\n",
    "\n",
    "        # 7. source\n",
    "        source = tweet['source']\n",
    "\n",
    "        # 8. Tweet text\n",
    "        text = tweet['text']\n",
    "        \n",
    "        # Assemble all data in a list\n",
    "        res = [author_id, created_at, geo, tweet_id, lang, like_count, quote_count, reply_count, retweet_count, source, text]\n",
    "        \n",
    "        # Append the result to the CSV file\n",
    "        csvWriter.writerow(res)\n",
    "        counter += 1\n",
    "\n",
    "    # When done, close the CSV file\n",
    "    csvFile.close()\n",
    "\n",
    "    # Print the number of tweets for this iteration\n",
    "    print(\"# of Tweets added from this response: \", counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "82ddd20f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def custom_connect_to_endpoint(custom_url, params, next_token=None):\n",
    "#     params['next_token'] = next_token\n",
    "#     response = requests.request(\"GET\", custom_url, auth=bearer_oauth, params=params)\n",
    "#     print(response.status_code)\n",
    "#     if response.status_code != 200:\n",
    "#         raise Exception(\n",
    "#             \"Request returned an error: {} {}\".format(\n",
    "#                 response.status_code, response.text\n",
    "#             )\n",
    "#         )\n",
    "#     return response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "02229e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def make_csv(json_response, file_path, csvWriter):\n",
    "#   counter = 0\n",
    "  \n",
    " \n",
    "#   for tweet in json_response['data']:\n",
    "      \n",
    "#       # We will create a variable for each since some of the keys might not exist for some tweets\n",
    "#       # So we will account for that\n",
    "\n",
    "#       # 1. Author ID\n",
    "#       author_id = str(tweet['author_id'])\n",
    "\n",
    "#       # 2. Time created\n",
    "#       created_at = dateutil.parser.parse(tweet['created_at'])\n",
    "\n",
    "#       # example = \n",
    "#       # {\n",
    "#       #           \"country\": \"United States\",\n",
    "#       #           \"country_code\": \"US\",\n",
    "#       #           \"full_name\": \"Chicago, IL\",\n",
    "#       #           \"geo\": {\n",
    "#       #               \"bbox\": [\n",
    "#       #                   -87.940033,\n",
    "#       #                   41.644102,\n",
    "#       #                   -87.523993,\n",
    "#       #                   42.0230669\n",
    "#       #               ],\n",
    "#       #               \"properties\": {},\n",
    "#       #               \"type\": \"Feature\"\n",
    "#       #           },\n",
    "#       #           \"id\": \"1d9a5370a355ab0c\",\n",
    "#       #           \"name\": \"Chicago\",\n",
    "#       #           \"place_type\": \"city\"\n",
    "#       #       }\n",
    "\n",
    "#       geo = \"\"\n",
    "#       country = \"\"\n",
    "#       country_code = \"\"\n",
    "#       place_full_name = \"\"\n",
    "#       place_name = \"\"\n",
    "#       place_type = \"\"\n",
    "#       # 3. Geolocation\n",
    "#       if ('geo' in tweet):   \n",
    "#           geo = tweet['geo']['place_id']\n",
    "#           for place in json_response['includes']['places']:\n",
    "#             if place['id'] == geo:\n",
    "#               country = place['country']\n",
    "#               country_code = place['country_code']\n",
    "#               place_full_name = place['full_name']\n",
    "#               place_name = place['name']\n",
    "#               place_type = place['place_type']\n",
    "#       else:\n",
    "#           geo = \"\"\n",
    "\n",
    "#       # 4. Tweet ID\n",
    "#       tweet_id = str(tweet['id'])\n",
    "\n",
    "#       # 5. Language\n",
    "#       lang = tweet['lang']\n",
    "\n",
    "#       # 6. Tweet metrics\n",
    "#       retweet_count = tweet['public_metrics']['retweet_count']\n",
    "#       reply_count = tweet['public_metrics']['reply_count']\n",
    "#       like_count = tweet['public_metrics']['like_count']\n",
    "#       quote_count = tweet['public_metrics']['quote_count']\n",
    "\n",
    "#       # 7. source\n",
    "#       source = tweet['source']\n",
    "\n",
    "#       # 8. Tweet text\n",
    "#       text = tweet['text']\n",
    "\n",
    "#       # 9 hashtags\n",
    "#       tags = ''\n",
    "#       if ('hashtags' in tweet['entities']):  \n",
    "#         for tag in tweet['entities']['hashtags']:\n",
    "#           tags += tag['tag'] + str(',')\n",
    "#         tags = tags[:-1]\n",
    "\n",
    "#       #sensitive\n",
    "#       sensitive = tweet['possibly_sensitive']\n",
    "\n",
    "#       # urls for further analysis\n",
    "#       urls = ''\n",
    "#       if 'urls' in tweet['entities']:\n",
    "#         for url in tweet['entities']['urls']:\n",
    "#           urls += url['url'] + str(',')\n",
    "#         urls = urls[:-1]\n",
    "      \n",
    "#       #annotations\n",
    "#       context_text = ''\n",
    "#       context_probability = 0\n",
    "#       context_type = ''\n",
    "#       if 'tweets' in json_response['includes']:\n",
    "#         for tweets_for_annotation in json_response['includes']['tweets']:\n",
    "#             if tweets_for_annotation['conversation_id'] == tweet['conversation_id']:\n",
    "#               if 'entities' in tweets_for_annotation:\n",
    "#                 if 'annotations' in tweets_for_annotation['entities']:\n",
    "#                   for annotation in tweets_for_annotation['entities']['annotations']:\n",
    "#                     context_text = annotation['normalized_text']\n",
    "#                     context_probability = annotation['probability']\n",
    "#                     context_type = annotation['type']\n",
    "\n",
    "\n",
    "      \n",
    "#       # Assemble all data in a list\n",
    "#       res = [author_id, created_at, geo, country, country_code, place_full_name, place_name, place_type, tweet_id, lang, like_count, quote_count, reply_count, retweet_count, source, text, tags, sensitive, urls, context_text, context_probability, context_type]\n",
    "      \n",
    "#       # Append the result to the CSV file\n",
    "#       csvWriter.writerow(res)\n",
    "#       counter += 1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#   # Print the number of tweets for this iteration\n",
    "#   print(\"# of Tweets added from this response: \", counter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77785771",
   "metadata": {},
   "source": [
    "# update query list every day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "150178ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def main():\n",
    "#     query_list = ['canada']\n",
    "#     total_tweets = 0\n",
    "    \n",
    "#     csvFile = open(file_path, \"a\", newline=\"\", encoding='utf-8')\n",
    "#     csvWriter = csv.writer(csvFile)\n",
    "#     csvWriter.writerow(['author id', 'created_at', 'geo', 'country', 'country_code', 'place_full_name', 'place_name', 'place_type', 'id','lang', 'like_count', 'quote_count', 'reply_count','retweet_count','source','tweet', 'hashtags', 'sensitive', 'urls', 'context_text', 'context_probability', 'context_type'])\n",
    "    \n",
    "#     for tag in query_list:\n",
    "#         count = 0\n",
    "#         max_count = 1000\n",
    "#         flag = True\n",
    "#         next_token = None\n",
    "        \n",
    "#         while flag:\n",
    "#             if count >= max_count:\n",
    "#                 break\n",
    "#             print(\"-------------------\")\n",
    "#             print(\"Token: \", next_token)\n",
    "#             custom_url, params = custom_create_url(tag)\n",
    "#             custom_json_response = custom_connect_to_endpoint(custom_url, params, next_token)\n",
    "#             result_count = custom_json_response['meta']['result_count']\n",
    "\n",
    "#             if 'next_token' in custom_json_response['meta']:\n",
    "#                 # Save the token to use for next call\n",
    "#                 next_token = custom_json_response['meta']['next_token']\n",
    "#                 print(\"Next Token: \", next_token)\n",
    "#                 if result_count is not None and result_count > 0 and next_token is not None:\n",
    "#                     make_csv(custom_json_response, file_path, csvWriter)\n",
    "#                     count += result_count\n",
    "#                     total_tweets += result_count\n",
    "#                     print(\"Total # of Tweets added: \", total_tweets)\n",
    "#                     print(\"-------------------\")\n",
    "#                     time.sleep(5)\n",
    "#             # If no next token exists\n",
    "#             else:\n",
    "#                 print('inside else')\n",
    "#                 if result_count is not None and result_count > 0:\n",
    "#                     print(\"-------------------\")\n",
    "#                     make_csv(custom_json_response, file_path, csvWriter)\n",
    "#                     count += result_count\n",
    "#                     total_tweets += result_count\n",
    "#                     print(\"Total # of Tweets added: \", total_tweets)\n",
    "#                     print(\"-------------------\")\n",
    "#                     time.sleep(5)\n",
    "#                 #Since this is the final request, turn flag to false to move to the next time period.\n",
    "#                 flag = False\n",
    "#                 next_token = None\n",
    "#             time.sleep(5)\n",
    "#     csvFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "78c20e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    #Inputs for tweets\n",
    "    bearer_token = auth()\n",
    "    headers = create_headers(bearer_token)\n",
    "    keyword = \"narendramodi\"\n",
    "    s_time = ['00:00:00', '01:00:00','02:00:00','03:00:00','04:00:00','05:00:00','06:00:00','07:00:00','08:00:00','09:00:00','10:00:00', '11:00:00', '12:00:00', '13:00:00', '14:00:00', '15:00:00', '16:00:00', '17:00:00', '18:00:00', '19:00:00', '20:00:00', '21:00:00', '22:00:00', '23:00:00']\n",
    "    e_time = ['01:00:00', '02:00:00','03:00:00','04:00:00','05:00:00','06:00:00','07:00:00','08:00:00','09:00:00','10:00:00','11:00:00', '12:00:00', '13:00:00', '14:00:00', '15:00:00', '16:00:00', '17:00:00', '18:00:00', '19:00:00', '20:00:00', '21:00:00', '22:00:00', '23:00:00','23:59:59']\n",
    "    max_results = 20\n",
    "\n",
    "    #Total number of tweets we collected from the loop\n",
    "    total_tweets = 0\n",
    "\n",
    "    # Create file\n",
    "    csvFile = open(\"data.csv\", \"a\", newline=\"\", encoding='utf-8')\n",
    "    csvWriter = csv.writer(csvFile)\n",
    "\n",
    "    #Create headers for the data you want to save, in this example, we only want save these columns in our dataset\n",
    "    csvWriter.writerow(['author id', 'created_at', 'geo', 'id','lang', 'like_count', 'quote_count', 'reply_count','retweet_count','source','tweet'])\n",
    "    csvFile.close()\n",
    "    \n",
    "    date='2022-02-16'\n",
    "\n",
    "    for i in range(0,len(s_time)):\n",
    "\n",
    "        # Inputs\n",
    "        count = 0 # Counting tweets per time period\n",
    "        max_count = 100 # Max tweets per time period\n",
    "        flag = True\n",
    "        next_token = None\n",
    "\n",
    "        # Check if flag is true\n",
    "        while flag:\n",
    "            # Check if max_count reached\n",
    "            if count >= max_count:\n",
    "                break\n",
    "            print(\"-------------------\")\n",
    "            print(\"Token: \", next_token)\n",
    "            url = create_url(keyword, date+'T'+s_time[i]+'.000Z',date+'T'+e_time[i]+'.000Z', max_results)\n",
    "            json_response = connect_to_endpoint(url[0], headers, url[1], next_token)\n",
    "            result_count = json_response['meta']['result_count']\n",
    "\n",
    "            if 'next_token' in json_response['meta']:\n",
    "                # Save the token to use for next call\n",
    "                next_token = json_response['meta']['next_token']\n",
    "                print(\"Next Token: \", next_token)\n",
    "                if result_count is not None and result_count > 0 and next_token is not None:\n",
    "                    print(\"Start Date: \", date+'T'+s_time[i]+'.000Z')\n",
    "                    append_to_csv(json_response, \"data.csv\")\n",
    "                    count += result_count\n",
    "                    total_tweets += result_count\n",
    "                    print(\"Total # of Tweets added: \", total_tweets)\n",
    "                    print(\"-------------------\")\n",
    "                    time.sleep(1)                \n",
    "            # If no next token exists\n",
    "            else:\n",
    "                if result_count is not None and result_count > 0:\n",
    "                    print(\"-------------------\")\n",
    "                    print(\"Start Date: \", date+'T'+s_time[i]+'.000Z')\n",
    "                    append_to_csv(json_response, \"data.csv\")\n",
    "                    count += result_count\n",
    "                    total_tweets += result_count\n",
    "                    print(\"Total # of Tweets added: \", total_tweets)\n",
    "                    print(\"-------------------\")\n",
    "                    time.sleep(1)\n",
    "\n",
    "                #Since this is the final request, turn flag to false to move to the next time period.\n",
    "                flag = False\n",
    "                next_token = None\n",
    "            time.sleep(1)\n",
    "    print(\"Total number of results: \", total_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0cad1812",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def main():\n",
    "#     csvFile = open(file_path, \"a\", newline=\"\", encoding='utf-8')\n",
    "#     csvWriter = csv.writer(csvFile)\n",
    "#     csvWriter.writerow(['author id', 'created_at', 'geo', 'country', 'country_code', 'place_full_name', 'place_name', 'place_type', 'id','lang', 'like_count', 'quote_count', 'reply_count','retweet_count','source','tweet', 'hashtags', 'sensitive', 'urls', 'context_text', 'context_probability', 'context_type'])\n",
    "  \n",
    "#     query_list = ['nasa']\n",
    "#     # url = create_url()\n",
    "#     # json_response = connect_to_endpoint(url)\n",
    "#     for tag in query_list: \n",
    "#       custom_url, params = custom_create_url(tag)\n",
    "#       custom_json_response = custom_connect_to_endpoint(custom_url, params)\n",
    "#       make_csv(custom_json_response, file_path, csvWriter)\n",
    "#       print(json.dumps(custom_json_response, indent=4, sort_keys=True))\n",
    "    \n",
    "#     # When done, close the CSV file\n",
    "#     csvFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9bc246e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------\n",
      "Token:  None\n",
      "Endpoint Response Code: 200\n",
      "-------------------\n",
      "Start Date:  2022-02-16T00:00:00.000Z\n",
      "# of Tweets added from this response:  2\n",
      "Total # of Tweets added:  2\n",
      "-------------------\n",
      "-------------------\n",
      "Token:  None\n",
      "Endpoint Response Code: 200\n",
      "-------------------\n",
      "Start Date:  2022-02-16T01:00:00.000Z\n",
      "# of Tweets added from this response:  4\n",
      "Total # of Tweets added:  6\n",
      "-------------------\n",
      "-------------------\n",
      "Token:  None\n",
      "Endpoint Response Code: 200\n",
      "-------------------\n",
      "Start Date:  2022-02-16T02:00:00.000Z\n",
      "# of Tweets added from this response:  5\n",
      "Total # of Tweets added:  11\n",
      "-------------------\n",
      "-------------------\n",
      "Token:  None\n",
      "Endpoint Response Code: 200\n",
      "-------------------\n",
      "Start Date:  2022-02-16T03:00:00.000Z\n",
      "# of Tweets added from this response:  18\n",
      "Total # of Tweets added:  29\n",
      "-------------------\n",
      "-------------------\n",
      "Token:  None\n",
      "Endpoint Response Code: 200\n",
      "-------------------\n",
      "Start Date:  2022-02-16T04:00:00.000Z\n",
      "# of Tweets added from this response:  20\n",
      "Total # of Tweets added:  49\n",
      "-------------------\n",
      "-------------------\n",
      "Token:  None\n",
      "Endpoint Response Code: 200\n",
      "-------------------\n",
      "Start Date:  2022-02-16T05:00:00.000Z\n",
      "# of Tweets added from this response:  20\n",
      "Total # of Tweets added:  69\n",
      "-------------------\n",
      "-------------------\n",
      "Token:  None\n",
      "Endpoint Response Code: 200\n",
      "-------------------\n",
      "Start Date:  2022-02-16T06:00:00.000Z\n",
      "# of Tweets added from this response:  20\n",
      "Total # of Tweets added:  89\n",
      "-------------------\n",
      "-------------------\n",
      "Token:  None\n",
      "Endpoint Response Code: 200\n",
      "-------------------\n",
      "Start Date:  2022-02-16T07:00:00.000Z\n",
      "# of Tweets added from this response:  20\n",
      "Total # of Tweets added:  109\n",
      "-------------------\n",
      "-------------------\n",
      "Token:  None\n",
      "Endpoint Response Code: 200\n",
      "-------------------\n",
      "Start Date:  2022-02-16T08:00:00.000Z\n",
      "# of Tweets added from this response:  13\n",
      "Total # of Tweets added:  122\n",
      "-------------------\n",
      "-------------------\n",
      "Token:  None\n",
      "Endpoint Response Code: 200\n",
      "-------------------\n",
      "Start Date:  2022-02-16T09:00:00.000Z\n",
      "# of Tweets added from this response:  20\n",
      "Total # of Tweets added:  142\n",
      "-------------------\n",
      "-------------------\n",
      "Token:  None\n",
      "Endpoint Response Code: 200\n",
      "-------------------\n",
      "Start Date:  2022-02-16T10:00:00.000Z\n",
      "# of Tweets added from this response:  20\n",
      "Total # of Tweets added:  162\n",
      "-------------------\n",
      "-------------------\n",
      "Token:  None\n",
      "Endpoint Response Code: 200\n",
      "-------------------\n",
      "Start Date:  2022-02-16T11:00:00.000Z\n",
      "# of Tweets added from this response:  17\n",
      "Total # of Tweets added:  179\n",
      "-------------------\n",
      "-------------------\n",
      "Token:  None\n",
      "Endpoint Response Code: 200\n",
      "-------------------\n",
      "Start Date:  2022-02-16T12:00:00.000Z\n",
      "# of Tweets added from this response:  20\n",
      "Total # of Tweets added:  199\n",
      "-------------------\n",
      "-------------------\n",
      "Token:  None\n",
      "Endpoint Response Code: 200\n",
      "-------------------\n",
      "Start Date:  2022-02-16T13:00:00.000Z\n",
      "# of Tweets added from this response:  20\n",
      "Total # of Tweets added:  219\n",
      "-------------------\n",
      "-------------------\n",
      "Token:  None\n",
      "Endpoint Response Code: 200\n",
      "-------------------\n",
      "Start Date:  2022-02-16T14:00:00.000Z\n",
      "# of Tweets added from this response:  20\n",
      "Total # of Tweets added:  239\n",
      "-------------------\n",
      "-------------------\n",
      "Token:  None\n",
      "Endpoint Response Code: 200\n",
      "-------------------\n",
      "Start Date:  2022-02-16T15:00:00.000Z\n",
      "# of Tweets added from this response:  20\n",
      "Total # of Tweets added:  259\n",
      "-------------------\n",
      "-------------------\n",
      "Token:  None\n",
      "Endpoint Response Code: 200\n",
      "-------------------\n",
      "Start Date:  2022-02-16T16:00:00.000Z\n",
      "# of Tweets added from this response:  17\n",
      "Total # of Tweets added:  276\n",
      "-------------------\n",
      "-------------------\n",
      "Token:  None\n",
      "Endpoint Response Code: 200\n",
      "-------------------\n",
      "Start Date:  2022-02-16T17:00:00.000Z\n",
      "# of Tweets added from this response:  14\n",
      "Total # of Tweets added:  290\n",
      "-------------------\n",
      "-------------------\n",
      "Token:  None\n",
      "Endpoint Response Code: 200\n",
      "-------------------\n",
      "Start Date:  2022-02-16T18:00:00.000Z\n",
      "# of Tweets added from this response:  16\n",
      "Total # of Tweets added:  306\n",
      "-------------------\n",
      "-------------------\n",
      "Token:  None\n",
      "Endpoint Response Code: 200\n",
      "-------------------\n",
      "Start Date:  2022-02-16T19:00:00.000Z\n",
      "# of Tweets added from this response:  2\n",
      "Total # of Tweets added:  308\n",
      "-------------------\n",
      "-------------------\n",
      "Token:  None\n",
      "Endpoint Response Code: 200\n",
      "-------------------\n",
      "Token:  None\n",
      "Endpoint Response Code: 200\n",
      "-------------------\n",
      "Start Date:  2022-02-16T21:00:00.000Z\n",
      "# of Tweets added from this response:  2\n",
      "Total # of Tweets added:  310\n",
      "-------------------\n",
      "-------------------\n",
      "Token:  None\n",
      "Endpoint Response Code: 200\n",
      "-------------------\n",
      "Start Date:  2022-02-16T22:00:00.000Z\n",
      "# of Tweets added from this response:  1\n",
      "Total # of Tweets added:  311\n",
      "-------------------\n",
      "-------------------\n",
      "Token:  None\n",
      "Endpoint Response Code: 200\n",
      "Total number of results:  311\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
