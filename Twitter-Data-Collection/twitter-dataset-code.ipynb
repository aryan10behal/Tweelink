{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "-UPir6fuqvAU"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "import json\n",
    "import csv\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import dateutil.parser\n",
    "import unicodedata\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "upnQtvY6WTOR"
   },
   "source": [
    "# Enter your path of data file here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "FzZLHbuoJA_Y"
   },
   "outputs": [],
   "source": [
    "file_path = 'Twitter-Dataset/data.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S0zS2ShgW_p_"
   },
   "source": [
    "# Enter bearer token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "0hDC-hGwq16n"
   },
   "outputs": [],
   "source": [
    "bearer_token = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x1B2jN0j2Jh3"
   },
   "source": [
    "# Change date here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "6dZjeOgRDjT4"
   },
   "outputs": [],
   "source": [
    "# update the dates by 1 day each every day\n",
    "dates = ['2022-02-13', '2022-02-14', '2022-02-15']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "ZqH84o5aDuk1"
   },
   "outputs": [],
   "source": [
    "s_time = ['00:00:00', '01:00:00','02:00:00','03:00:00','04:00:00','05:00:00','06:00:00','07:00:00','08:00:00','09:00:00','10:00:00', '11:00:00', '12:00:00', '13:00:00', '14:00:00', '15:00:00', '16:00:00', '17:00:00', '18:00:00', '19:00:00', '20:00:00', '21:00:00', '22:00:00', '23:00:00'] \n",
    "e_time = ['01:00:00', '02:00:00','03:00:00','04:00:00','05:00:00','06:00:00','07:00:00','08:00:00','09:00:00','10:00:00','11:00:00', '12:00:00', '13:00:00', '14:00:00', '15:00:00', '16:00:00', '17:00:00', '18:00:00', '19:00:00', '20:00:00', '21:00:00', '22:00:00', '23:00:00','23:59:59']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "xE_hYVjnxnq3"
   },
   "outputs": [],
   "source": [
    "def custom_create_url(tag, s_date, e_date):\n",
    "  url = \"https://api.twitter.com/2/tweets/search/recent\"\n",
    "  params = {\n",
    "      'query': '#'+tag+' lang:en -is:retweet',\n",
    "      'start_time': s_date,\n",
    "      'end_time': e_date,\n",
    "      'max_results':20,\n",
    "      'sort_order': 'relevancy',\n",
    "      'tweet.fields': 'attachments,author_id,context_annotations,conversation_id,created_at,entities,geo,id,in_reply_to_user_id,lang,public_metrics,possibly_sensitive,referenced_tweets,reply_settings,source,text,withheld',\n",
    "      'expansions': 'attachments.poll_ids,attachments.media_keys,author_id,entities.mentions.username,geo.place_id,in_reply_to_user_id,referenced_tweets.id,referenced_tweets.id.author_id',\n",
    "      'user.fields': 'created_at,description,entities,id,location,name,pinned_tweet_id,profile_image_url,protected,public_metrics,url,username,verified,withheld',\n",
    "      'media.fields': 'duration_ms,height,media_key,preview_image_url,type,url,width,public_metrics,alt_text',\n",
    "      'place.fields': 'contained_within,country,country_code,full_name,geo,id,name,place_type',\n",
    "      'poll.fields': 'duration_minutes,end_datetime,id,options,voting_status'\n",
    "  }\n",
    "  return url, params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "ZyoTyeairKvV"
   },
   "outputs": [],
   "source": [
    "def bearer_oauth(r):\n",
    "    \"\"\"\n",
    "    Method required by bearer token authentication.\n",
    "    \"\"\"\n",
    "\n",
    "    r.headers[\"Authorization\"] = f\"Bearer {bearer_token}\"\n",
    "    r.headers[\"User-Agent\"] = \"v2TweetLookupPython\"\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "bM4EG6nYz4Qj"
   },
   "outputs": [],
   "source": [
    "def custom_connect_to_endpoint(custom_url, params):\n",
    "    response = requests.request(\"GET\", custom_url, auth=bearer_oauth, params=params)\n",
    "    print(response.status_code)\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(\n",
    "            \"Request returned an error: {} {}\".format(\n",
    "                response.status_code, response.text\n",
    "            )\n",
    "        )\n",
    "    return response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "6hw8SW2fILFk"
   },
   "outputs": [],
   "source": [
    "def make_csv(json_response, file_path, csvWriter):\n",
    "  counter = 0\n",
    "  \n",
    "  if 'data' in json_response:\n",
    "    for tweet in json_response['data']:\n",
    "        \n",
    "        # We will create a variable for each since some of the keys might not exist for some tweets\n",
    "        # So we will account for that\n",
    "\n",
    "        # 1. Author ID\n",
    "        author_id = tweet['author_id']\n",
    "\n",
    "        # 2. Time created\n",
    "        created_at = dateutil.parser.parse(tweet['created_at'])\n",
    "\n",
    "        # example = \n",
    "        # {\n",
    "        #           \"country\": \"United States\",\n",
    "        #           \"country_code\": \"US\",\n",
    "        #           \"full_name\": \"Chicago, IL\",\n",
    "        #           \"geo\": {\n",
    "        #               \"bbox\": [\n",
    "        #                   -87.940033,\n",
    "        #                   41.644102,\n",
    "        #                   -87.523993,\n",
    "        #                   42.0230669\n",
    "        #               ],\n",
    "        #               \"properties\": {},\n",
    "        #               \"type\": \"Feature\"\n",
    "        #           },\n",
    "        #           \"id\": \"1d9a5370a355ab0c\",\n",
    "        #           \"name\": \"Chicago\",\n",
    "        #           \"place_type\": \"city\"\n",
    "        #       }\n",
    "\n",
    "        geo = \"\"\n",
    "        country = \"\"\n",
    "        country_code = \"\"\n",
    "        place_full_name = \"\"\n",
    "        place_name = \"\"\n",
    "        place_type = \"\"\n",
    "        # 3. Geolocation\n",
    "        if ('geo' in tweet):   \n",
    "            geo = tweet['geo']['place_id']\n",
    "            for place in json_response['includes']['places']:\n",
    "              if place['id'] == geo:\n",
    "                country = place['country']\n",
    "                country_code = place['country_code']\n",
    "                place_full_name = place['full_name']\n",
    "                place_name = place['name']\n",
    "                place_type = place['place_type']\n",
    "        else:\n",
    "            geo = \"\"\n",
    "\n",
    "        # 4. Tweet ID\n",
    "        tweet_id = tweet['id']\n",
    "\n",
    "        # 5. Language\n",
    "        lang = tweet['lang']\n",
    "\n",
    "        # 6. Tweet metrics\n",
    "        retweet_count = tweet['public_metrics']['retweet_count']\n",
    "        reply_count = tweet['public_metrics']['reply_count']\n",
    "        like_count = tweet['public_metrics']['like_count']\n",
    "        quote_count = tweet['public_metrics']['quote_count']\n",
    "\n",
    "        # 7. source\n",
    "        source = tweet['source']\n",
    "\n",
    "        # 8. Tweet text\n",
    "        text = tweet['text']\n",
    "\n",
    "        # 9 hashtags\n",
    "        tags = ''\n",
    "        if ('hashtags' in tweet['entities']):  \n",
    "          for tag in tweet['entities']['hashtags']:\n",
    "            tags += tag['tag'] + str(',')\n",
    "          tags = tags[:-1]\n",
    "\n",
    "        #sensitive\n",
    "        sensitive = tweet['possibly_sensitive']\n",
    "\n",
    "        # urls for further analysis\n",
    "        urls = ''\n",
    "        if 'urls' in tweet['entities']:\n",
    "          for url in tweet['entities']['urls']:\n",
    "            urls += url['url'] + str(',')\n",
    "          urls = urls[:-1]\n",
    "        \n",
    "        #annotations\n",
    "        context_text = ''\n",
    "        context_probability = 0\n",
    "        context_type = ''\n",
    "        if 'tweets' in json_response['includes']:\n",
    "          for tweets_for_annotation in json_response['includes']['tweets']:\n",
    "            if tweets_for_annotation['conversation_id'] == tweet['conversation_id']:\n",
    "              if 'entities' in tweets_for_annotation:\n",
    "                if 'annotations' in tweets_for_annotation['entities']:\n",
    "                  for annotation in tweets_for_annotation['entities']['annotations']:\n",
    "                    context_text = annotation['normalized_text']\n",
    "                    context_probability = annotation['probability']\n",
    "                    context_type = annotation['type']\n",
    "\n",
    "\n",
    "        \n",
    "        # Assemble all data in a list\n",
    "        res = [author_id, created_at, geo, country, country_code, place_full_name, place_name, place_type, tweet_id, lang, like_count, quote_count, reply_count, retweet_count, source, text, tags, sensitive, urls, context_text, context_probability, context_type]\n",
    "        \n",
    "        # Append the result to the CSV file\n",
    "        csvWriter.writerow(res)\n",
    "        counter += 1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "  # Print the number of tweets for this iteration\n",
    "  print(\"# of Tweets added from this response: \", counter) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p7daKroQX0JP"
   },
   "source": [
    "# update query list every day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "xBoosARMrQW7"
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Updateee this listtttt\n",
    "    query_list = ['canada', 'nasa']\n",
    "\n",
    "    csvFile = open(file_path, \"a\", newline=\"\", encoding='utf-8')\n",
    "    csvWriter = csv.writer(csvFile)\n",
    "    csvWriter.writerow(['author id', 'created_at', 'geo', 'country', 'country_code', 'place_full_name', 'place_name', 'place_type', 'id','lang', 'like_count', 'quote_count', 'reply_count','retweet_count','source','tweet', 'hashtags', 'sensitive', 'urls', 'context_text', 'context_probability', 'context_type'])\n",
    "    for date in dates:\n",
    "      for i in range(len(s_time)):\n",
    "        # url = create_url()\n",
    "        # json_response = connect_to_endpoint(url)\n",
    "        # 'start_time': '2022-02-16T00:00:00.000Z',\n",
    "        cur_s_time = date+'T'+s_time[i]+'.000Z'\n",
    "        cur_e_time = date+'T'+e_time[i]+'.000Z'\n",
    "        print(cur_s_time, cur_e_time)\n",
    "        for tag in query_list: \n",
    "          custom_url, params = custom_create_url(tag, cur_s_time, cur_e_time)\n",
    "          custom_json_response = custom_connect_to_endpoint(custom_url, params)\n",
    "          make_csv(custom_json_response, file_path, csvWriter)\n",
    "          # print(json.dumps(custom_json_response, indent=4, sort_keys=True))\n",
    "    \n",
    "    # When done, close the CSV file\n",
    "    csvFile.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JN9DOqWorSyS",
    "outputId": "a3ea3f78-d7c5-4724-eea5-d708bcf0fd1a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-02-13T00:00:00.000Z 2022-02-13T01:00:00.000Z\n",
      "200\n",
      "# of Tweets added from this response:  20\n",
      "200\n",
      "# of Tweets added from this response:  18\n",
      "2022-02-13T01:00:00.000Z 2022-02-13T02:00:00.000Z\n",
      "200\n",
      "# of Tweets added from this response:  20\n",
      "200\n",
      "# of Tweets added from this response:  18\n",
      "2022-02-13T02:00:00.000Z 2022-02-13T03:00:00.000Z\n",
      "200\n",
      "# of Tweets added from this response:  20\n",
      "200\n",
      "# of Tweets added from this response:  20\n",
      "2022-02-13T03:00:00.000Z 2022-02-13T04:00:00.000Z\n",
      "200\n",
      "# of Tweets added from this response:  20\n",
      "200\n",
      "# of Tweets added from this response:  15\n",
      "2022-02-13T04:00:00.000Z 2022-02-13T05:00:00.000Z\n",
      "200\n",
      "# of Tweets added from this response:  20\n",
      "200\n",
      "# of Tweets added from this response:  14\n",
      "2022-02-13T05:00:00.000Z 2022-02-13T06:00:00.000Z\n",
      "200\n",
      "# of Tweets added from this response:  20\n",
      "200\n",
      "# of Tweets added from this response:  20\n",
      "2022-02-13T06:00:00.000Z 2022-02-13T07:00:00.000Z\n",
      "200\n",
      "# of Tweets added from this response:  20\n",
      "200\n",
      "# of Tweets added from this response:  20\n",
      "2022-02-13T07:00:00.000Z 2022-02-13T08:00:00.000Z\n",
      "200\n",
      "# of Tweets added from this response:  20\n",
      "200\n",
      "# of Tweets added from this response:  20\n",
      "2022-02-13T08:00:00.000Z 2022-02-13T09:00:00.000Z\n",
      "200\n",
      "# of Tweets added from this response:  20\n",
      "200\n",
      "# of Tweets added from this response:  20\n",
      "2022-02-13T09:00:00.000Z 2022-02-13T10:00:00.000Z\n",
      "200\n",
      "# of Tweets added from this response:  20\n",
      "200\n",
      "# of Tweets added from this response:  20\n",
      "2022-02-13T10:00:00.000Z 2022-02-13T11:00:00.000Z\n",
      "200\n",
      "# of Tweets added from this response:  20\n",
      "200\n",
      "# of Tweets added from this response:  20\n",
      "2022-02-13T11:00:00.000Z 2022-02-13T12:00:00.000Z\n",
      "200\n",
      "# of Tweets added from this response:  20\n",
      "200\n",
      "# of Tweets added from this response:  20\n",
      "2022-02-13T12:00:00.000Z 2022-02-13T13:00:00.000Z\n",
      "200\n",
      "# of Tweets added from this response:  20\n",
      "200\n",
      "# of Tweets added from this response:  20\n",
      "2022-02-13T13:00:00.000Z 2022-02-13T14:00:00.000Z\n",
      "200\n",
      "# of Tweets added from this response:  20\n",
      "200\n",
      "# of Tweets added from this response:  20\n",
      "2022-02-13T14:00:00.000Z 2022-02-13T15:00:00.000Z\n",
      "200\n",
      "# of Tweets added from this response:  20\n",
      "200\n",
      "# of Tweets added from this response:  20\n",
      "2022-02-13T15:00:00.000Z 2022-02-13T16:00:00.000Z\n",
      "200\n",
      "# of Tweets added from this response:  20\n",
      "200\n",
      "# of Tweets added from this response:  20\n",
      "2022-02-13T16:00:00.000Z 2022-02-13T17:00:00.000Z\n",
      "200\n",
      "# of Tweets added from this response:  20\n",
      "200\n",
      "# of Tweets added from this response:  20\n",
      "2022-02-13T17:00:00.000Z 2022-02-13T18:00:00.000Z\n",
      "200\n",
      "# of Tweets added from this response:  20\n",
      "200\n",
      "# of Tweets added from this response:  20\n",
      "2022-02-13T18:00:00.000Z 2022-02-13T19:00:00.000Z\n",
      "200\n",
      "# of Tweets added from this response:  20\n",
      "200\n",
      "# of Tweets added from this response:  20\n",
      "2022-02-13T19:00:00.000Z 2022-02-13T20:00:00.000Z\n",
      "200\n",
      "# of Tweets added from this response:  20\n",
      "200\n",
      "# of Tweets added from this response:  20\n",
      "2022-02-13T20:00:00.000Z 2022-02-13T21:00:00.000Z\n",
      "200\n",
      "# of Tweets added from this response:  20\n",
      "200\n",
      "# of Tweets added from this response:  20\n",
      "2022-02-13T21:00:00.000Z 2022-02-13T22:00:00.000Z\n",
      "200\n",
      "# of Tweets added from this response:  20\n",
      "200\n",
      "# of Tweets added from this response:  20\n",
      "2022-02-13T22:00:00.000Z 2022-02-13T23:00:00.000Z\n",
      "200\n",
      "# of Tweets added from this response:  20\n",
      "200\n",
      "# of Tweets added from this response:  20\n",
      "2022-02-13T23:00:00.000Z 2022-02-13T23:59:59.000Z\n",
      "200\n",
      "# of Tweets added from this response:  20\n",
      "200\n",
      "# of Tweets added from this response:  20\n",
      "2022-02-14T00:00:00.000Z 2022-02-14T01:00:00.000Z\n",
      "200\n",
      "# of Tweets added from this response:  20\n",
      "200\n",
      "# of Tweets added from this response:  20\n",
      "2022-02-14T01:00:00.000Z 2022-02-14T02:00:00.000Z\n",
      "200\n",
      "# of Tweets added from this response:  20\n",
      "200\n",
      "# of Tweets added from this response:  19\n",
      "2022-02-14T02:00:00.000Z 2022-02-14T03:00:00.000Z\n",
      "200\n",
      "# of Tweets added from this response:  20\n",
      "200\n",
      "# of Tweets added from this response:  18\n",
      "2022-02-14T03:00:00.000Z 2022-02-14T04:00:00.000Z\n",
      "200\n",
      "# of Tweets added from this response:  20\n",
      "200\n",
      "# of Tweets added from this response:  20\n",
      "2022-02-14T04:00:00.000Z 2022-02-14T05:00:00.000Z\n",
      "200\n",
      "# of Tweets added from this response:  20\n",
      "200\n",
      "# of Tweets added from this response:  20\n",
      "2022-02-14T05:00:00.000Z 2022-02-14T06:00:00.000Z\n",
      "200\n",
      "# of Tweets added from this response:  20\n",
      "200\n",
      "# of Tweets added from this response:  20\n",
      "2022-02-14T06:00:00.000Z 2022-02-14T07:00:00.000Z\n",
      "200\n",
      "# of Tweets added from this response:  20\n",
      "200\n",
      "# of Tweets added from this response:  20\n",
      "2022-02-14T07:00:00.000Z 2022-02-14T08:00:00.000Z\n",
      "200\n",
      "# of Tweets added from this response:  20\n",
      "200\n",
      "# of Tweets added from this response:  17\n",
      "2022-02-14T08:00:00.000Z 2022-02-14T09:00:00.000Z\n",
      "200\n",
      "# of Tweets added from this response:  20\n",
      "200\n",
      "# of Tweets added from this response:  20\n",
      "2022-02-14T09:00:00.000Z 2022-02-14T10:00:00.000Z\n",
      "200\n",
      "# of Tweets added from this response:  20\n",
      "200\n",
      "# of Tweets added from this response:  20\n",
      "2022-02-14T10:00:00.000Z 2022-02-14T11:00:00.000Z\n",
      "200\n",
      "# of Tweets added from this response:  20\n",
      "200\n",
      "# of Tweets added from this response:  17\n",
      "2022-02-14T11:00:00.000Z 2022-02-14T12:00:00.000Z\n",
      "200\n",
      "# of Tweets added from this response:  20\n",
      "200\n",
      "# of Tweets added from this response:  20\n",
      "2022-02-14T12:00:00.000Z 2022-02-14T13:00:00.000Z\n",
      "200\n",
      "# of Tweets added from this response:  20\n",
      "200\n",
      "# of Tweets added from this response:  19\n",
      "2022-02-14T13:00:00.000Z 2022-02-14T14:00:00.000Z\n",
      "200\n",
      "# of Tweets added from this response:  20\n",
      "200\n",
      "# of Tweets added from this response:  20\n",
      "2022-02-14T14:00:00.000Z 2022-02-14T15:00:00.000Z\n",
      "200\n",
      "# of Tweets added from this response:  20\n",
      "200\n",
      "# of Tweets added from this response:  20\n",
      "2022-02-14T15:00:00.000Z 2022-02-14T16:00:00.000Z\n",
      "200\n",
      "# of Tweets added from this response:  20\n",
      "200\n",
      "# of Tweets added from this response:  20\n",
      "2022-02-14T16:00:00.000Z 2022-02-14T17:00:00.000Z\n",
      "200\n",
      "# of Tweets added from this response:  20\n",
      "200\n",
      "# of Tweets added from this response:  20\n",
      "2022-02-14T17:00:00.000Z 2022-02-14T18:00:00.000Z\n",
      "200\n",
      "# of Tweets added from this response:  20\n",
      "200\n",
      "# of Tweets added from this response:  20\n",
      "2022-02-14T18:00:00.000Z 2022-02-14T19:00:00.000Z\n",
      "200\n",
      "# of Tweets added from this response:  20\n",
      "200\n",
      "# of Tweets added from this response:  20\n",
      "2022-02-14T19:00:00.000Z 2022-02-14T20:00:00.000Z\n",
      "200\n",
      "# of Tweets added from this response:  20\n",
      "200\n",
      "# of Tweets added from this response:  20\n",
      "2022-02-14T20:00:00.000Z 2022-02-14T21:00:00.000Z\n",
      "200\n",
      "# of Tweets added from this response:  20\n",
      "200\n",
      "# of Tweets added from this response:  20\n",
      "2022-02-14T21:00:00.000Z 2022-02-14T22:00:00.000Z\n",
      "200\n",
      "# of Tweets added from this response:  20\n",
      "200\n",
      "# of Tweets added from this response:  20\n",
      "2022-02-14T22:00:00.000Z 2022-02-14T23:00:00.000Z\n",
      "200\n",
      "# of Tweets added from this response:  20\n",
      "200\n",
      "# of Tweets added from this response:  20\n",
      "2022-02-14T23:00:00.000Z 2022-02-14T23:59:59.000Z\n",
      "200\n",
      "# of Tweets added from this response:  20\n",
      "200\n",
      "# of Tweets added from this response:  20\n",
      "2022-02-15T00:00:00.000Z 2022-02-15T01:00:00.000Z\n",
      "200\n",
      "# of Tweets added from this response:  20\n",
      "200\n",
      "# of Tweets added from this response:  20\n",
      "2022-02-15T01:00:00.000Z 2022-02-15T02:00:00.000Z\n",
      "200\n",
      "# of Tweets added from this response:  20\n",
      "200\n",
      "# of Tweets added from this response:  19\n",
      "2022-02-15T02:00:00.000Z 2022-02-15T03:00:00.000Z\n",
      "200\n",
      "# of Tweets added from this response:  20\n",
      "200\n",
      "# of Tweets added from this response:  20\n",
      "2022-02-15T03:00:00.000Z 2022-02-15T04:00:00.000Z\n",
      "200\n",
      "# of Tweets added from this response:  20\n",
      "200\n",
      "# of Tweets added from this response:  19\n",
      "2022-02-15T04:00:00.000Z 2022-02-15T05:00:00.000Z\n",
      "200\n",
      "# of Tweets added from this response:  20\n",
      "200\n",
      "# of Tweets added from this response:  20\n",
      "2022-02-15T05:00:00.000Z 2022-02-15T06:00:00.000Z\n",
      "200\n",
      "# of Tweets added from this response:  20\n",
      "200\n",
      "# of Tweets added from this response:  20\n",
      "2022-02-15T06:00:00.000Z 2022-02-15T07:00:00.000Z\n",
      "200\n",
      "# of Tweets added from this response:  20\n",
      "200\n",
      "# of Tweets added from this response:  20\n",
      "2022-02-15T07:00:00.000Z 2022-02-15T08:00:00.000Z\n",
      "200\n",
      "# of Tweets added from this response:  20\n",
      "200\n",
      "# of Tweets added from this response:  20\n",
      "2022-02-15T08:00:00.000Z 2022-02-15T09:00:00.000Z\n",
      "200\n",
      "# of Tweets added from this response:  20\n",
      "200\n",
      "# of Tweets added from this response:  20\n",
      "2022-02-15T09:00:00.000Z 2022-02-15T10:00:00.000Z\n",
      "200\n",
      "# of Tweets added from this response:  20\n",
      "200\n",
      "# of Tweets added from this response:  20\n",
      "2022-02-15T10:00:00.000Z 2022-02-15T11:00:00.000Z\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "# of Tweets added from this response:  20\n",
      "200\n",
      "# of Tweets added from this response:  20\n",
      "2022-02-15T11:00:00.000Z 2022-02-15T12:00:00.000Z\n",
      "200\n",
      "# of Tweets added from this response:  20\n",
      "200\n",
      "# of Tweets added from this response:  19\n",
      "2022-02-15T12:00:00.000Z 2022-02-15T13:00:00.000Z\n",
      "200\n",
      "# of Tweets added from this response:  20\n",
      "200\n",
      "# of Tweets added from this response:  20\n",
      "2022-02-15T13:00:00.000Z 2022-02-15T14:00:00.000Z\n",
      "200\n",
      "# of Tweets added from this response:  20\n",
      "200\n",
      "# of Tweets added from this response:  20\n",
      "2022-02-15T14:00:00.000Z 2022-02-15T15:00:00.000Z\n",
      "200\n",
      "# of Tweets added from this response:  20\n",
      "200\n",
      "# of Tweets added from this response:  20\n",
      "2022-02-15T15:00:00.000Z 2022-02-15T16:00:00.000Z\n",
      "200\n",
      "# of Tweets added from this response:  20\n",
      "200\n",
      "# of Tweets added from this response:  20\n",
      "2022-02-15T16:00:00.000Z 2022-02-15T17:00:00.000Z\n",
      "200\n",
      "# of Tweets added from this response:  20\n",
      "200\n",
      "# of Tweets added from this response:  20\n",
      "2022-02-15T17:00:00.000Z 2022-02-15T18:00:00.000Z\n",
      "200\n",
      "# of Tweets added from this response:  20\n",
      "200\n",
      "# of Tweets added from this response:  20\n",
      "2022-02-15T18:00:00.000Z 2022-02-15T19:00:00.000Z\n",
      "200\n",
      "# of Tweets added from this response:  20\n",
      "200\n",
      "# of Tweets added from this response:  20\n",
      "2022-02-15T19:00:00.000Z 2022-02-15T20:00:00.000Z\n",
      "200\n",
      "# of Tweets added from this response:  20\n",
      "200\n",
      "# of Tweets added from this response:  20\n",
      "2022-02-15T20:00:00.000Z 2022-02-15T21:00:00.000Z\n",
      "200\n",
      "# of Tweets added from this response:  20\n",
      "200\n",
      "# of Tweets added from this response:  20\n",
      "2022-02-15T21:00:00.000Z 2022-02-15T22:00:00.000Z\n",
      "200\n",
      "# of Tweets added from this response:  20\n",
      "200\n",
      "# of Tweets added from this response:  20\n",
      "2022-02-15T22:00:00.000Z 2022-02-15T23:00:00.000Z\n",
      "200\n",
      "# of Tweets added from this response:  20\n",
      "200\n",
      "# of Tweets added from this response:  20\n",
      "2022-02-15T23:00:00.000Z 2022-02-15T23:59:59.000Z\n",
      "200\n",
      "# of Tweets added from this response:  20\n",
      "200\n",
      "# of Tweets added from this response:  20\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "twitter_recent_search.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
