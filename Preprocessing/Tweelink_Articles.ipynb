{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Tweelink_Articles.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "z4H8HrNIgLA-"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import csv\n",
        "import json\n",
        "from itertools import islice\n",
        "from collections import OrderedDict"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Importing essential libraries\n",
        "import pandas as pd\n",
        "import re\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import os\n",
        "import nltk\n",
        "from glob import glob\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "from tqdm import tqdm\n",
        "import pickle\n",
        "import math\n",
        "from sklearn.model_selection import train_test_split\n",
        "import operator\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('corpus')\n",
        "from nltk.corpus import wordnet as wn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PIErUCcHh2xe",
        "outputId": "b610d4dd-712d-4550-80ea-ea8d7ad1ec0d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Error loading corpus: Package 'corpus' not found in index\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "swJZXLJLkKoc",
        "outputId": "4533cb30-0ae1-4f19-f9ac-09cb3a1c9201"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Gajlb9cmjaf",
        "outputId": "2fa79d4a-914c-4550-c082-ae1520157a11"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "preprocessed_vocabulary = dict()\n",
        "\n",
        "#Converting to lowercase\n",
        "def to_lower_case(text):\n",
        "  text = text.lower()\n",
        "  return text\n",
        "\n",
        "def remove_at_word(text):\n",
        "  data = text.split()\n",
        "  data = [d for d in data if d[0]!='@']\n",
        "  text = ' '.join(data)\n",
        "  return text\n",
        "\n",
        "def remove_hashtag(text):\n",
        "  data = text.split()\n",
        "  data = [d if (d[0]!='#' or len(d) == 1) else d[1:] for d in data]\n",
        "  data = [d for d in data if d[0]!='#']\n",
        "  text = ' '.join(data)\n",
        "  return text\n",
        "\n",
        "def remove_URL(text):\n",
        "  text = re.sub(r\"http\\S+\", \"\", text)\n",
        "  text = re.sub(r'bit.ly\\S+', '', text, flags=re.MULTILINE)\n",
        "  return text\n",
        "\n",
        "#Removing stopwords\n",
        "def remove_stopwords(text):\n",
        "  stopword = stopwords.words('english')\n",
        "  new_list = [x for x in text.split() if x not in stopword]\n",
        "  return ' '.join(new_list)\n",
        "\n",
        "#Removing punctuations\n",
        "def remove_punctuations(text):\n",
        "  punctuations = '''!()-[|]`{};:'\"\\,<>./?@#$=+%^&*_~'''\n",
        "  new_list = ['' if x in punctuations else x for x in text.split()]\n",
        "  new_list_final = []\n",
        "  for token in new_list:\n",
        "    new_token=\"\"\n",
        "    for char in token:\n",
        "      if(char not in punctuations):\n",
        "        new_token+=char\n",
        "    if(len(new_token)!=0):\n",
        "      new_list_final.append(new_token)\n",
        "  return ' '.join(new_list_final)\n",
        "\n",
        "def remove_non_utf(text):\n",
        "  text = text.encode('ascii', 'ignore').decode()\n",
        "  return text\n",
        "\n",
        "\n",
        "def lemmatization(text):\n",
        "  wordnet_lemmatizer = WordNetLemmatizer()\n",
        "  new_list = [wordnet_lemmatizer.lemmatize(x, pos = wn.synsets(x)[0].pos()) if len(wn.synsets(x)) > 0 else wordnet_lemmatizer.lemmatize(x) for x in text]\n",
        "  return new_list\n",
        "\n",
        "#Tokenization\n",
        "def tokenization(text):\n",
        "  return word_tokenize(text)\n",
        "\n",
        "def text_preprocess(text):\n",
        "  #global preprocessed_vocabulary\n",
        "  text = to_lower_case(text)\n",
        "  text = remove_at_word(text)\n",
        "  text = remove_hashtag(text)\n",
        "  text = remove_URL(text)\n",
        "  text = remove_stopwords(text)\n",
        "  text = remove_punctuations(text)\n",
        "  text = remove_non_utf(text)\n",
        "  text = [x.strip() for x in text.split() if len(x.strip())]\n",
        "  text = lemmatization(text)\n",
        "  for token in text:\n",
        "    if token in preprocessed_vocabulary.keys():\n",
        "      preprocessed_vocabulary[token] += 1\n",
        "    else:\n",
        "      preprocessed_vocabulary[token] = 1\n",
        "  return text\n",
        "\n",
        "def data_preprocess(data):\n",
        "  data['text'] = data['text'].apply(text_preprocess)"
      ],
      "metadata": {
        "id": "4yZ5WlnOiJSG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_json(csvFilePath):\n",
        "     \n",
        "    # create a dictionary\n",
        "    \n",
        "    fieldnames = [\"S. No.\", \t\"Base Hashtag\",\t\"Link\",\t\"Date\", \"Location\", \"Title\", \"Body\"]\n",
        "    # Open a csv reader called DictReader\n",
        "    with open(csvFilePath, encoding='utf-8') as csvf:\n",
        "        csvReader = csv.DictReader(csvf, fieldnames)\n",
        "         \n",
        "        # Convert each row into a dictionary\n",
        "        # and add it to data\n",
        "        for rows in csvReader:\n",
        "             \n",
        "            # Assuming a column named 'No' to\n",
        "            # be the primary key\n",
        "            \n",
        "            sliced = islice(rows.items(), 7)  # o.iteritems() is o.items() in Python 3\n",
        "            sliced_o = OrderedDict(sliced)\n",
        "            \n",
        "            key1 = rows['Base Hashtag']\n",
        "            key2 = rows['S. No.']\n",
        "            filename = key1+\"_\"+key2\n",
        "            filepath = \"/content/drive/MyDrive/IR/Tweelink_Articles/\"+filename+\".json\"\n",
        "            jsonfile = open(filepath, 'w')\n",
        "            json.dump(sliced_o, jsonfile)\n",
        "           \n",
        "\n",
        "\n",
        "            "
      ],
      "metadata": {
        "id": "0UwoLLtQgh-j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_json_preprocessed(csvFilePath):\n",
        "     \n",
        "    # create a dictionary\n",
        "    \n",
        "    fieldnames = [\"S. No.\", \t\"Base Hashtag\",\t\"Link\",\t\"Date\", \"Location\", \"Title\", \"Body\"]\n",
        "    \n",
        "    # Open a csv reader called DictReader\n",
        "    with open(csvFilePath, encoding='utf-8') as csvf:\n",
        "        csvReader = csv.DictReader(csvf, fieldnames)\n",
        "         \n",
        "        # Convert each row into a dictionary\n",
        "        # and add it to data\n",
        "        c = 0\n",
        "        for rows in csvReader:\n",
        "            if(c==0):\n",
        "              c+=1\n",
        "              continue\n",
        "            # Assuming a column named 'No' to\n",
        "            # be the primary key\n",
        "            \n",
        "            sliced = islice(rows.items(), 7)  # o.iteritems() is o.items() in Python 3\n",
        "            sliced_o = OrderedDict(sliced)\n",
        "            \n",
        "            key1 = rows['Base Hashtag']\n",
        "            key2 = rows['S. No.']\n",
        "            sliced_o['Body_processed'] = sliced_o['Title']+\" \"+sliced_o['Body']\n",
        "          \n",
        "            sliced_o['Body_processed'] = text_preprocess(sliced_o['Body_processed'])\n",
        "           \n",
        "            filename = key1+\"_\"+key2\n",
        "            filepath = \"/content/drive/MyDrive/IR/Tweelink_Articles_Processed/\"+filename+\".json\"\n",
        "            jsonfile = open(filepath, 'w')\n",
        "            json.dump(sliced_o, jsonfile)\n",
        "  \n",
        "\n",
        "          \n",
        "           "
      ],
      "metadata": {
        "id": "DYyuEtOZi3RM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "make_json(\"Hashtags and Article Database - Aryan.csv\")"
      ],
      "metadata": {
        "id": "yNDaipksH39e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "make_json_preprocessed(\"Hashtags and Article Database - Aryan.csv\")"
      ],
      "metadata": {
        "id": "EjK3jYFqnUlG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f = open('/content/drive/MyDrive/Tweelink_Articles_Processed/TaylorSwift_36.json')\n",
        "data = json.load(f)\n",
        "print(data)"
      ],
      "metadata": {
        "id": "V4nu3pMhpYVy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e3ff2317-d278-46a2-b890-49159e83c3bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'S. No.': '36', 'Base Hashtag': 'TaylorSwift', 'Link': 'https://www.cheatsheet.com/entertainment/kanye-wests-beef-billie-eilish-reminds-fans-2009-vmas-scandal-taylor-swift.html/', 'Date': '2022-02-14', 'Location': 'USA', 'Title': 'Kanye West’s Beef With Billie Eilish Reminds Fans of His 2009 VMAs Scandal With Taylor Swift', 'Body': 'Kanye West recently demanded an apology from Billie Eilish, and fans are comparing it to when the Donda rapper interrupted Taylor Swift at the 2009 MTV VMAs. Here’s what happened and what fans are saying about it. On Feb. 5, Billie Eilish paused her Happier Than Ever tour show in Atlanta, Georgia to help an audience member who couldn’t breathe. “We’re taking care of our people, hold on. I wait for people to be OK before I keep going,” the “Bad Guy” singer said. Eilish has stopped her shows in the past when she noticed safety issues in the crowd. In November 2021, 10 people died and hundreds were injured at Travis Scott’s Astroworld show in Houston, Texas. The audience was packed so tightly that they couldn’t breathe, and many were trampled.  Eilish and Kanye West are scheduled to headline Coachella this spring. On Feb. 10, the rapper threatened in a since-deleted Instagram post that unless Eilish apologizes to Travis Scott, he won’t perform at the music festival.  The 20-year-old singer did not name Scott or the Astroworld tragedy when she stopped her show, but West seemingly believed the pop star was slamming his fellow rapper. “COME ON BILLIE WE LOVE YOU PLEASE APOLOGIZE TO TRAV AND TO THE FAMILIES OF THE PEOPLE WHO LOST THEIR LIVES,” read West’s Instagram post caption. “NO ONE INTENDED THIS TO HAPPEN TRAV DIDN’T HAVE ANY IDEA OF WHAT WAS HAPPENING WHEN HE WAS ON STAGE AND WAS VERY HURT BY WHAT HAPPENED AND YES TRAV WILL BE WITH ME AT COACHELLA BUT NOW I NEED BILLIE TO APOLOGIZE BEFORE I PERFORM.” Eilish responded by commenting, “Literally never said a thing about Travis. Was just helping a fan.” Fans are comparing Kanye West and Billie Eilish’s interaction to the rapper’s feud with another young female pop star – Taylor Swift.', 'Body_processed': ['kanye', 'west', '’', 'beef', 'billie', 'eilish', 'reminds', 'fan', '2009', 'vmas', 'scandal', 'taylor', 'swift', 'kanye', 'west', 'recently', 'demanded', 'apology', 'billie', 'eilish', 'fan', 'comparing', 'donda', 'rapper', 'interrupted', 'taylor', 'swift', '2009', 'mtv', 'vmas', '’', 'happened', 'fan', 'saying', 'feb', '5', 'billie', 'eilish', 'paused', 'happier', 'ever', 'tour', 'show', 'atlanta', 'georgia', 'help', 'audience', 'member', '’', 'breathe', '“', '’', 'taking', 'care', 'people', 'hold', 'wait', 'people', 'ok', 'keep', 'going', '”', '“', 'bad', 'guy', '”', 'singer', 'said', 'eilish', 'stopped', 'show', 'past', 'noticed', 'safety', 'issue', 'crowd', 'november', '2021', '10', 'people', 'died', 'hundred', 'injured', 'travis', 'scott', '’', 'astroworld', 'show', 'houston', 'texas', 'audience', 'packed', 'tightly', '’', 'breathe', 'many', 'trampled', 'eilish', 'kanye', 'west', 'scheduled', 'headline', 'coachella', 'spring', 'feb', '10', 'rapper', 'threatened', 'sincedeleted', 'instagram', 'post', 'unless', 'eilish', 'apologizes', 'travis', 'scott', '’', 'perform', 'music', 'festival', '20yearold', 'singer', 'name', 'scott', 'astroworld', 'tragedy', 'stopped', 'show', 'west', 'seemingly', 'believed', 'pop', 'star', 'slamming', 'fellow', 'rapper', '“', 'come', 'billie', 'love', 'please', 'apologize', 'trav', 'family', 'people', 'lost', 'life', '”', 'read', 'west', '’', 'instagram', 'post', 'caption', '“', 'one', 'intended', 'happen', 'trav', '’', 'idea', 'happening', 'stage', 'hurt', 'happened', 'yes', 'trav', 'coachella', 'need', 'billie', 'apologize', 'perform', '”', 'eilish', 'responded', 'commenting', '“', 'literally', 'never', 'said', 'thing', 'travis', 'helping', 'fan', '”', 'fan', 'comparing', 'kanye', 'west', 'billie', 'eilish', '’', 'interaction', 'rapper', '’', 'feud', 'another', 'young', 'female', 'pop', 'star', '–', 'taylor', 'swift']}\n"
          ]
        }
      ]
    }
  ]
}